#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»»åŠ¡ç›‘æ§å™¨

å®ç°ä»»åŠ¡æ‰§è¡Œçš„å®æ—¶ç›‘æ§ã€æ€§èƒ½ç»Ÿè®¡å’Œå¼‚å¸¸æ£€æµ‹
"""

import json
import logging
import time
from collections import defaultdict, deque
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional

try:
    import redis

    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class TaskMetrics:
    """ä»»åŠ¡æ€§èƒ½æŒ‡æ ‡"""

    task_id: str
    worker_id: str
    start_time: float
    end_time: float = None
    duration: float = None
    status: str = "running"
    items_scraped: int = 0
    pages_crawled: int = 0
    errors_count: int = 0
    memory_usage: float = 0.0
    cpu_usage: float = 0.0

    def to_dict(self) -> Dict:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict) -> "TaskMetrics":
        return cls(**data)


class TaskMonitor:
    """ä»»åŠ¡ç›‘æ§å™¨"""

    def __init__(self, redis_url="redis://localhost:6379/0"):
        self.redis_url = redis_url
        self.redis = None

        # Redisé”®å
        self.metrics_key = "crawler:task_metrics"
        self.performance_key = "crawler:performance"
        self.alerts_key = "crawler:alerts"
        self.hourly_stats_key = "crawler:hourly_stats"

        # ç›‘æ§é…ç½®
        self.alert_thresholds = {
            "task_duration_max": 3600,  # ä»»åŠ¡æœ€å¤§æ‰§è¡Œæ—¶é—´(ç§’)
            "error_rate_max": 0.1,  # æœ€å¤§é”™è¯¯ç‡
            "memory_usage_max": 1024,  # æœ€å¤§å†…å­˜ä½¿ç”¨(MB)
            "cpu_usage_max": 90,  # æœ€å¤§CPUä½¿ç”¨ç‡(%)
            "queue_size_max": 1000,  # æœ€å¤§é˜Ÿåˆ—å¤§å°
        }

        # æ€§èƒ½ç»Ÿè®¡çª—å£
        self.stats_window_size = 100  # ä¿ç•™æœ€è¿‘100ä¸ªä»»åŠ¡çš„ç»Ÿè®¡

        # åˆå§‹åŒ–Redisè¿æ¥
        self.connect_redis()

        logger.info("ä»»åŠ¡ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")

    def connect_redis(self) -> bool:
        """è¿æ¥Redis"""
        if not REDIS_AVAILABLE:
            logger.error("Redisä¸å¯ç”¨ï¼Œè¯·å®‰è£…redis-py")
            return False

        try:
            self.redis = redis.from_url(self.redis_url)
            self.redis.ping()
            logger.info("Redisè¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"Redisè¿æ¥å¤±è´¥: {e}")
            return False

    def start_task_monitoring(self, task_id: str, worker_id: str) -> bool:
        """å¼€å§‹ç›‘æ§ä»»åŠ¡"""
        if not self.redis:
            return False

        try:
            metrics = TaskMetrics(
                task_id=task_id, worker_id=worker_id, start_time=time.time()
            )

            self.redis.hset(self.metrics_key, task_id, json.dumps(metrics.to_dict()))

            logger.debug(f"å¼€å§‹ç›‘æ§ä»»åŠ¡: {task_id}")
            return True

        except Exception as e:
            logger.error(f"å¼€å§‹ä»»åŠ¡ç›‘æ§å¤±è´¥: {e}")
            return False

    def update_task_metrics(self, task_id: str, metrics_update: Dict) -> bool:
        """æ›´æ–°ä»»åŠ¡æŒ‡æ ‡"""
        if not self.redis:
            return False

        try:
            # è·å–ç°æœ‰æŒ‡æ ‡
            metrics_data = self.redis.hget(self.metrics_key, task_id)
            if not metrics_data:
                logger.warning(f"ä»»åŠ¡æŒ‡æ ‡ä¸å­˜åœ¨: {task_id}")
                return False

            metrics = TaskMetrics.from_dict(json.loads(metrics_data))

            # æ›´æ–°æŒ‡æ ‡
            for key, value in metrics_update.items():
                if hasattr(metrics, key):
                    setattr(metrics, key, value)

            # ä¿å­˜æ›´æ–°åçš„æŒ‡æ ‡
            self.redis.hset(self.metrics_key, task_id, json.dumps(metrics.to_dict()))

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘å‘Šè­¦
            self.check_task_alerts(metrics)

            return True

        except Exception as e:
            logger.error(f"æ›´æ–°ä»»åŠ¡æŒ‡æ ‡å¤±è´¥: {e}")
            return False

    def complete_task_monitoring(
        self, task_id: str, final_metrics: Dict = None
    ) -> bool:
        """å®Œæˆä»»åŠ¡ç›‘æ§"""
        if not self.redis:
            return False

        try:
            # è·å–ä»»åŠ¡æŒ‡æ ‡
            metrics_data = self.redis.hget(self.metrics_key, task_id)
            if not metrics_data:
                logger.warning(f"ä»»åŠ¡æŒ‡æ ‡ä¸å­˜åœ¨: {task_id}")
                return False

            metrics = TaskMetrics.from_dict(json.loads(metrics_data))

            # è®¾ç½®ç»“æŸæ—¶é—´å’ŒçŠ¶æ€
            metrics.end_time = time.time()
            metrics.duration = metrics.end_time - metrics.start_time
            metrics.status = "completed"

            # åº”ç”¨æœ€ç»ˆæŒ‡æ ‡
            if final_metrics:
                for key, value in final_metrics.items():
                    if hasattr(metrics, key):
                        setattr(metrics, key, value)

            # ä¿å­˜æœ€ç»ˆæŒ‡æ ‡
            self.redis.hset(self.metrics_key, task_id, json.dumps(metrics.to_dict()))

            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self.update_performance_stats(metrics)

            # æ›´æ–°å°æ—¶ç»Ÿè®¡
            self.update_hourly_stats(metrics)

            logger.debug(f"å®Œæˆä»»åŠ¡ç›‘æ§: {task_id} (è€—æ—¶: {metrics.duration:.2f}ç§’)")
            return True

        except Exception as e:
            logger.error(f"å®Œæˆä»»åŠ¡ç›‘æ§å¤±è´¥: {e}")
            return False

    def fail_task_monitoring(self, task_id: str, error_info: Dict) -> bool:
        """ä»»åŠ¡å¤±è´¥ç›‘æ§"""
        if not self.redis:
            return False

        try:
            # è·å–ä»»åŠ¡æŒ‡æ ‡
            metrics_data = self.redis.hget(self.metrics_key, task_id)
            if not metrics_data:
                logger.warning(f"ä»»åŠ¡æŒ‡æ ‡ä¸å­˜åœ¨: {task_id}")
                return False

            metrics = TaskMetrics.from_dict(json.loads(metrics_data))

            # è®¾ç½®å¤±è´¥çŠ¶æ€
            metrics.end_time = time.time()
            metrics.duration = metrics.end_time - metrics.start_time
            metrics.status = "failed"
            metrics.errors_count += 1

            # ä¿å­˜å¤±è´¥æŒ‡æ ‡
            self.redis.hset(self.metrics_key, task_id, json.dumps(metrics.to_dict()))

            # è®°å½•é”™è¯¯å‘Šè­¦
            self.record_alert(
                "task_failed",
                {
                    "task_id": task_id,
                    "worker_id": metrics.worker_id,
                    "error": error_info,
                    "duration": metrics.duration,
                },
            )

            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self.update_performance_stats(metrics)

            logger.warning(f"ä»»åŠ¡å¤±è´¥ç›‘æ§: {task_id}")
            return True

        except Exception as e:
            logger.error(f"ä»»åŠ¡å¤±è´¥ç›‘æ§å¤±è´¥: {e}")
            return False

    def check_task_alerts(self, metrics: TaskMetrics):
        """æ£€æŸ¥ä»»åŠ¡å‘Šè­¦"""
        current_time = time.time()

        # æ£€æŸ¥ä»»åŠ¡æ‰§è¡Œæ—¶é—´
        if (
            metrics.start_time
            and current_time - metrics.start_time
            > self.alert_thresholds["task_duration_max"]
        ):
            self.record_alert(
                "task_timeout",
                {
                    "task_id": metrics.task_id,
                    "worker_id": metrics.worker_id,
                    "duration": current_time - metrics.start_time,
                },
            )

        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        if metrics.memory_usage > self.alert_thresholds["memory_usage_max"]:
            self.record_alert(
                "high_memory_usage",
                {
                    "task_id": metrics.task_id,
                    "worker_id": metrics.worker_id,
                    "memory_usage": metrics.memory_usage,
                },
            )

        # æ£€æŸ¥CPUä½¿ç”¨ç‡
        if metrics.cpu_usage > self.alert_thresholds["cpu_usage_max"]:
            self.record_alert(
                "high_cpu_usage",
                {
                    "task_id": metrics.task_id,
                    "worker_id": metrics.worker_id,
                    "cpu_usage": metrics.cpu_usage,
                },
            )

    def record_alert(self, alert_type: str, alert_data: Dict):
        """è®°å½•å‘Šè­¦"""
        if not self.redis:
            return

        try:
            alert = {
                "type": alert_type,
                "data": alert_data,
                "timestamp": time.time(),
                "severity": self.get_alert_severity(alert_type),
            }

            # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºé”®ç¡®ä¿å”¯ä¸€æ€§
            alert_key = f"{alert_type}:{int(time.time() * 1000)}"

            self.redis.hset(self.alerts_key, alert_key, json.dumps(alert))

            # è®¾ç½®å‘Šè­¦è¿‡æœŸæ—¶é—´ï¼ˆ7å¤©ï¼‰
            self.redis.expire(self.alerts_key, 7 * 24 * 3600)

            logger.warning(f"è®°å½•å‘Šè­¦: {alert_type} - {alert_data}")

        except Exception as e:
            logger.error(f"è®°å½•å‘Šè­¦å¤±è´¥: {e}")

    def get_alert_severity(self, alert_type: str) -> str:
        """è·å–å‘Šè­¦ä¸¥é‡ç¨‹åº¦"""
        severity_map = {
            "task_timeout": "high",
            "task_failed": "medium",
            "high_memory_usage": "medium",
            "high_cpu_usage": "low",
            "queue_overflow": "high",
        }
        return severity_map.get(alert_type, "low")

    def update_performance_stats(self, metrics: TaskMetrics):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        if not self.redis:
            return

        try:
            # è·å–ç°æœ‰ç»Ÿè®¡
            stats_data = self.redis.get(self.performance_key)
            if stats_data:
                stats = json.loads(stats_data)
            else:
                stats = {
                    "total_tasks": 0,
                    "completed_tasks": 0,
                    "failed_tasks": 0,
                    "total_duration": 0.0,
                    "total_items": 0,
                    "total_pages": 0,
                    "avg_duration": 0.0,
                    "success_rate": 0.0,
                    "throughput": 0.0,
                    "last_updated": time.time(),
                }

            # æ›´æ–°ç»Ÿè®¡
            stats["total_tasks"] += 1
            if metrics.status == "completed":
                stats["completed_tasks"] += 1
            elif metrics.status == "failed":
                stats["failed_tasks"] += 1

            if metrics.duration:
                stats["total_duration"] += metrics.duration

            stats["total_items"] += metrics.items_scraped
            stats["total_pages"] += metrics.pages_crawled

            # è®¡ç®—å¹³å‡å€¼å’Œæ¯”ç‡
            if stats["total_tasks"] > 0:
                stats["avg_duration"] = stats["total_duration"] / stats["total_tasks"]
                stats["success_rate"] = stats["completed_tasks"] / stats["total_tasks"]

            # è®¡ç®—ååé‡ï¼ˆä»»åŠ¡/å°æ—¶ï¼‰
            time_diff = time.time() - stats["last_updated"]
            if time_diff > 0:
                stats["throughput"] = stats["total_tasks"] / (time_diff / 3600)

            stats["last_updated"] = time.time()

            # ä¿å­˜ç»Ÿè®¡
            self.redis.set(self.performance_key, json.dumps(stats))

        except Exception as e:
            logger.error(f"æ›´æ–°æ€§èƒ½ç»Ÿè®¡å¤±è´¥: {e}")

    def update_hourly_stats(self, metrics: TaskMetrics):
        """æ›´æ–°å°æ—¶ç»Ÿè®¡"""
        if not self.redis:
            return

        try:
            # è·å–å½“å‰å°æ—¶
            current_hour = datetime.now().strftime("%Y-%m-%d-%H")
            hour_key = f"{self.hourly_stats_key}:{current_hour}"

            # è·å–å°æ—¶ç»Ÿè®¡
            hour_stats_data = self.redis.get(hour_key)
            if hour_stats_data:
                hour_stats = json.loads(hour_stats_data)
            else:
                hour_stats = {
                    "hour": current_hour,
                    "tasks_count": 0,
                    "completed_count": 0,
                    "failed_count": 0,
                    "total_duration": 0.0,
                    "total_items": 0,
                    "worker_stats": defaultdict(int),
                }

            # æ›´æ–°å°æ—¶ç»Ÿè®¡
            hour_stats["tasks_count"] += 1
            if metrics.status == "completed":
                hour_stats["completed_count"] += 1
            elif metrics.status == "failed":
                hour_stats["failed_count"] += 1

            if metrics.duration:
                hour_stats["total_duration"] += metrics.duration

            hour_stats["total_items"] += metrics.items_scraped
            hour_stats["worker_stats"][metrics.worker_id] += 1

            # ä¿å­˜å°æ—¶ç»Ÿè®¡
            self.redis.set(hour_key, json.dumps(hour_stats))
            self.redis.expire(hour_key, 30 * 24 * 3600)  # ä¿ç•™30å¤©

        except Exception as e:
            logger.error(f"æ›´æ–°å°æ—¶ç»Ÿè®¡å¤±è´¥: {e}")

    def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.redis:
            return {}

        try:
            stats_data = self.redis.get(self.performance_key)
            if stats_data:
                return json.loads(stats_data)
            return {}

        except Exception as e:
            logger.error(f"è·å–æ€§èƒ½ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    def get_recent_alerts(self, hours: int = 24) -> List[Dict]:
        """è·å–æœ€è¿‘çš„å‘Šè­¦"""
        if not self.redis:
            return []

        try:
            alerts_data = self.redis.hgetall(self.alerts_key)
            recent_alerts = []
            cutoff_time = time.time() - (hours * 3600)

            for alert_key, alert_data in alerts_data.items():
                alert = json.loads(alert_data)
                if alert["timestamp"] > cutoff_time:
                    recent_alerts.append(alert)

            # æŒ‰æ—¶é—´æ’åº
            recent_alerts.sort(key=lambda x: x["timestamp"], reverse=True)
            return recent_alerts

        except Exception as e:
            logger.error(f"è·å–æœ€è¿‘å‘Šè­¦å¤±è´¥: {e}")
            return []

    def get_hourly_stats(self, hours: int = 24) -> List[Dict]:
        """è·å–å°æ—¶ç»Ÿè®¡"""
        if not self.redis:
            return []

        try:
            hourly_stats = []
            current_time = datetime.now()

            for i in range(hours):
                hour_time = current_time - timedelta(hours=i)
                hour_key = (
                    f"{self.hourly_stats_key}:{hour_time.strftime('%Y-%m-%d-%H')}"
                )

                stats_data = self.redis.get(hour_key)
                if stats_data:
                    stats = json.loads(stats_data)
                    hourly_stats.append(stats)
                else:
                    # å¡«å……ç©ºæ•°æ®
                    hourly_stats.append(
                        {
                            "hour": hour_time.strftime("%Y-%m-%d-%H"),
                            "tasks_count": 0,
                            "completed_count": 0,
                            "failed_count": 0,
                            "total_duration": 0.0,
                            "total_items": 0,
                            "worker_stats": {},
                        }
                    )

            return list(reversed(hourly_stats))

        except Exception as e:
            logger.error(f"è·å–å°æ—¶ç»Ÿè®¡å¤±è´¥: {e}")
            return []

    def get_worker_performance(self, worker_id: str = None) -> Dict:
        """è·å–å·¥ä½œèŠ‚ç‚¹æ€§èƒ½"""
        if not self.redis:
            return {}

        try:
            # è·å–æ‰€æœ‰ä»»åŠ¡æŒ‡æ ‡
            all_metrics = self.redis.hgetall(self.metrics_key)
            worker_stats = defaultdict(
                lambda: {
                    "total_tasks": 0,
                    "completed_tasks": 0,
                    "failed_tasks": 0,
                    "total_duration": 0.0,
                    "avg_duration": 0.0,
                    "success_rate": 0.0,
                    "total_items": 0,
                }
            )

            for task_id, metrics_data in all_metrics.items():
                metrics = TaskMetrics.from_dict(json.loads(metrics_data))

                if worker_id and metrics.worker_id != worker_id:
                    continue

                stats = worker_stats[metrics.worker_id]
                stats["total_tasks"] += 1

                if metrics.status == "completed":
                    stats["completed_tasks"] += 1
                elif metrics.status == "failed":
                    stats["failed_tasks"] += 1

                if metrics.duration:
                    stats["total_duration"] += metrics.duration

                stats["total_items"] += metrics.items_scraped

            # è®¡ç®—å¹³å‡å€¼å’Œæ¯”ç‡
            for wid, stats in worker_stats.items():
                if stats["total_tasks"] > 0:
                    stats["avg_duration"] = (
                        stats["total_duration"] / stats["total_tasks"]
                    )
                    stats["success_rate"] = (
                        stats["completed_tasks"] / stats["total_tasks"]
                    )

            if worker_id:
                return worker_stats.get(worker_id, {})
            else:
                return dict(worker_stats)

        except Exception as e:
            logger.error(f"è·å–å·¥ä½œèŠ‚ç‚¹æ€§èƒ½å¤±è´¥: {e}")
            return {}

    def cleanup_old_metrics(self, days: int = 7):
        """æ¸…ç†æ—§çš„æŒ‡æ ‡æ•°æ®"""
        if not self.redis:
            return

        try:
            cutoff_time = time.time() - (days * 24 * 3600)
            all_metrics = self.redis.hgetall(self.metrics_key)

            removed_count = 0
            for task_id, metrics_data in all_metrics.items():
                metrics = TaskMetrics.from_dict(json.loads(metrics_data))
                if metrics.start_time < cutoff_time:
                    self.redis.hdel(self.metrics_key, task_id)
                    removed_count += 1

            logger.info(f"æ¸…ç†äº† {removed_count} ä¸ªæ—§çš„ä»»åŠ¡æŒ‡æ ‡")

        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æŒ‡æ ‡å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•ä»»åŠ¡ç›‘æ§å™¨"""
    print("ğŸ“Š ä»»åŠ¡ç›‘æ§å™¨æµ‹è¯•")
    print("=" * 60)

    if not REDIS_AVAILABLE:
        print("âŒ Redisä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install redis")
        return False

    # åˆ›å»ºç›‘æ§å™¨
    monitor = TaskMonitor()

    if not monitor.redis:
        print("âŒ Redisè¿æ¥å¤±è´¥")
        return False

    # æ¨¡æ‹Ÿä»»åŠ¡ç›‘æ§
    test_tasks = [
        {"task_id": "task_001", "worker_id": "worker_001"},
        {"task_id": "task_002", "worker_id": "worker_002"},
        {"task_id": "task_003", "worker_id": "worker_001"},
    ]

    print("ğŸš€ å¼€å§‹ç›‘æ§æµ‹è¯•ä»»åŠ¡...")
    for task in test_tasks:
        success = monitor.start_task_monitoring(task["task_id"], task["worker_id"])
        print(f"   {task['task_id']}: {'âœ…' if success else 'âŒ'}")

    # æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œå’ŒæŒ‡æ ‡æ›´æ–°
    print("\nğŸ“ˆ æ›´æ–°ä»»åŠ¡æŒ‡æ ‡...")
    for i, task in enumerate(test_tasks):
        metrics_update = {
            "items_scraped": 10 + i * 5,
            "pages_crawled": 2 + i,
            "memory_usage": 100 + i * 50,
            "cpu_usage": 30 + i * 20,
        }
        success = monitor.update_task_metrics(task["task_id"], metrics_update)
        print(f"   {task['task_id']}: {'âœ…' if success else 'âŒ'}")

    # æ¨¡æ‹Ÿä»»åŠ¡å®Œæˆ
    print("\nâœ… å®Œæˆä»»åŠ¡ç›‘æ§...")
    for i, task in enumerate(test_tasks):
        if i < 2:  # å‰ä¸¤ä¸ªä»»åŠ¡æˆåŠŸ
            final_metrics = {"items_scraped": 15 + i * 5}
            success = monitor.complete_task_monitoring(task["task_id"], final_metrics)
        else:  # æœ€åä¸€ä¸ªä»»åŠ¡å¤±è´¥
            error_info = {"error": "Connection timeout", "code": "TIMEOUT"}
            success = monitor.fail_task_monitoring(task["task_id"], error_info)

        print(f"   {task['task_id']}: {'âœ…' if success else 'âŒ'}")

    # è·å–æ€§èƒ½ç»Ÿè®¡
    print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    stats = monitor.get_performance_stats()
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.2f}")
        else:
            print(f"   {key}: {value}")

    # è·å–å‘Šè­¦ä¿¡æ¯
    print("\nğŸš¨ æœ€è¿‘å‘Šè­¦:")
    alerts = monitor.get_recent_alerts(hours=1)
    if alerts:
        for alert in alerts[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            print(f"   {alert['type']}: {alert['data']}")
    else:
        print("   æ— å‘Šè­¦")

    # è·å–å·¥ä½œèŠ‚ç‚¹æ€§èƒ½
    print("\nğŸ‘¥ å·¥ä½œèŠ‚ç‚¹æ€§èƒ½:")
    worker_perf = monitor.get_worker_performance()
    for worker_id, perf in worker_perf.items():
        print(f"   {worker_id}:")
        print(f"      æ€»ä»»åŠ¡: {perf['total_tasks']}")
        print(f"      æˆåŠŸç‡: {perf['success_rate']:.2%}")
        print(f"      å¹³å‡è€—æ—¶: {perf['avg_duration']:.2f}ç§’")

    print("\nâœ… ä»»åŠ¡ç›‘æ§å™¨æµ‹è¯•å®Œæˆ")
    return True


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
