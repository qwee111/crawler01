#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦å™¨

å®ç°åˆ†å¸ƒå¼çˆ¬è™«ä»»åŠ¡çš„è°ƒåº¦ã€åˆ†å‘å’Œç®¡ç†
"""

import hashlib
import json
import logging
import time
from dataclasses import asdict, dataclass
from enum import Enum
from typing import Dict, List, Optional

try:
    import redis

    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§"""

    LOW = 1
    NORMAL = 2
    HIGH = 3
    URGENT = 4


class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€"""

    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    RETRYING = "retrying"


@dataclass
class CrawlTask:
    """çˆ¬è™«ä»»åŠ¡æ•°æ®ç»“æ„"""

    spider_name: str
    url: str
    priority: TaskPriority
    site_config: Dict
    task_id: str = None
    retry_count: int = 0
    max_retries: int = 3
    created_at: float = None
    scheduled_at: float = None
    status: TaskStatus = TaskStatus.PENDING
    metadata: Dict = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()
        if self.task_id is None:
            self.task_id = self.generate_task_id()
        if self.metadata is None:
            self.metadata = {}

    def generate_task_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€ä»»åŠ¡ID"""
        # å¦‚æœ url ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨ site_name ä½œä¸ºä»»åŠ¡ ID çš„ä¸€éƒ¨åˆ†
        identifier = self.url if self.url else self.site_config.get("site", "default")
        content = f"{self.spider_name}:{identifier}:{self.created_at}"
        return hashlib.md5(content.encode()).hexdigest()

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        data = asdict(self)
        data["priority"] = self.priority.value
        data["status"] = self.status.value
        return data

    @classmethod
    def from_dict(cls, data: Dict) -> "CrawlTask":
        """ä»å­—å…¸åˆ›å»ºä»»åŠ¡"""
        data["priority"] = TaskPriority(data["priority"])
        data["status"] = TaskStatus(data["status"])
        return cls(**data)


class DistributedTaskScheduler:
    """åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦å™¨"""

    def __init__(self, redis_url="redis://localhost:6379/0"):
        self.redis_url = redis_url
        self.redis = None

        # Redisé”®å
        self.task_queue_key = "crawler:task_queue"
        self.processing_key = "crawler:processing"
        self.completed_key = "crawler:completed"
        self.failed_key = "crawler:failed"
        self.stats_key = "crawler:stats"

        # åˆå§‹åŒ–Redisè¿æ¥
        self.connect_redis()

        logger.info("åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")

    def connect_redis(self) -> bool:
        """è¿æ¥Redis"""
        if not REDIS_AVAILABLE:
            logger.error("Redisä¸å¯ç”¨ï¼Œè¯·å®‰è£…redis-py")
            return False

        try:
            self.redis = redis.from_url(self.redis_url)
            # æµ‹è¯•è¿æ¥
            self.redis.ping()
            logger.info("Redisè¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"Redisè¿æ¥å¤±è´¥: {e}")
            return False

    def submit_task(self, task: CrawlTask) -> bool:
        """æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        if not self.redis:
            logger.error("Redisæœªè¿æ¥")
            return False

        try:
            # å¦‚æœ url ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨ site_name ä½œä¸ºä»»åŠ¡çš„æ ‡è¯†
            if not task.url:
                task.url = task.site_config.get("site", "default")
                task.task_id = task.generate_task_id()  # é‡æ–°ç”Ÿæˆä»»åŠ¡IDä»¥åæ˜ æ–°çš„url

            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å­˜åœ¨
            if self.is_task_exists(task.task_id):
                logger.warning(f"ä»»åŠ¡å·²å­˜åœ¨: {task.task_id}")
                return False

            # æ ¹æ®ä¼˜å…ˆçº§åˆ†é…åˆ°ä¸åŒé˜Ÿåˆ—
            queue_key = f"{self.task_queue_key}:{task.priority.name.lower()}"

            # åºåˆ—åŒ–ä»»åŠ¡
            task_data = json.dumps(task.to_dict())

            # æ·»åŠ åˆ°Redisé˜Ÿåˆ—
            self.redis.lpush(queue_key, task_data)

            # æ›´æ–°ç»Ÿè®¡
            self.update_stats("tasks_submitted", 1)

            logger.info(f"ä»»åŠ¡æäº¤æˆåŠŸ: {task.task_id} (ä¼˜å…ˆçº§: {task.priority.name})")
            return True

        except Exception as e:
            logger.error(f"æäº¤ä»»åŠ¡å¤±è´¥: {e}")
            return False

    def get_next_task(self, worker_id: str) -> Optional[CrawlTask]:
        """è·å–ä¸‹ä¸€ä¸ªä»»åŠ¡"""
        if not self.redis:
            return None

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥é˜Ÿåˆ—
        priority_queues = [
            f"{self.task_queue_key}:urgent",
            f"{self.task_queue_key}:high",
            f"{self.task_queue_key}:normal",
            f"{self.task_queue_key}:low",
        ]

        for queue_key in priority_queues:
            try:
                # éé˜»å¡è·å–ä»»åŠ¡
                task_data = self.redis.rpop(queue_key)
                if task_data:
                    task_dict = json.loads(task_data)
                    task = CrawlTask.from_dict(task_dict)

                    # æ ‡è®°ä»»åŠ¡ä¸ºå¤„ç†ä¸­
                    self.mark_task_processing(task, worker_id)

                    logger.info(f"åˆ†é…ä»»åŠ¡ç»™å·¥ä½œèŠ‚ç‚¹ {worker_id}: {task.task_id}")
                    return task

            except Exception as e:
                logger.error(f"è·å–ä»»åŠ¡å¤±è´¥: {e}")
                continue

        return None

    def mark_task_processing(self, task: CrawlTask, worker_id: str):
        """æ ‡è®°ä»»åŠ¡ä¸ºå¤„ç†ä¸­"""
        if not self.redis:
            return

        task.status = TaskStatus.PROCESSING
        processing_data = {
            "task": task.to_dict(),
            "worker_id": worker_id,
            "start_time": time.time(),
        }

        self.redis.hset(self.processing_key, task.task_id, json.dumps(processing_data))

        # æ›´æ–°ç»Ÿè®¡
        self.update_stats("tasks_processing", 1)

    def complete_task(self, task_id: str, result: Dict):
        """å®Œæˆä»»åŠ¡"""
        if not self.redis:
            return

        try:
            # ä»å¤„ç†ä¸­ç§»é™¤
            processing_data = self.redis.hget(self.processing_key, task_id)
            if processing_data:
                self.redis.hdel(self.processing_key, task_id)
                self.update_stats("tasks_processing", -1)

            # æ·»åŠ åˆ°å®Œæˆé˜Ÿåˆ—
            completion_data = {
                "task_id": task_id,
                "result": result,
                "completed_at": time.time(),
            }

            self.redis.hset(self.completed_key, task_id, json.dumps(completion_data))

            # æ›´æ–°ç»Ÿè®¡
            self.update_stats("tasks_completed", 1)

            logger.info(f"ä»»åŠ¡å®Œæˆ: {task_id}")

        except Exception as e:
            logger.error(f"å®Œæˆä»»åŠ¡å¤±è´¥: {e}")

    def fail_task(self, task_id: str, error: str, retry: bool = True):
        """ä»»åŠ¡å¤±è´¥å¤„ç†"""
        if not self.redis:
            return

        try:
            processing_data = self.redis.hget(self.processing_key, task_id)
            if not processing_data:
                logger.warning(f"æœªæ‰¾åˆ°å¤„ç†ä¸­çš„ä»»åŠ¡: {task_id}")
                return

            processing_info = json.loads(processing_data)
            task_dict = processing_info["task"]
            task = CrawlTask.from_dict(task_dict)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
            if retry and task.retry_count < task.max_retries:
                task.retry_count += 1
                task.status = TaskStatus.RETRYING
                # æŒ‡æ•°é€€é¿ç­–ç•¥
                delay = (2**task.retry_count) * 60
                task.scheduled_at = time.time() + delay

                # é‡æ–°æäº¤ä»»åŠ¡
                self.submit_task(task)
                logger.info(f"ä»»åŠ¡é‡è¯•: {task_id} (ç¬¬{task.retry_count}æ¬¡)")
            else:
                # æ ‡è®°ä¸ºæœ€ç»ˆå¤±è´¥
                task.status = TaskStatus.FAILED
                failure_data = {
                    "task": task.to_dict(),
                    "error": error,
                    "failed_at": time.time(),
                    "retry_count": task.retry_count,
                }

                self.redis.hset(self.failed_key, task_id, json.dumps(failure_data))

                # æ›´æ–°ç»Ÿè®¡
                self.update_stats("tasks_failed", 1)
                logger.error(f"ä»»åŠ¡æœ€ç»ˆå¤±è´¥: {task_id} - {error}")

            # ä»å¤„ç†ä¸­ç§»é™¤
            self.redis.hdel(self.processing_key, task_id)
            self.update_stats("tasks_processing", -1)

        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")

    def is_task_exists(self, task_id: str) -> bool:
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å­˜åœ¨"""
        if not self.redis:
            return False

        # æ£€æŸ¥å„ä¸ªçŠ¶æ€çš„ä»»åŠ¡
        return (
            self.redis.hexists(self.processing_key, task_id)
            or self.redis.hexists(self.completed_key, task_id)
            or self.redis.hexists(self.failed_key, task_id)
        )

    def get_queue_size(self, priority: TaskPriority = None) -> int:
        """è·å–é˜Ÿåˆ—å¤§å°"""
        if not self.redis:
            return 0

        if priority:
            queue_key = f"{self.task_queue_key}:{priority.name.lower()}"
            return self.redis.llen(queue_key)
        else:
            # è¿”å›æ‰€æœ‰é˜Ÿåˆ—çš„æ€»å¤§å°
            total = 0
            for p in TaskPriority:
                queue_key = f"{self.task_queue_key}:{p.name.lower()}"
                total += self.redis.llen(queue_key)
            return total

    def get_stats(self) -> Dict:
        """è·å–è°ƒåº¦å™¨ç»Ÿè®¡ä¿¡æ¯"""
        if not self.redis:
            return {}

        try:
            stats = {}
            for key in [
                "tasks_submitted",
                "tasks_processing",
                "tasks_completed",
                "tasks_failed",
            ]:
                value = self.redis.hget(self.stats_key, key)
                stats[key] = int(value) if value else 0

            # æ·»åŠ é˜Ÿåˆ—å¤§å°ä¿¡æ¯
            stats["queue_sizes"] = {}
            for priority in TaskPriority:
                queue_key = f"{self.task_queue_key}:{priority.name.lower()}"
                stats["queue_sizes"][priority.name.lower()] = self.redis.llen(queue_key)

            stats["total_queue_size"] = sum(stats["queue_sizes"].values())
            stats["processing_count"] = self.redis.hlen(self.processing_key)

            return stats

        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def update_stats(self, key: str, increment: int):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        if not self.redis:
            return

        try:
            self.redis.hincrby(self.stats_key, key, increment)
        except Exception as e:
            logger.error(f"æ›´æ–°ç»Ÿè®¡å¤±è´¥: {e}")

    def clear_completed_tasks(self, older_than_hours: int = 24):
        """æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡"""
        if not self.redis:
            return

        try:
            cutoff_time = time.time() - (older_than_hours * 3600)
            completed_tasks = self.redis.hgetall(self.completed_key)

            removed_count = 0
            for task_id, task_data in completed_tasks.items():
                task_info = json.loads(task_data)
                if task_info.get("completed_at", 0) < cutoff_time:
                    self.redis.hdel(self.completed_key, task_id)
                    removed_count += 1

            logger.info(f"æ¸…ç†äº† {removed_count} ä¸ªå·²å®Œæˆçš„ä»»åŠ¡")

        except Exception as e:
            logger.error(f"æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")

    def get_task_status(self, task_id: str) -> Optional[Dict]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        if not self.redis:
            return None

        # æ£€æŸ¥å¤„ç†ä¸­çš„ä»»åŠ¡
        processing_data = self.redis.hget(self.processing_key, task_id)
        if processing_data:
            return json.loads(processing_data)

        # æ£€æŸ¥å·²å®Œæˆçš„ä»»åŠ¡
        completed_data = self.redis.hget(self.completed_key, task_id)
        if completed_data:
            return json.loads(completed_data)

        # æ£€æŸ¥å¤±è´¥çš„ä»»åŠ¡
        failed_data = self.redis.hget(self.failed_key, task_id)
        if failed_data:
            return json.loads(failed_data)

        return None

    def submit_ai_report_task(self, site_name: str, days_ago: int = 7, priority: TaskPriority = TaskPriority.NORMAL) -> bool:
        """
        æäº¤AIæŠ¥å‘Šç”Ÿæˆä»»åŠ¡åˆ°é˜Ÿåˆ—ã€‚
        """
        if not self.redis:
            logger.error("Redisæœªè¿æ¥")
            return False

        # AIæŠ¥å‘Šç”Ÿæˆä»»åŠ¡çš„spider_nameå›ºå®šä¸º 'ai_report_generator'
        # url ä½¿ç”¨ site_name ä½œä¸ºå”¯ä¸€æ ‡è¯†
        task = CrawlTask(
            spider_name="ai_report_generator",
            url=site_name,  # ä½¿ç”¨ç«™ç‚¹åç§°ä½œä¸ºURLï¼Œæ–¹ä¾¿è¯†åˆ«
            priority=priority,
            site_config={"site": site_name, "days_ago": days_ago},
            metadata={"task_type": "ai_report_generation", "site_name": site_name, "days_ago": days_ago}
        )
        
        return self.submit_task(task)

    def submit_bochaai_task(self, priority: TaskPriority = TaskPriority.NORMAL) -> bool:
        """æäº¤ bochaai_spider ä»»åŠ¡ï¼ˆæ— éœ€URLä¸é™„åŠ å‚æ•°ï¼‰ã€‚"""
        if not self.redis:
            logger.error("Redisæœªè¿æ¥")
            return False

        task = CrawlTask(
            spider_name="bochaai_spider",
            url="",  # ä¸éœ€è¦ URL
            priority=priority,
            site_config={},
            metadata={"task_type": "bochaai"},
        )
        return self.submit_task(task)


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•è°ƒåº¦å™¨"""
    print("ğŸš€ åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦å™¨æµ‹è¯•")
    print("=" * 60)

    if not REDIS_AVAILABLE:
        print("âŒ Redisä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install redis")
        return False

    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = DistributedTaskScheduler()

    if not scheduler.redis:
        print("âŒ Redisè¿æ¥å¤±è´¥")
        return False

    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    test_tasks = [
        CrawlTask(
            spider_name="adaptive",
            url="https://www.bjcdc.org/cdcmodule/jkdt/bsxw/index.shtml",
            priority=TaskPriority.HIGH,
            site_config={"site": "bjcdc"},
        ),
        CrawlTask(
            spider_name="adaptive",
            url="https://www.bjcdc.org/cdcmodule/jkdt/jcdt/index.shtml",
            priority=TaskPriority.NORMAL,
            site_config={"site": "bjcdc"},
        ),
        CrawlTask(
            spider_name="adaptive",
            url="https://www.bjcdc.org/cdcmodule/jkdt/yqbb/index.shtml",
            priority=TaskPriority.URGENT,
            site_config={"site": "bjcdc"},
        ),
    ]

    # æäº¤çˆ¬è™«ä»»åŠ¡
    print("ğŸ“¤ æäº¤æµ‹è¯•çˆ¬è™«ä»»åŠ¡...")
    for task in test_tasks:
        success = scheduler.submit_task(task)
        print(f"   çˆ¬è™«ä»»åŠ¡ {task.task_id[:8]}... : {'âœ…' if success else 'âŒ'}")

    # æäº¤AIæŠ¥å‘Šç”Ÿæˆä»»åŠ¡
    print("\nğŸ“¤ æäº¤AIæŠ¥å‘Šç”Ÿæˆä»»åŠ¡...")
    ai_report_site = "jxcdc" # ç¤ºä¾‹ç«™ç‚¹
    ai_report_task_success = scheduler.submit_ai_report_task(ai_report_site, days_ago=7, priority=TaskPriority.HIGH)
    print(f"   AIæŠ¥å‘Šä»»åŠ¡ ({ai_report_site}) : {'âœ…' if ai_report_task_success else 'âŒ'}")

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š è°ƒåº¦å™¨ç»Ÿè®¡:")
    stats = scheduler.get_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")

    # æ¨¡æ‹Ÿå·¥ä½œèŠ‚ç‚¹è·å–ä»»åŠ¡
    print("\nğŸ”„ æ¨¡æ‹Ÿå·¥ä½œèŠ‚ç‚¹è·å–ä»»åŠ¡...")
    worker_id = "test_worker_001"

    for i in range(3):
        task = scheduler.get_next_task(worker_id)
        if task:
            print(f"   è·å–ä»»åŠ¡: {task.task_id[:8]}... (ä¼˜å…ˆçº§: {task.priority.name})")

            # æ¨¡æ‹Ÿä»»åŠ¡å®Œæˆ
            result = {"status": "success", "items_count": 10}
            scheduler.complete_task(task.task_id, result)
            print(f"   å®Œæˆä»»åŠ¡: {task.task_id[:8]}...")
        else:
            print("   æ²¡æœ‰æ›´å¤šä»»åŠ¡")
            break

    # æœ€ç»ˆç»Ÿè®¡
    print("\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
    final_stats = scheduler.get_stats()
    for key, value in final_stats.items():
        print(f"   {key}: {value}")

    print("\nâœ… è°ƒåº¦å™¨æµ‹è¯•å®Œæˆ")
    return True


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
