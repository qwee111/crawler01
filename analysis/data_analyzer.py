#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åˆ†æå¼•æ“

å¯¹çˆ¬å–çš„ç–¾ç—…é˜²æ§æ•°æ®è¿›è¡Œæ™ºèƒ½åˆ†æ
"""

import json
import logging
import os
import re
import sys
from collections import Counter, defaultdict

# import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import pymongo

    MONGO_AVAILABLE = True
except ImportError:
    MONGO_AVAILABLE = False

try:
    import jieba
    import jieba.analyse

    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False

try:
    import matplotlib.pyplot as plt
    import seaborn as sns

    plt.rcParams["font.sans-serif"] = ["SimHei", "Microsoft YaHei"]  # æ”¯æŒä¸­æ–‡
    plt.rcParams["axes.unicode_minus"] = False
    PLOT_AVAILABLE = True
except ImportError:
    PLOT_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DiseaseDataAnalyzer:
    """ç–¾ç—…æ•°æ®åˆ†æå™¨"""

    def __init__(
        self,
        mongo_uri="mongodb://admin:password123@localhost:27017/",
        db_name="crawler_db",
    ):
        self.mongo_uri = mongo_uri
        self.db_name = db_name
        self.client = None
        self.db = None

        # ç–¾ç—…å…³é”®è¯åˆ†ç±»
        self.disease_categories = {
            "å‘¼å¸é“ç–¾ç—…": ["æµæ„Ÿ", "æ–°å† ", "COVID", "è‚ºç‚", "å’³å—½", "å‘çƒ­", "æ„Ÿå†’", "å“®å–˜"],
            "æ¶ˆåŒ–é“ç–¾ç—…": ["è…¹æ³»", "è¯ºå¦‚", "è½®çŠ¶", "é£Ÿç‰©ä¸­æ¯’", "è‚ ç‚", "èƒƒç‚"],
            "ä¼ æŸ“ç—…": ["ç»“æ ¸", "è‰¾æ»‹", "HIV", "è‚ç‚", "æ‰‹è¶³å£", "éº»ç–¹", "æ°´ç—˜", "æµè„‘"],
            "æ…¢æ€§ç—…": ["ç³–å°¿ç—…", "é«˜è¡€å‹", "å¿ƒè„ç—…", "ç™Œç—‡", "è‚¿ç˜¤", "æ…¢ç—…"],
            "ç–«è‹—ç›¸å…³": ["ç–«è‹—", "æ¥ç§", "å…ç–«", "é¢„é˜²æ¥ç§"],
            "å…¬å…±å«ç”Ÿ": ["ç›‘æµ‹", "é¢„è­¦", "é˜²æ§", "æ¶ˆæ¯’", "éš”ç¦»", "åº”æ€¥", "å«ç”Ÿ"],
        }

        # åœ°åŒºä¿¡æ¯
        self.beijing_districts = [
            "æœé˜³",
            "æµ·æ·€",
            "ä¸°å°",
            "çŸ³æ™¯å±±",
            "é—¨å¤´æ²Ÿ",
            "æˆ¿å±±",
            "é€šå·",
            "é¡ºä¹‰",
            "æ˜Œå¹³",
            "å¤§å…´",
            "æ€€æŸ”",
            "å¹³è°·",
            "å¯†äº‘",
            "å»¶åº†",
            "ä¸œåŸ",
            "è¥¿åŸ",
        ]

        self.connect_database()

    def connect_database(self):
        """è¿æ¥æ•°æ®åº“"""
        if not MONGO_AVAILABLE:
            logger.warning("pymongoæœªå®‰è£…ï¼Œæ— æ³•è¿æ¥MongoDB")
            return False

        try:
            self.client = pymongo.MongoClient(self.mongo_uri)
            self.db = self.client[self.db_name]
            # æµ‹è¯•è¿æ¥
            self.client.admin.command("ping")
            logger.info("MongoDBè¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"MongoDBè¿æ¥å¤±è´¥: {e}")
            return False

    def load_data_from_mongodb(self, collection_name="bjcdc_data", limit=None):
        """ä»MongoDBåŠ è½½æ•°æ®"""
        if self.db is None:
            logger.error("æ•°æ®åº“æœªè¿æ¥")
            return []

        try:
            collection = self.db[collection_name]
            query = {}

            if limit:
                cursor = collection.find(query).limit(limit)
            else:
                cursor = collection.find(query)

            data = list(cursor)
            logger.info(f"ä»MongoDBåŠ è½½äº† {len(data)} æ¡æ•°æ®")
            return data
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return []

    def load_data_from_json(self, json_file_path):
        """ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                data = []
                for line in f:
                    line = line.strip()
                    if line:
                        data.append(json.loads(line))

            logger.info(f"ä»JSONæ–‡ä»¶åŠ è½½äº† {len(data)} æ¡æ•°æ®")
            return data
        except Exception as e:
            logger.error(f"JSONæ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return []

    def extract_disease_keywords(self, text):
        """æå–ç–¾ç—…å…³é”®è¯"""
        if not text:
            return []

        found_keywords = []
        text_lower = text.lower()

        for category, keywords in self.disease_categories.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    found_keywords.append({"keyword": keyword, "category": category})

        return found_keywords

    def extract_location_info(self, text):
        """æå–åœ°åŒºä¿¡æ¯"""
        if not text:
            return []

        found_locations = []
        for district in self.beijing_districts:
            if district in text:
                found_locations.append(district)

        return found_locations

    def analyze_time_trends(self, data):
        """åˆ†ææ—¶é—´è¶‹åŠ¿"""
        logger.info("åˆ†ææ—¶é—´è¶‹åŠ¿...")

        # æŒ‰æ—¥æœŸç»Ÿè®¡
        date_counts = defaultdict(int)
        disease_by_date = defaultdict(lambda: defaultdict(int))

        for item in data:
            # æå–æ—¥æœŸ
            date_str = None
            if "publish_dates" in item and item["publish_dates"]:
                if isinstance(item["publish_dates"], list):
                    date_str = item["publish_dates"][0]
                else:
                    date_str = item["publish_dates"]
            elif "crawl_timestamp" in item:
                date_str = item["crawl_timestamp"][:10]  # å–æ—¥æœŸéƒ¨åˆ†

            if date_str:
                # æ¸…ç†æ—¥æœŸæ ¼å¼
                date_str = re.sub(r"[ã€ã€‘\[\]]", "", date_str)
                try:
                    if len(date_str) == 10 and "-" in date_str:
                        date_counts[date_str] += 1

                        # åˆ†æç–¾ç—…ç±»å‹
                        text = str(item.get("news_titles", "")) + str(
                            item.get("title", "")
                        )
                        keywords = self.extract_disease_keywords(text)
                        for kw in keywords:
                            disease_by_date[date_str][kw["category"]] += 1
                except:
                    continue

        # è½¬æ¢ä¸ºæ—¶é—´åºåˆ—
        if date_counts:
            dates = sorted(date_counts.keys())
            trend_data = {
                "dates": dates,
                "counts": [date_counts[date] for date in dates],
                "disease_trends": dict(disease_by_date),
            }

            logger.info(f"æ—¶é—´è¶‹åŠ¿åˆ†æå®Œæˆï¼Œè¦†ç›– {len(dates)} å¤©")
            return trend_data

        return {}

    def analyze_disease_distribution(self, data):
        """åˆ†æç–¾ç—…åˆ†å¸ƒ"""
        logger.info("åˆ†æç–¾ç—…åˆ†å¸ƒ...")

        category_counts = defaultdict(int)
        keyword_counts = defaultdict(int)

        for item in data:
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
            text_fields = ["title", "news_titles", "content"]
            combined_text = ""

            for field in text_fields:
                if field in item and item[field]:
                    if isinstance(item[field], list):
                        combined_text += " ".join(str(x) for x in item[field])
                    else:
                        combined_text += str(item[field])

            # æå–ç–¾ç—…å…³é”®è¯
            keywords = self.extract_disease_keywords(combined_text)
            for kw in keywords:
                category_counts[kw["category"]] += 1
                keyword_counts[kw["keyword"]] += 1

        distribution_data = {
            "categories": dict(category_counts),
            "keywords": dict(keyword_counts),
            "top_categories": dict(Counter(category_counts).most_common(10)),
            "top_keywords": dict(Counter(keyword_counts).most_common(20)),
        }

        logger.info(f"ç–¾ç—…åˆ†å¸ƒåˆ†æå®Œæˆï¼Œå‘ç° {len(category_counts)} ä¸ªç±»åˆ«")
        return distribution_data

    def analyze_geographic_distribution(self, data):
        """åˆ†æåœ°ç†åˆ†å¸ƒ"""
        logger.info("åˆ†æåœ°ç†åˆ†å¸ƒ...")

        location_counts = defaultdict(int)
        location_diseases = defaultdict(lambda: defaultdict(int))

        for item in data:
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
            text_fields = ["title", "news_titles", "content"]
            combined_text = ""

            for field in text_fields:
                if field in item and item[field]:
                    if isinstance(item[field], list):
                        combined_text += " ".join(str(x) for x in item[field])
                    else:
                        combined_text += str(item[field])

            # æå–åœ°åŒºä¿¡æ¯
            locations = self.extract_location_info(combined_text)
            diseases = self.extract_disease_keywords(combined_text)

            for location in locations:
                location_counts[location] += 1
                for disease in diseases:
                    location_diseases[location][disease["category"]] += 1

        geographic_data = {
            "location_counts": dict(location_counts),
            "location_diseases": dict(location_diseases),
            "top_locations": dict(Counter(location_counts).most_common(10)),
        }

        logger.info(f"åœ°ç†åˆ†å¸ƒåˆ†æå®Œæˆï¼Œè¦†ç›– {len(location_counts)} ä¸ªåœ°åŒº")
        return geographic_data

    def analyze_content_quality(self, data):
        """åˆ†æå†…å®¹è´¨é‡"""
        logger.info("åˆ†æå†…å®¹è´¨é‡...")

        quality_metrics = {
            "total_items": len(data),
            "items_with_title": 0,
            "items_with_content": 0,
            "items_with_date": 0,
            "items_with_disease_keywords": 0,
            "avg_title_length": 0,
            "avg_content_length": 0,
            "disease_relevance_rate": 0,
        }

        title_lengths = []
        content_lengths = []
        disease_relevant_count = 0

        for item in data:
            # æ ‡é¢˜è´¨é‡
            if item.get("title") or (item.get("news_titles") and item["news_titles"]):
                quality_metrics["items_with_title"] += 1
                title = item.get("title", "") or (
                    item.get("news_titles", [""])[0] if item.get("news_titles") else ""
                )
                title_lengths.append(len(str(title)))

            # å†…å®¹è´¨é‡
            if item.get("content") or item.get("news_titles"):
                quality_metrics["items_with_content"] += 1
                content = str(item.get("content", "")) + str(
                    item.get("news_titles", "")
                )
                content_lengths.append(len(content))

            # æ—¥æœŸè´¨é‡
            if item.get("publish_dates") or item.get("crawl_timestamp"):
                quality_metrics["items_with_date"] += 1

            # ç–¾ç—…ç›¸å…³æ€§
            text = (
                str(item.get("title", ""))
                + str(item.get("news_titles", ""))
                + str(item.get("content", ""))
            )
            if self.extract_disease_keywords(text):
                quality_metrics["items_with_disease_keywords"] += 1
                disease_relevant_count += 1

        # è®¡ç®—å¹³å‡å€¼
        if title_lengths:
            quality_metrics["avg_title_length"] = np.mean(title_lengths)
        if content_lengths:
            quality_metrics["avg_content_length"] = np.mean(content_lengths)
        if quality_metrics["total_items"] > 0:
            quality_metrics["disease_relevance_rate"] = (
                disease_relevant_count / quality_metrics["total_items"]
            )

        logger.info(f"å†…å®¹è´¨é‡åˆ†æå®Œæˆï¼Œç–¾ç—…ç›¸å…³æ€§: {quality_metrics['disease_relevance_rate']:.2%}")
        return quality_metrics

    def generate_comprehensive_analysis(self, data_source=None):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        logger.info("å¼€å§‹ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")

        # åŠ è½½æ•°æ®
        if data_source is None:
            data = self.load_data_from_mongodb()
        elif isinstance(data_source, str):
            data = self.load_data_from_json(data_source)
        else:
            data = data_source

        if not data:
            logger.error("æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æ")
            return None

        # æ‰§è¡Œå„é¡¹åˆ†æ
        analysis_results = {
            "metadata": {
                "analysis_time": datetime.now().isoformat(),
                "data_count": len(data),
                "analyzer_version": "1.0",
            },
            "time_trends": self.analyze_time_trends(data),
            "disease_distribution": self.analyze_disease_distribution(data),
            "geographic_distribution": self.analyze_geographic_distribution(data),
            "content_quality": self.analyze_content_quality(data),
        }

        # ç”Ÿæˆæ‘˜è¦
        summary = self.generate_analysis_summary(analysis_results)
        analysis_results["summary"] = summary

        logger.info("ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return analysis_results

    def generate_analysis_summary(self, analysis_results):
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        summary = {"key_findings": [], "recommendations": [], "data_insights": {}}

        # æ•°æ®æ´å¯Ÿ
        quality = analysis_results.get("content_quality", {})
        disease_dist = analysis_results.get("disease_distribution", {})
        geo_dist = analysis_results.get("geographic_distribution", {})

        summary["data_insights"] = {
            "total_analyzed": quality.get("total_items", 0),
            "disease_relevance": f"{quality.get('disease_relevance_rate', 0):.1%}",
            "top_disease_category": max(
                disease_dist.get("categories", {}).items(), key=lambda x: x[1]
            )[0]
            if disease_dist.get("categories")
            else "N/A",
            "most_active_district": max(
                geo_dist.get("location_counts", {}).items(), key=lambda x: x[1]
            )[0]
            if geo_dist.get("location_counts")
            else "N/A",
        }

        # å…³é”®å‘ç°
        if quality.get("disease_relevance_rate", 0) > 0.8:
            summary["key_findings"].append("æ•°æ®ç–¾ç—…ç›¸å…³æ€§å¾ˆé«˜ï¼Œå†…å®¹è´¨é‡è‰¯å¥½")
        elif quality.get("disease_relevance_rate", 0) > 0.5:
            summary["key_findings"].append("æ•°æ®ç–¾ç—…ç›¸å…³æ€§ä¸­ç­‰ï¼Œéœ€è¦ä¼˜åŒ–è¿‡æ»¤è§„åˆ™")
        else:
            summary["key_findings"].append("æ•°æ®ç–¾ç—…ç›¸å…³æ€§è¾ƒä½ï¼Œå»ºè®®è°ƒæ•´çˆ¬å–ç­–ç•¥")

        # å»ºè®®
        summary["recommendations"].append("å®šæœŸç›‘æ§ç–¾ç—…è¶‹åŠ¿å˜åŒ–")
        summary["recommendations"].append("é‡ç‚¹å…³æ³¨é«˜å‘åœ°åŒºçš„ç–«æƒ…åŠ¨æ€")
        summary["recommendations"].append("å»ºç«‹ç–¾ç—…é¢„è­¦æœºåˆ¶")

        return summary

    def save_analysis_results(self, results, output_file="analysis_results.json"):
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)

            logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ ç–¾ç—…æ•°æ®åˆ†æå¼•æ“")
    print("=" * 60)

    # åˆ›å»ºåˆ†æå™¨
    analyzer = DiseaseDataAnalyzer()

    # ç”Ÿæˆç»¼åˆåˆ†æ
    results = analyzer.generate_comprehensive_analysis()

    if results:
        # ä¿å­˜ç»“æœ
        analyzer.save_analysis_results(results, "reports/disease_analysis_report.json")

        # æ˜¾ç¤ºæ‘˜è¦
        print("\nğŸ“Š åˆ†ææ‘˜è¦:")
        print("-" * 40)
        summary = results.get("summary", {})
        insights = summary.get("data_insights", {})

        print(f"ğŸ“ˆ åˆ†ææ•°æ®é‡: {insights.get('total_analyzed', 0)} æ¡")
        print(f"ğŸ¦  ç–¾ç—…ç›¸å…³æ€§: {insights.get('disease_relevance', 'N/A')}")
        print(f"ğŸ¥ ä¸»è¦ç–¾ç—…ç±»åˆ«: {insights.get('top_disease_category', 'N/A')}")
        print(f"ğŸ™ï¸ æœ€æ´»è·ƒåœ°åŒº: {insights.get('most_active_district', 'N/A')}")

        print("\nğŸ’¡ å…³é”®å‘ç°:")
        for finding in summary.get("key_findings", []):
            print(f"   â€¢ {finding}")

        print("\nğŸ¯ å»ºè®®:")
        for rec in summary.get("recommendations", []):
            print(f"   â€¢ {rec}")

        print("\nâœ… åˆ†æå®Œæˆï¼è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜")
        return True
    else:
        print("âŒ åˆ†æå¤±è´¥")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
