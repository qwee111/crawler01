#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬å–æ•°æ®åˆ†æè„šæœ¬

åˆ†æå½“å‰çˆ¬è™«ç³»ç»Ÿçˆ¬å–çš„æ‰€æœ‰æ•°æ®
"""

import glob
import json
import os
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

# import pandas as pd  # æš‚æ—¶ä¸ä½¿ç”¨pandas


def analyze_json_files():
    """åˆ†æJSONæ•°æ®æ–‡ä»¶"""
    print("ğŸ“Š åˆ†æJSONæ•°æ®æ–‡ä»¶...")

    data_dir = Path("data")
    if not data_dir.exists():
        print("âŒ dataç›®å½•ä¸å­˜åœ¨")
        return {}

    json_files = list(data_dir.glob("*.json"))

    if not json_files:
        print("âŒ æœªæ‰¾åˆ°JSONæ•°æ®æ–‡ä»¶")
        return {}

    print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶:")

    all_data = []
    file_stats = {}

    for json_file in json_files:
        print(f"   ğŸ“„ {json_file.name}")

        try:
            with open(json_file, "r", encoding="utf-8") as f:
                content = f.read().strip()

                if not content:
                    print(f"      âš ï¸ æ–‡ä»¶ä¸ºç©º")
                    continue

                # å¤„ç†å¤šè¡ŒJSONæ ¼å¼
                items = []
                for line in content.split("\n"):
                    line = line.strip()
                    if line:
                        try:
                            item = json.loads(line)
                            items.append(item)
                        except json.JSONDecodeError:
                            # å°è¯•è§£ææ•´ä¸ªæ–‡ä»¶ä½œä¸ºJSONæ•°ç»„
                            try:
                                items = json.loads(content)
                                break
                            except:
                                continue

                file_stats[json_file.name] = {
                    "items_count": len(items),
                    "file_size": json_file.stat().st_size,
                    "created_time": datetime.fromtimestamp(json_file.stat().st_ctime),
                }

                all_data.extend(items)

                print(f"      âœ… è§£ææˆåŠŸï¼ŒåŒ…å« {len(items)} æ¡æ•°æ®")

                # æ˜¾ç¤ºæ•°æ®æ ·ä¾‹
                if items:
                    sample = items[0]
                    print(f"      ğŸ“‹ æ•°æ®å­—æ®µ: {list(sample.keys())}")

        except Exception as e:
            print(f"      âŒ è§£æå¤±è´¥: {e}")

    return {
        "all_data": all_data,
        "file_stats": file_stats,
        "total_items": len(all_data),
    }


def analyze_data_content(data_analysis):
    """åˆ†ææ•°æ®å†…å®¹"""
    print("\nğŸ” åˆ†ææ•°æ®å†…å®¹...")

    all_data = data_analysis.get("all_data", [])

    if not all_data:
        print("âŒ æ²¡æœ‰æ•°æ®å¯åˆ†æ")
        return

    print(f"ğŸ“Š æ€»æ•°æ®æ¡æ•°: {len(all_data)}")

    # åˆ†ææ•°æ®å­—æ®µ
    all_fields = set()
    field_counts = Counter()

    for item in all_data:
        if isinstance(item, dict):
            fields = set(item.keys())
            all_fields.update(fields)
            for field in fields:
                field_counts[field] += 1

    print(f"\nğŸ“‹ æ•°æ®å­—æ®µç»Ÿè®¡ (å…± {len(all_fields)} ä¸ªå­—æ®µ):")
    for field, count in field_counts.most_common():
        percentage = (count / len(all_data)) * 100
        print(f"   {field:<20} {count:>6} æ¡ ({percentage:>5.1f}%)")

    # åˆ†ææ•°æ®æ¥æº
    sources = Counter()
    urls = Counter()

    for item in all_data:
        if isinstance(item, dict):
            # åˆ†æURLæ¥æº
            if "url" in item:
                from urllib.parse import urlparse

                parsed = urlparse(item["url"])
                domain = parsed.netloc
                sources[domain] += 1
                urls[item["url"]] += 1

            # åˆ†æç½‘ç«™æ ‡è¯†
            if "site" in item:
                sources[f"site:{item['site']}"] += 1

    if sources:
        print("\nğŸŒ æ•°æ®æ¥æºç»Ÿè®¡:")
        for source, count in sources.most_common():
            print(f"   {source:<30} {count:>6} æ¡")

    # åˆ†ææ•°æ®ç±»å‹
    data_types = Counter()
    for item in all_data:
        if isinstance(item, dict):
            if "title" in item and item["title"]:
                data_types["æœ‰æ ‡é¢˜"] += 1
            if "content" in item and item["content"]:
                data_types["æœ‰å†…å®¹"] += 1
            if "links" in item and item["links"]:
                data_types["æœ‰é“¾æ¥"] += 1
            if "images" in item and item["images"]:
                data_types["æœ‰å›¾ç‰‡"] += 1

    if data_types:
        print("\nğŸ“ æ•°æ®ç±»å‹ç»Ÿè®¡:")
        for data_type, count in data_types.most_common():
            percentage = (count / len(all_data)) * 100
            print(f"   {data_type:<15} {count:>6} æ¡ ({percentage:>5.1f}%)")

    # æ˜¾ç¤ºæ•°æ®æ ·ä¾‹
    print("\nğŸ“„ æ•°æ®æ ·ä¾‹:")
    for i, item in enumerate(all_data[:3]):
        print(f"\n   æ ·ä¾‹ {i+1}:")
        if isinstance(item, dict):
            for key, value in item.items():
                if isinstance(value, str) and len(value) > 100:
                    value = value[:100] + "..."
                elif isinstance(value, list) and len(value) > 3:
                    value = value[:3] + ["..."]
                print(f"     {key}: {value}")
        else:
            print(f"     {item}")


def check_database_data():
    """æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ•°æ®"""
    print("\nğŸ’¾ æ£€æŸ¥æ•°æ®åº“æ•°æ®...")

    # æ£€æŸ¥MongoDB
    try:
        import subprocess

        result = subprocess.run(
            [
                "docker",
                "exec",
                "crawler_mongodb",
                "mongosh",
                "--eval",
                'db.adminCommand("listCollections")',
            ],
            capture_output=True,
            text=True,
            timeout=10,
        )

        if result.returncode == 0:
            print("âœ… MongoDBè¿æ¥æˆåŠŸ")
            # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥æŸ¥è¯¢æ•°æ®
        else:
            print("âš ï¸ MongoDBè¿æ¥å¤±è´¥æˆ–æ— æ•°æ®")

    except Exception as e:
        print(f"âš ï¸ MongoDBæ£€æŸ¥å¤±è´¥: {e}")

    # æ£€æŸ¥PostgreSQL
    try:
        result = subprocess.run(
            [
                "docker",
                "exec",
                "crawler_postgresql",
                "psql",
                "-U",
                "crawler",
                "-d",
                "crawler",
                "-c",
                "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';",
            ],
            capture_output=True,
            text=True,
            timeout=10,
        )

        if result.returncode == 0:
            print("âœ… PostgreSQLè¿æ¥æˆåŠŸ")
            print(f"   è¾“å‡º: {result.stdout.strip()}")
        else:
            print("âš ï¸ PostgreSQLè¿æ¥å¤±è´¥æˆ–æ— æ•°æ®")

    except Exception as e:
        print(f"âš ï¸ PostgreSQLæ£€æŸ¥å¤±è´¥: {e}")


def analyze_crawl_logs():
    """åˆ†æçˆ¬è™«æ—¥å¿—"""
    print("\nğŸ“‹ åˆ†æçˆ¬è™«æ—¥å¿—...")

    log_file = Path("logs/scrapy.log")
    if not log_file.exists():
        print("âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨")
        return

    try:
        with open(log_file, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        lines = content.split("\n")
        print(f"ğŸ“„ æ—¥å¿—æ–‡ä»¶å¤§å°: {len(lines)} è¡Œ")

        # ç»Ÿè®¡çˆ¬è™«è¿è¡Œæ¬¡æ•°
        spider_starts = [line for line in lines if "Spider opened" in line]
        spider_closes = [line for line in lines if "Spider closed" in line]

        print(f"ğŸ•·ï¸ çˆ¬è™«å¯åŠ¨æ¬¡æ•°: {len(spider_starts)}")
        print(f"ğŸ çˆ¬è™«å®Œæˆæ¬¡æ•°: {len(spider_closes)}")

        # ç»Ÿè®¡è¯·æ±‚å’Œå“åº”
        requests = [line for line in lines if "Scraped from" in line or "GET" in line]
        responses = [line for line in lines if "response" in line.lower()]

        print(f"ğŸ“¤ è¯·æ±‚æ•°é‡: {len(requests)}")
        print(f"ğŸ“¥ å“åº”æ•°é‡: {len(responses)}")

        # ç»Ÿè®¡é”™è¯¯
        errors = [line for line in lines if "ERROR" in line]
        warnings = [line for line in lines if "WARNING" in line]

        print(f"âŒ é”™è¯¯æ•°é‡: {len(errors)}")
        print(f"âš ï¸ è­¦å‘Šæ•°é‡: {len(warnings)}")

        # ç»Ÿè®¡çŠ¶æ€ç 
        status_codes = Counter()
        for line in lines:
            if "status=" in line:
                try:
                    status_part = line.split("status=")[1].split()[0]
                    status_code = status_part.rstrip(",")
                    status_codes[status_code] += 1
                except:
                    continue

        if status_codes:
            print("\nğŸ“Š HTTPçŠ¶æ€ç ç»Ÿè®¡:")
            for code, count in status_codes.most_common():
                print(f"   {code}: {count} æ¬¡")

        # æ˜¾ç¤ºæœ€è¿‘çš„æ—¥å¿—
        print("\nğŸ“ æœ€è¿‘çš„æ—¥å¿— (æœ€å10è¡Œ):")
        for line in lines[-10:]:
            if line.strip():
                print(f"   {line}")

    except Exception as e:
        print(f"âŒ æ—¥å¿—åˆ†æå¤±è´¥: {e}")


def generate_summary_report(data_analysis):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    print("\nğŸ“Š ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")

    file_stats = data_analysis.get("file_stats", {})
    total_items = data_analysis.get("total_items", 0)

    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_files": len(file_stats),
            "total_items": total_items,
            "total_size": sum(stats["file_size"] for stats in file_stats.values()),
        },
        "files": file_stats,
    }

    # ä¿å­˜æŠ¥å‘Š
    report_file = Path("data_analysis_report.json")
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)

    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    return report


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ çˆ¬å–æ•°æ®åˆ†ææŠ¥å‘Š")
    print("=" * 60)

    # åˆ†æJSONæ–‡ä»¶
    data_analysis = analyze_json_files()

    # åˆ†ææ•°æ®å†…å®¹
    analyze_data_content(data_analysis)

    # æ£€æŸ¥æ•°æ®åº“
    check_database_data()

    # åˆ†ææ—¥å¿—
    analyze_crawl_logs()

    # ç”ŸæˆæŠ¥å‘Š
    report = generate_summary_report(data_analysis)

    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“‹ æ•°æ®çˆ¬å–æ€»ç»“")
    print("=" * 60)

    summary = report["summary"]
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶æ•°é‡: {summary['total_files']}")
    print(f"ğŸ“Š æ•°æ®æ¡ç›®æ€»æ•°: {summary['total_items']}")
    print(f"ğŸ’¾ æ•°æ®æ€»å¤§å°: {summary['total_size']:,} å­—èŠ‚")

    if summary["total_items"] > 0:
        print("\nâœ… çˆ¬è™«ç³»ç»Ÿå·²æˆåŠŸçˆ¬å–æ•°æ®ï¼")
        print("ğŸ“ æ•°æ®ä¿å­˜ä½ç½®: data/ ç›®å½•")
        print("ğŸ“„ æ•°æ®æ ¼å¼: JSON")
        print("ğŸ” è¯¦ç»†æŠ¥å‘Š: data_analysis_report.json")
    else:
        print("\nâš ï¸ æš‚æ— çˆ¬å–æ•°æ®")
        print("ğŸ’¡ å»ºè®®è¿è¡Œçˆ¬è™«: uv run scrapy crawl adaptive -a site=test_site")

    print("\nğŸŠ æ•°æ®åˆ†æå®Œæˆï¼")


if __name__ == "__main__":
    main()
