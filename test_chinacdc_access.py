#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• chinacdc ç½‘ç«™è®¿é—®å’Œåçˆ¬è™«æœºåˆ¶æ£€æµ‹è„šæœ¬

ç”¨äºæ£€æµ‹ï¼š
1. ç½‘ç«™æ˜¯å¦å¯è®¿é—®
2. å“åº”ç±»å‹å’Œå†…å®¹
3. åçˆ¬è™«æœºåˆ¶
4. å‹ç¼©ç¼–ç æ”¯æŒ
"""

import json
import re
import time
from urllib.parse import urlparse

import requests


class ChinaCDCTester:
    """ä¸­å›½ç–¾æ§ä¸­å¿ƒç½‘ç«™æµ‹è¯•å™¨"""

    def __init__(self):
        self.test_urls = [
            "https://www.chinacdc.cn/jksj/jksj01/",
            "https://www.chinacdc.cn/jksj/xgbdyq/",
            "https://www.chinacdc.cn/jksj/jksj02/",
            "https://www.chinacdc.cn/jksj/jksj03/",
        ]

        # ä¸åŒçš„è¯·æ±‚å¤´é…ç½®
        self.headers_configs = {
            "scrapy_default": {
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "User-Agent": "Scrapy/2.11.0 (+https://scrapy.org)",
            },
            "chrome_browser": {
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Cache-Control": "max-age=0",
                "sec-ch-ua": '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": '"Windows"',
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Upgrade-Insecure-Requests": "1",
            },
            "no_br_encoding": {
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate",  # ç§»é™¤ br
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            },
        }

    def test_single_url(self, url, headers_name, headers, timeout=30):
        """æµ‹è¯•å•ä¸ªURL"""
        print(f"\n{'='*60}")
        print(f"ğŸ” æµ‹è¯•URL: {url}")
        print(f"ğŸ“‹ è¯·æ±‚å¤´é…ç½®: {headers_name}")
        print(f"{'='*60}")

        try:
            # å‘é€è¯·æ±‚
            start_time = time.time()
            response = requests.get(
                url, headers=headers, timeout=timeout, allow_redirects=True
            )
            response_time = time.time() - start_time

            # åŸºæœ¬ä¿¡æ¯
            print(f"âœ… è¯·æ±‚æˆåŠŸ")
            print(f"ğŸ“Š çŠ¶æ€ç : {response.status_code}")
            print(f"â±ï¸ å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(response.content)} å­—èŠ‚")

            # å“åº”å¤´åˆ†æ
            print(f"\nğŸ“‹ å“åº”å¤´ä¿¡æ¯:")
            important_headers = [
                "Content-Type",
                "Content-Encoding",
                "Content-Length",
                "Server",
                "Set-Cookie",
                "Cache-Control",
                "X-Frame-Options",
                "X-Content-Type-Options",
                "Strict-Transport-Security",
            ]

            for header in important_headers:
                value = response.headers.get(header)
                if value:
                    print(f"  {header}: {value}")

            # é‡å®šå‘ä¿¡æ¯
            if response.history:
                print(f"\nğŸ”„ é‡å®šå‘å†å²:")
                for i, resp in enumerate(response.history):
                    print(f"  {i+1}. {resp.status_code} -> {resp.url}")
                print(f"  æœ€ç»ˆ: {response.status_code} -> {response.url}")

            # å†…å®¹ç±»å‹åˆ†æ
            content_type = response.headers.get("Content-Type", "").lower()
            print(f"\nğŸ“„ å†…å®¹åˆ†æ:")
            print(f"  Content-Type: {content_type}")

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æœ¬å†…å®¹
            is_text = any(
                t in content_type
                for t in [
                    "text/",
                    "application/json",
                    "application/xml",
                    "application/xhtml",
                ]
            )
            print(f"  æ˜¯å¦ä¸ºæ–‡æœ¬: {'æ˜¯' if is_text else 'å¦'}")

            if is_text:
                try:
                    text_content = response.text
                    print(f"  æ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")

                    # æ£€æŸ¥é¡µé¢å†…å®¹ç‰¹å¾
                    self.analyze_page_content(text_content)

                except Exception as e:
                    print(f"  âŒ æ–‡æœ¬è§£ç å¤±è´¥: {e}")
                    print(f"  åŸå§‹å†…å®¹å‰100å­—èŠ‚: {response.content[:100]}")
            else:
                print(f"  âš ï¸ éæ–‡æœ¬å†…å®¹ï¼Œå‰100å­—èŠ‚: {response.content[:100]}")

            # åçˆ¬è™«æœºåˆ¶æ£€æµ‹
            self.detect_anti_crawl(response)

            return {
                "success": True,
                "status_code": response.status_code,
                "content_type": content_type,
                "is_text": is_text,
                "response_time": response_time,
                "content_length": len(response.content),
                "headers": dict(response.headers),
            }

        except requests.exceptions.Timeout:
            print(f"âŒ è¯·æ±‚è¶…æ—¶ (>{timeout}ç§’)")
            return {"success": False, "error": "timeout"}

        except requests.exceptions.ConnectionError as e:
            print(f"âŒ è¿æ¥é”™è¯¯: {e}")
            return {"success": False, "error": f"connection_error: {e}"}

        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            return {"success": False, "error": f"request_error: {e}"}

        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
            return {"success": False, "error": f"unknown_error: {e}"}

    def analyze_page_content(self, text_content):
        """åˆ†æé¡µé¢å†…å®¹ç‰¹å¾"""
        print(f"\nğŸ” é¡µé¢å†…å®¹ç‰¹å¾:")

        # HTMLç»“æ„æ£€æŸ¥
        has_html = bool(re.search(r"<html[^>]*>", text_content, re.IGNORECASE))
        has_head = bool(re.search(r"<head[^>]*>", text_content, re.IGNORECASE))
        has_body = bool(re.search(r"<body[^>]*>", text_content, re.IGNORECASE))

        print(f"  HTMLç»“æ„: {'å®Œæ•´' if all([has_html, has_head, has_body]) else 'ä¸å®Œæ•´'}")

        # æ ‡é¢˜æå–
        title_match = re.search(
            r"<title[^>]*>(.*?)</title>", text_content, re.IGNORECASE | re.DOTALL
        )
        if title_match:
            title = title_match.group(1).strip()
            print(f"  é¡µé¢æ ‡é¢˜: {title[:100]}{'...' if len(title) > 100 else ''}")

        # JavaScriptæ£€æŸ¥
        js_count = len(re.findall(r"<script[^>]*>", text_content, re.IGNORECASE))
        print(f"  JavaScriptè„šæœ¬æ•°é‡: {js_count}")

        # é“¾æ¥æ£€æŸ¥
        link_count = len(re.findall(r"<a[^>]+href[^>]*>", text_content, re.IGNORECASE))
        print(f"  é“¾æ¥æ•°é‡: {link_count}")

        # å†…å®¹é¢„è§ˆ
        # ç§»é™¤HTMLæ ‡ç­¾è·å–çº¯æ–‡æœ¬
        clean_text = re.sub(r"<[^>]+>", "", text_content)
        clean_text = re.sub(r"\s+", " ", clean_text).strip()

        if clean_text:
            preview = clean_text[:200] + ("..." if len(clean_text) > 200 else "")
            print(f"  å†…å®¹é¢„è§ˆ: {preview}")

    def detect_anti_crawl(self, response):
        """æ£€æµ‹åçˆ¬è™«æœºåˆ¶"""
        print(f"\nğŸ›¡ï¸ åçˆ¬è™«æœºåˆ¶æ£€æµ‹:")

        detected_mechanisms = []

        # çŠ¶æ€ç æ£€æŸ¥
        if response.status_code in [403, 412, 429, 503]:
            detected_mechanisms.append(f"å¯ç–‘çŠ¶æ€ç : {response.status_code}")

        # å“åº”å¤´æ£€æŸ¥
        headers = response.headers
        if "cf-ray" in headers or "cloudflare" in str(headers).lower():
            detected_mechanisms.append("Cloudflareé˜²æŠ¤")

        if "x-rate-limit" in str(headers).lower():
            detected_mechanisms.append("é¢‘ç‡é™åˆ¶")

        # å†…å®¹æ£€æŸ¥ï¼ˆå¦‚æœæ˜¯æ–‡æœ¬ï¼‰
        try:
            content = response.text.lower()

            # éªŒè¯ç æ£€æµ‹
            captcha_patterns = [
                r"captcha",
                r"éªŒè¯ç ",
                r"recaptcha",
                r"hcaptcha",
                r"geetest",
                r"slider.*verify",
                r"puzzle.*verify",
            ]
            for pattern in captcha_patterns:
                if re.search(pattern, content):
                    detected_mechanisms.append(f"éªŒè¯ç æ£€æµ‹: {pattern}")
                    break

            # JavaScriptæŒ‘æˆ˜æ£€æµ‹
            js_challenge_patterns = [
                r"challenge.*js",
                r"anti.*bot",
                r"protection.*mode",
                r"ddos.*guard",
                r"bot.*detection",
            ]
            for pattern in js_challenge_patterns:
                if re.search(pattern, content):
                    detected_mechanisms.append(f"JSæŒ‘æˆ˜: {pattern}")
                    break

            # é¢‘ç‡é™åˆ¶æ£€æµ‹
            rate_limit_patterns = [
                r"rate.*limit",
                r"too.*many.*requests",
                r"è¯·æ±‚è¿‡äºé¢‘ç¹",
                r"è®¿é—®é¢‘ç‡",
                r"429",
            ]
            for pattern in rate_limit_patterns:
                if re.search(pattern, content):
                    detected_mechanisms.append(f"é¢‘ç‡é™åˆ¶: {pattern}")
                    break

            # IPå°ç¦æ£€æµ‹
            ip_block_patterns = [
                r"ip.*block",
                r"access.*denied",
                r"forbidden",
                r"banned",
                r"blocked",
                r"ip.*ç¦æ­¢",
                r"è®¿é—®è¢«æ‹’ç»",
            ]
            for pattern in ip_block_patterns:
                if re.search(pattern, content):
                    detected_mechanisms.append(f"IPå°ç¦: {pattern}")
                    break

        except Exception:
            pass  # éæ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡å†…å®¹æ£€æµ‹

        if detected_mechanisms:
            print(f"  âš ï¸ æ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶:")
            for mechanism in detected_mechanisms:
                print(f"    - {mechanism}")
        else:
            print(f"  âœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„åçˆ¬è™«æœºåˆ¶")

    def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹ chinacdc ç½‘ç«™è®¿é—®æµ‹è¯•")
        print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")

        results = {}

        for url in self.test_urls:
            results[url] = {}

            for headers_name, headers in self.headers_configs.items():
                print(f"\nâ³ ç­‰å¾…1ç§’é¿å…è¯·æ±‚è¿‡å¿«...")
                time.sleep(1)

                result = self.test_single_url(url, headers_name, headers)
                results[url][headers_name] = result

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_report(results)

        return results

    def generate_report(self, results):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šæ±‡æ€»")
        print(f"{'='*80}")

        for url in results:
            print(f"\nğŸŒ URL: {url}")
            url_results = results[url]

            success_count = sum(1 for r in url_results.values() if r.get("success"))
            total_count = len(url_results)

            print(
                f"  æˆåŠŸç‡: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)"
            )

            for headers_name, result in url_results.items():
                if result.get("success"):
                    status = result.get("status_code")
                    content_type = result.get("content_type", "")[:50]
                    is_text = "æ–‡æœ¬" if result.get("is_text") else "éæ–‡æœ¬"
                    time_taken = result.get("response_time", 0)
                    print(
                        f"    {headers_name}: âœ… {status} | {is_text} | {time_taken:.2f}s | {content_type}"
                    )
                else:
                    error = result.get("error", "unknown")
                    print(f"    {headers_name}: âŒ {error}")

        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
        with open("chinacdc_test_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: chinacdc_test_results.json")


if __name__ == "__main__":
    tester = ChinaCDCTester()
    results = tester.run_comprehensive_test()
