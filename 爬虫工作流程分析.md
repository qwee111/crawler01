# 🕷️ 爬虫系统工作流程与反爬虫应对策略

**更新时间**: 2025年8月4日
**系统版本**: v3.0 (第三阶段)

---

## 📊 整体架构流程图

```
用户启动爬虫
     ↓
┌─────────────────────────────────────────────────────────┐
│                1. 爬虫初始化阶段                          │
├─────────────────────────────────────────────────────────┤
│ • 加载网站配置 (config/sites/{site}.yaml)               │
│ • 初始化规则引擎 (RuleEngine)                           │
│ • 设置起始URL和允许域名                                  │
│ • 动态更新爬虫设置                                       │
└─────────────────────────────────────────────────────────┘
     ↓
┌─────────────────────────────────────────────────────────┐
│                2. 请求生成阶段                           │
├─────────────────────────────────────────────────────────┤
│ • start_requests() 生成初始请求                         │
│ • 应用网站特定的请求头                                   │
│ • 设置请求延迟和并发控制                                 │
└─────────────────────────────────────────────────────────┘
     ↓
┌─────────────────────────────────────────────────────────┐
│                3. 下载中间件处理                         │
├─────────────────────────────────────────────────────────┤
│ 🔄 ProxyMiddleware (代理轮换)                           │
│ 🎭 UserAgentMiddleware (UA轮换)                         │
│ 🛡️ AntiCrawlMiddleware (反爬虫检测)                     │
│ 🤖 SeleniumMiddleware (JS渲染)                          │
│ 🎯 BehaviorSimulationMiddleware (行为模拟)              │
└─────────────────────────────────────────────────────────┘
     ↓
┌─────────────────────────────────────────────────────────┐
│                4. 响应处理阶段                           │
├─────────────────────────────────────────────────────────┤
│ • 反爬虫检测和应对                                       │
│ • 响应状态码检查                                         │
│ • 内容质量验证                                           │
└─────────────────────────────────────────────────────────┘
     ↓
┌─────────────────────────────────────────────────────────┐
│                5. 数据提取阶段                           │
├─────────────────────────────────────────────────────────┤
│ • 规则引擎数据提取                                       │
│ • 配置化字段提取                                         │
│ • 链接发现和跟进                                         │
└─────────────────────────────────────────────────────────┘
     ↓
┌─────────────────────────────────────────────────────────┐
│                6. 数据处理管道                           │
├─────────────────────────────────────────────────────────┤
│ 🔧 EnhancedExtractionPipeline (增强提取)               │
│ 📊 DataEnrichmentPipeline (数据丰富化)                 │
│ 🧹 ComprehensiveDataPipeline (综合处理)                │
│   ├── 数据清洗 (CleaningPipeline)                      │
│   ├── 数据验证 (ValidationPipeline)                    │
│   └── 质量评估 (QualityMonitor)                        │
│ 💾 MongoPipeline (数据存储)                            │
└─────────────────────────────────────────────────────────┘
     ↓
┌─────────────────────────────────────────────────────────┐
│                7. 数据存储阶段                           │
├─────────────────────────────────────────────────────────┤
│ • JSON文件存储 (data/*.json)                           │
│ • MongoDB存储 (crawler_db)                             │
│ • 质量报告生成                                           │
└─────────────────────────────────────────────────────────┘
```

---

## 🛡️ 反爬虫应对策略详解

### 1. **检测机制** (AntiCrawlDetector)

#### 支持的反爬虫类型检测：
```python
DETECTION_RULES = {
    'captcha': {
        'indicators': ['验证码', 'captcha', 'verify', '人机验证'],
        'selectors': ['img[src*="captcha"]', '.captcha', '#verify'],
        'confidence_threshold': 0.8
    },
    'js_challenge': {
        'indicators': ['正在跳转', '页面跳转中', 'redirecting'],
        'js_patterns': ['setTimeout', 'location.href', 'window.location'],
        'confidence_threshold': 0.7
    },
    'rate_limit': {
        'status_codes': [412, 429, 503],
        'indicators': ['访问频率', '请求过快', 'too many requests'],
        'confidence_threshold': 0.9
    },
    'ip_block': {
        'status_codes': [403, 451],
        'indicators': ['IP被封', 'access denied', 'forbidden'],
        'confidence_threshold': 0.8
    },
    'user_agent_check': {
        'indicators': ['浏览器不支持', 'browser not supported'],
        'confidence_threshold': 0.7
    }
}
```

#### 检测流程：
1. **状态码检查**: 检测412、429、403等异常状态码
2. **内容分析**: 扫描页面内容中的反爬虫关键词
3. **元素检测**: 查找验证码图片、JS跳转代码等
4. **行为分析**: 检测异常的响应时间和重定向
5. **置信度评分**: 为每种检测类型计算0-1的置信度分数

### 2. **应对策略** (AntiCrawlStrategy)

#### 策略映射：
```python
STRATEGIES = {
    'captcha': [
        'use_selenium',      # 使用Selenium处理
        'change_proxy',      # 更换代理
        'increase_delay',    # 增加延迟
        'rotate_user_agent'  # 轮换User-Agent
    ],
    'rate_limit': [
        'increase_delay',    # 增加请求间隔
        'change_proxy',      # 更换IP
        'reduce_concurrency' # 降低并发数
    ],
    'js_challenge': [
        'use_selenium',      # 使用浏览器渲染
        'wait_and_retry'     # 等待后重试
    ],
    'ip_block': [
        'change_proxy',      # 立即更换代理
        'long_delay'         # 长时间延迟
    ]
}
```

### 3. **中间件协作机制**

#### 中间件执行顺序：
```python
DOWNLOADER_MIDDLEWARES = {
    'crawler.middlewares.ProxyMiddleware': 350,                    # 代理管理
    'crawler.middlewares.CustomUserAgentMiddleware': 400,         # UA轮换
    'crawler.selenium_middleware.SeleniumMiddleware': 585,        # Selenium处理
    'anti_crawl.middleware.AntiCrawlMiddleware': 590,             # 反爬虫检测
    'anti_crawl.middleware.BehaviorSimulationMiddleware': 600,    # 行为模拟
    'anti_crawl.middleware.HeaderRotationMiddleware': 605,       # 请求头轮换
}
```

#### 协作流程：
1. **请求预处理**: ProxyMiddleware选择代理，UserAgentMiddleware设置UA
2. **反爬虫检测**: AntiCrawlMiddleware检测响应中的反爬虫机制
3. **策略执行**: 根据检测结果自动应用对应策略
4. **行为模拟**: BehaviorSimulationMiddleware模拟人类浏览行为
5. **请求重试**: 必要时重新发起请求

---

## 🔄 具体工作流程

### 启动命令示例：
```bash
uv run scrapy crawl adaptive -a site=nhc_new
```

### 详细执行步骤：

#### 1. **初始化阶段**
```python
# 1.1 加载网站配置
site_rule = rule_engine.get_rule("nhc_new")
# 配置包含：
# - start_urls: 起始URL列表
# - allowed_domains: 允许的域名
# - request_settings: 请求设置
# - fields: 数据提取规则

# 1.2 设置爬虫参数
self.start_urls = ["http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml"]
self.allowed_domains = ["nhc.gov.cn", "www.nhc.gov.cn"]
```

#### 2. **请求发送**
```python
# 2.1 生成请求
request = Request(
    url="http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml",
    headers={
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)...',
        'Accept': 'text/html,application/xhtml+xml...',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
    }
)

# 2.2 中间件处理
# - ProxyMiddleware: 添加代理 proxy='http://proxy:port'
# - UserAgentMiddleware: 可能轮换UA
# - 应用延迟: download_delay=3秒
```

#### 3. **响应处理**
```python
# 3.1 反爬虫检测
if response.status == 412:
    detection_result = {
        'detected': ['rate_limit'],
        'confidence': {'rate_limit': 0.9},
        'suggestions': ['增加请求间隔', '使用代理池轮换IP']
    }

# 3.2 策略应用
if 'rate_limit' in detection_result['detected']:
    # 增加延迟到10秒
    # 更换代理IP
    # 重新发起请求
```

#### 4. **数据提取**
```python
# 4.1 使用规则引擎提取
data = rule_engine.extract_data(response, site_rule)
# 提取字段：title, content, publish_date, confirmed_cases等

# 4.2 链接发现
links = rule_engine.get_links(response, site_rule)
# 发现详情页链接并加入队列
```

#### 5. **数据处理**
```python
# 5.1 数据丰富化
item['spider_name'] = 'adaptive'
item['crawl_timestamp'] = '2025-08-04T01:30:00'
item['content_fingerprint'] = 'md5_hash'

# 5.2 数据清洗
cleaned_title = clean_text(item['title'])  # 去除多余空格
cleaned_content = clean_html(item['content'])  # 移除HTML标签

# 5.3 数据验证
validation_result = validator.validate(item)
# 检查必需字段、格式正确性等

# 5.4 质量评估
quality_score = assessor.assess_quality(item)
# 评估完整性、准确性、一致性等
```

#### 6. **数据存储**
```python
# 6.1 JSON文件存储
# 保存到: data/adaptive_2025-08-04T01-30-00.json

# 6.2 MongoDB存储
# 保存到: crawler_db.adaptive_data 集合

# 6.3 质量报告
# 生成: reports/quality_report_adaptive.json
```

---

## 🎯 反爬虫应对实例

### 实例1：遇到412状态码
```
请求 → 返回412状态码 → 检测为rate_limit → 应用策略：
1. 增加延迟到10秒
2. 更换代理IP
3. 轮换User-Agent
4. 重新发起请求
```

### 实例2：遇到验证码
```
请求 → 页面包含验证码 → 检测为captcha → 应用策略：
1. 启用Selenium浏览器
2. 更换代理IP
3. 增加随机延迟
4. 可选：调用验证码识别服务
```

### 实例3：JS跳转检测
```
请求 → 页面包含JS跳转 → 检测为js_challenge → 应用策略：
1. 自动启用Selenium渲染
2. 等待页面加载完成
3. 获取最终页面内容
```

---

## 📊 监控和统计

### 实时统计信息：
- **总请求数**: 跟踪发送的请求总数
- **反爬虫检测次数**: 统计各类型检测次数
- **策略应用次数**: 记录各策略使用频率
- **成功率**: 计算最终成功获取数据的比例
- **平均响应时间**: 监控网站响应性能
- **代理使用情况**: 跟踪代理池使用状态

### 质量监控：
- **数据完整性**: 必需字段的完整率
- **数据准确性**: 格式和内容的正确性
- **数据一致性**: 字段命名和类型一致性
- **数据时效性**: 爬取时间的新鲜度
- **整体质量评分**: 0-1分制综合评分

---

## 🔧 配置和调优

### 关键配置参数：
```python
# 反爬虫配置
ANTI_CRAWL_ENABLED = True
ANTI_CRAWL_AUTO_RETRY = True
ANTI_CRAWL_MAX_RETRIES = 3
ANTI_CRAWL_RETRY_DELAY = 5

# 行为模拟配置
BEHAVIOR_MIN_DELAY = 1.0
BEHAVIOR_MAX_DELAY = 5.0

# Selenium配置
SELENIUM_ENABLED = False  # 按需启用
SELENIUM_GRID_URL = 'http://localhost:4444'

# 数据质量配置
MIN_QUALITY_SCORE = 0.0
ENABLE_QUALITY_ASSESSMENT = True
```

### 性能调优建议：
1. **合理设置延迟**: 根据网站特点调整请求间隔
2. **代理池管理**: 维护高质量的代理IP池
3. **并发控制**: 避免过高并发触发反爬虫
4. **缓存策略**: 避免重复请求相同内容
5. **监控调整**: 根据成功率动态调整策略

这个工作流程确保了爬虫系统能够智能地应对各种反爬虫机制，同时保证数据质量和系统稳定性。
