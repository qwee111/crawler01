# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„æ•°æ®å¤„ç†ç®¡é“

é›†æˆç¬¬ä¸‰é˜¶æ®µçš„æ‰€æœ‰æ•°æ®å¤„ç†åŠŸèƒ½
"""

import logging
from typing import Any, Dict, Optional

from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem

from .cleaner import CleaningPipeline
from .extractor import ExtractionConfigManager
from .quality_assessor import QualityMonitor
from .validator import ValidationPipeline

logger = logging.getLogger(__name__)


class EnhancedExtractionPipeline:
    """å¢å¼ºæ•°æ®æå–ç®¡é“"""

    def __init__(self, config_dir=None):
        # ä½¿ç”¨æ–°çš„é…ç½®ç®¡ç†å™¨
        from .extractor import ExtractionConfigManager

        self.extraction_manager = ExtractionConfigManager()
        self.config_dir = config_dir

        self.stats = {
            "total_processed": 0,
            "extraction_success": 0,
            "extraction_failed": 0,
            "items_dropped": 0,
        }

        logger.info("ğŸ”§ EnhancedExtractionPipeline åˆå§‹åŒ–å®Œæˆ")

    @classmethod
    def from_crawler(cls, crawler):
        config_dir = crawler.settings.get("EXTRACTION_CONFIG_DIR", "config/extraction")
        # é…ç½®ç»Ÿä¸€åˆ° config/sitesï¼ŒEXTRACTION_CONFIG_DIR ä¸å†ç”Ÿæ•ˆï¼Œä»…æç¤ºä¸€æ¬¡
        try:
            if isinstance(config_dir, str) and "extraction" in config_dir:
                logger.warning("EXTRACTION_CONFIG_DIR å·²å¼ƒç”¨ï¼Œå¢å¼ºæå–ç»Ÿä¸€ä» config/sites åŠ è½½é…ç½®ã€‚")
        except Exception:
            pass
        return cls(config_dir=None)

    def process_item(self, item, spider):
        """å¤„ç†æ•°æ®é¡¹"""
        self.stats["total_processed"] += 1
        adapter = ItemAdapter(item)

        logger.info(
            f"ğŸ”§ EnhancedExtractionPipeline å¤„ç†æ•°æ®é¡¹: {adapter.get('url', 'unknown')}"
        )

        try:
            # è·å–ç½‘ç«™åç§°
            site_name = adapter.get("site") or getattr(
                spider, "target_site", spider.name
            )
            logger.info(f"ğŸ¯ ä½¿ç”¨ç½‘ç«™é…ç½®: {site_name}")

            # ä»…å¯¹è¯¦æƒ…é¡µæ‰§è¡Œå¢å¼ºæå–ï¼Œåˆ—è¡¨é¡µç›´æ¥è·³è¿‡ï¼ˆé¿å…ç©ºå†…å®¹é€ æˆçš„è¯¯åˆ¤ï¼‰
            page_type = (
                adapter.get("page_type")
                or (adapter.get("page_analysis") or {}).get("page_type")
                or self._infer_page_type(adapter)
            )
            if page_type != "detail_page":
                logger.info(f"â­ï¸ éè¯¦æƒ…é¡µï¼Œè·³è¿‡å¢å¼ºæå–: {page_type}")
                # ä»…è·³è¿‡å¢å¼ºï¼Œä¸ä¸¢å¼ƒ item
                self.stats["items_skipped"] = self.stats.get("items_skipped", 0) + 1
                return item

            # åˆ›å»ºæ¨¡æ‹Ÿå“åº”å¯¹è±¡ï¼ˆå¸¦å®‰å…¨å›é€€ç­–ç•¥ï¼‰
            response = self._create_response_from_item(adapter)
            logger.info(f"ğŸ“„ åˆ›å»ºæ¨¡æ‹Ÿå“åº”å¯¹è±¡ï¼Œå†…å®¹é•¿åº¦: {len(response.text)}")

            # è‹¥å†…å®¹ä»ä¸ºç©ºï¼Œåˆ™æ”¾å¼ƒå¢å¼ºæå–ï¼Œé¿å…è¦†ç›–å·²æœ‰å­—æ®µ
            if not response.text:
                logger.info("â­ï¸ æ¨¡æ‹Ÿå“åº”æ— å†…å®¹ï¼Œè·³è¿‡å¢å¼ºæå–")
                return item

            # ä½¿ç”¨é…ç½®åŒ–æå–å™¨é‡æ–°æå–æ•°æ®ï¼ˆä» config/sites çš„ extraction æ®µè¯»å–ï¼Œæ”¯æŒé¡µé¢ç±»å‹ï¼‰
            extracted_data = self.extraction_manager.extract_data(
                response,
                site_name,
                page_type=page_type,
            )
            logger.info(f"ğŸ“Š é…ç½®åŒ–æå–ç»“æœ: {len(extracted_data)} ä¸ªå­—æ®µ")

            # åˆå¹¶æå–çš„æ•°æ®ï¼ˆä»…æ›´æ–°éç©ºå€¼ï¼Œå¹¶åšå­—æ®µåå…¼å®¹æ˜ å°„ï¼‰
            updated_fields = 0
            for key, value in extracted_data.items():
                if value is not None:
                    adapter[key] = value
                    updated_fields += 1

            # å…¼å®¹æ˜ å°„ï¼šarticle_title/article_content â†’ title/content
            if (
                adapter.get("title") is None
                and adapter.get("article_title") is not None
            ):
                adapter["title"] = adapter.get("article_title")
                updated_fields += 1
            content_val = adapter.get("content") or adapter.get("article_content")
            if isinstance(content_val, list):
                content_val = " ".join(
                    [str(x).strip() for x in content_val if str(x).strip()]
                )
            if adapter.get("content") is None and content_val:
                adapter["content"] = content_val
                updated_fields += 1

            # è§„èŒƒåŒ– contentï¼šè‹¥ä¸ºåˆ—è¡¨åˆ™åˆå¹¶ï¼›è‹¥ä¸ºç©ºä½†æœ‰ raw_htmlï¼Œåˆ™æå–çº¯æ–‡æœ¬å¡«å……
            try:
                c_val = adapter.get("content")
                if isinstance(c_val, list):
                    c_val = " ".join([str(x).strip() for x in c_val if str(x).strip()])
                    adapter["content"] = c_val
                if (not adapter.get("content")) and adapter.get("raw_html"):
                    try:
                        from bs4 import BeautifulSoup

                        soup = BeautifulSoup(adapter.get("raw_html"), "html.parser")
                        text_only = " ".join(soup.get_text(separator=" ").split())
                        if text_only:
                            adapter["content"] = text_only
                    except Exception:
                        pass
            except Exception:
                pass

            # æ›´æ–°å†…å®¹ç»Ÿè®¡ï¼ˆé¿å…ä¸ DataEnrichmentPipeline é‡å¤ï¼‰
            if adapter.get("content"):
                try:
                    txt = adapter.get("content")
                    if "content_length" not in adapter:
                        adapter["content_length"] = len(txt)
                    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°
                    import re

                    if "chinese_char_count" not in adapter:
                        adapter["chinese_char_count"] = len(
                            re.findall(r"[\u4e00-\u9fff]", txt)
                        )
                except Exception:
                    pass

            logger.info(f"ğŸ“ æ›´æ–°äº† {updated_fields} ä¸ªå­—æ®µ")

            # æ·»åŠ æå–å…ƒæ•°æ®
            adapter["_extraction_metadata"] = {
                "extractor_used": site_name,
                "extraction_method": "enhanced",
                "fields_extracted": len(extracted_data),
            }

            self.stats["extraction_success"] += 1
            logger.info(f"âœ… å¢å¼ºæå–æˆåŠŸ: {adapter.get('url', 'unknown')}")

            return item

        except Exception as e:
            self.stats["extraction_failed"] += 1
            logger.error(f"âŒ å¢å¼ºæå–å¤±è´¥: {e}")
            import traceback

            logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

            # ä¿ç•™åŸå§‹æ•°æ®ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
            adapter["_extraction_error"] = str(e)
            return item

    def _create_response_from_item(self, adapter: ItemAdapter):
        """ä»æ•°æ®é¡¹åˆ›å»ºå“åº”å¯¹è±¡ï¼ˆå¸¦å®‰å…¨å›é€€ï¼‰"""

        class MockResponse:
            def __init__(self, url, text, status=200):
                self.url = url
                self.text = text
                self.status = status
                self.body = text.encode("utf-8") if text else b""
                self.headers = {}

            def json(self):
                import json

                return json.loads(self.text)

        url = adapter.get("url", "")
        # ä¼˜å…ˆé¡ºåºï¼šraw_html > content > article_content > full_content > text
        candidates = [
            adapter.get("raw_html"),
            adapter.get("content"),
            adapter.get("article_content"),
            adapter.get("full_content"),
            adapter.get("text"),
        ]
        content = None
        for val in candidates:
            if val:
                content = val
                break

        # åˆ—è¡¨/å¤šæ®µå†…å®¹åˆå¹¶
        if isinstance(content, list):
            content = " ".join([str(c).strip() for c in content if str(c).strip()])

        status = adapter.get("status") or adapter.get("status_code") or 200

        return MockResponse(url, str(content or ""), status)

    def _infer_page_type(self, adapter: ItemAdapter) -> str:
        """æ ¹æ®URLä¸å­—æ®µå¯å‘å¼åˆ¤æ–­é¡µé¢ç±»å‹"""
        try:
            url = (adapter.get("url") or "").lower()
            # æ˜æ˜¾çš„URLæ¨¡å¼
            if any(kw in url for kw in ["list", "index", "category", "åˆ—è¡¨"]):
                return "list_page"
            import re

            if re.search(r"/\d+\.s?html$", url) or any(
                kw in url for kw in ["detail", "article"]
            ):
                return "detail_page"
            # æ ¹æ®å­—æ®µç‰¹å¾
            items_val = adapter.get("items")
            if items_val:
                if isinstance(items_val, list) and len(items_val) >= 3:
                    return "list_page"
                if (
                    isinstance(items_val, str)
                    and "[" in items_val
                    and "title" in items_val
                ):
                    return "list_page"
            content_val = adapter.get("content") or adapter.get("article_content")
            if content_val:
                if isinstance(content_val, str) and len(content_val.strip()) > 50:
                    return "detail_page"
                if (
                    isinstance(content_val, list)
                    and len(" ".join(map(str, content_val))) > 50
                ):
                    return "detail_page"
        except Exception:
            pass
        return "unknown_page"

    def get_stats(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()


class ComprehensiveDataPipeline:
    """ç»¼åˆæ•°æ®å¤„ç†ç®¡é“"""

    def __init__(self, config=None):
        self.config = config or {}

        # åˆå§‹åŒ–å­ç®¡é“
        try:
            from .cleaner import CleaningPipeline
            from .quality_assessor import QualityMonitor
            from .validator import ValidationPipeline

            self.cleaning_pipeline = CleaningPipeline()
            self.validation_pipeline = ValidationPipeline()
            self.quality_monitor = QualityMonitor()

            logger.info("âœ… æ‰€æœ‰å­ç®¡é“åˆå§‹åŒ–æˆåŠŸ")

        except ImportError as e:
            logger.warning(f"âš ï¸ éƒ¨åˆ†å­ç®¡é“åˆå§‹åŒ–å¤±è´¥: {e}")
            # åˆ›å»ºç®€åŒ–ç‰ˆæœ¬
            self.cleaning_pipeline = None
            self.validation_pipeline = None
            self.quality_monitor = None

        self.stats = {
            "total_processed": 0,
            "cleaning_success": 0,
            "validation_success": 0,
            "quality_assessed": 0,
            "items_dropped": 0,
        }

        logger.info("ğŸ”§ ComprehensiveDataPipeline åˆå§‹åŒ–å®Œæˆ")

    @classmethod
    def from_crawler(cls, crawler):
        config = {
            "enable_cleaning": crawler.settings.getbool("ENABLE_DATA_CLEANING", True),
            "enable_validation": crawler.settings.getbool(
                "ENABLE_DATA_VALIDATION", True
            ),
            "enable_quality_assessment": crawler.settings.getbool(
                "ENABLE_QUALITY_ASSESSMENT", True
            ),
            "drop_invalid_items": crawler.settings.getbool("DROP_INVALID_ITEMS", False),
            "min_quality_score": crawler.settings.getfloat("MIN_QUALITY_SCORE", 0.0),
        }
        return cls(config)

    def process_item(self, item, spider):
        """å¤„ç†æ•°æ®é¡¹"""
        self.stats["total_processed"] += 1
        adapter = ItemAdapter(item)

        try:
            # 1. æ•°æ®æ¸…æ´—
            if self.config.get("enable_cleaning", True) and self.cleaning_pipeline:
                before_clean = ItemAdapter(item).asdict()
                item = self.cleaning_pipeline.process_item(item, spider)
                adapter = ItemAdapter(item)
                # è‹¥æ¸…æ´—å content/title ä¸¢å¤±ï¼Œåˆ™å›é€€åŸå€¼ï¼Œé¿å…å…³é”®ä¿¡æ¯è¢«æ¸…ç©º
                for key in ("content", "title"):
                    if (
                        adapter.get(key) is None or adapter.get(key) == ""
                    ) and before_clean.get(key):
                        adapter[key] = before_clean.get(key)
                self.stats["cleaning_success"] += 1
                logger.debug(f"æ•°æ®æ¸…æ´—å®Œæˆ: {adapter.get('url', 'unknown')}")

            # 2. æ•°æ®éªŒè¯
            if self.config.get("enable_validation", True) and self.validation_pipeline:
                item = self.validation_pipeline.process_item(item, spider)
                adapter = ItemAdapter(item)
                validation_result = adapter.get("_validation", {})

                if validation_result.get("is_valid", True):
                    self.stats["validation_success"] += 1
                else:
                    logger.warning(f"æ•°æ®éªŒè¯å¤±è´¥: {validation_result.get('errors', {})}")

                    # å¦‚æœé…ç½®ä¸ºä¸¢å¼ƒæ— æ•ˆæ•°æ®
                    if self.config.get("drop_invalid_items", False):
                        self.stats["items_dropped"] += 1
                        raise DropItem(f"æ•°æ®éªŒè¯å¤±è´¥: {validation_result.get('errors', {})}")

            # 2.5 åŸºäºæ¸…æ´—åçš„æ ‡é¢˜/URLï¼Œç¡®ä¿ article_id ä¸ title_slug æœ€ç»ˆä¸€è‡´
            try:
                if self.defer_slug:
                    self._ensure_article_identity_final(adapter)
                else:
                    # è‹¥æœªæ¨è¿Ÿï¼Œä¹Ÿå†ç¡®ä¿ä¸€æ¬¡ä¸€è‡´æ€§ï¼ˆå¹‚ç­‰ï¼‰
                    self._ensure_article_identity_final(adapter)
            except Exception:
                pass

            # 3. è´¨é‡è¯„ä¼°
            if (
                self.config.get("enable_quality_assessment", True)
                and self.quality_monitor
            ):
                item = self.quality_monitor.monitor_item(item)
                adapter = ItemAdapter(item)
                quality_report = adapter.get("_quality_report", {})
                quality_score = quality_report.get("overall_score", 0.0)

                self.stats["quality_assessed"] += 1

                # æ£€æŸ¥æœ€ä½è´¨é‡è¦æ±‚
                min_quality_score = self.config.get("min_quality_score", 0.0)
                if quality_score < min_quality_score:
                    self.stats["items_dropped"] += 1
                    raise DropItem(f"æ•°æ®è´¨é‡è¿‡ä½: {quality_score} < {min_quality_score}")

                logger.debug(f"è´¨é‡è¯„ä¼°å®Œæˆ: {quality_score}")

            # 4. æ·»åŠ å¤„ç†å…ƒæ•°æ®
            adapter["_processing_metadata"] = {
                "processed_at": self._get_current_time(),
                "pipeline_version": "3.0",
                "processing_stages": self._get_processing_stages(),
            }

            return item

        except DropItem:
            raise
        except Exception as e:
            logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return item

    def _get_processing_stages(self):
        """è·å–å¤„ç†é˜¶æ®µä¿¡æ¯"""
        stages = []
        if self.config.get("enable_cleaning", True):
            stages.append("cleaning")
        if self.config.get("enable_validation", True):
            stages.append("validation")
        if self.config.get("enable_quality_assessment", True):
            stages.append("quality_assessment")
        return stages

    def _get_current_time(self):
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime

        return datetime.now().isoformat()

    # ===== è¾…åŠ©ï¼šåœ¨æ¸…æ´—ä¹‹åå†æ¬¡ä¿è¯èº«ä»½/slug ä¸€è‡´ =====
    def _ensure_article_identity_final(self, adapter: ItemAdapter) -> None:
        import hashlib, re
        url = str(adapter.get("url", ""))
        if url and not adapter.get("article_id"):
            adapter["article_id"] = hashlib.sha1(url.encode("utf-8")).hexdigest()[:16]
        title = str(adapter.get("title", ""))
        if title:
            # ä½¿ç”¨ä¸ DataEnrichmentPipeline ç›¸åŒçš„æ¸…æ´—è§„åˆ™
            clean_title = self._clean_title_for_slug(title) if hasattr(self, "_clean_title_for_slug") else title
            slug = re.sub(r"[\s]+", "-", re.sub(r"[^\w\-\u4e00-\u9fff]", "", clean_title)).strip("-")[:60]
            if slug:
                adapter["title_slug"] = slug

    def close_spider(self, spider):
        """çˆ¬è™«å…³é—­æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("ğŸ“Š ComprehensiveDataPipeline ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in self.stats.items():
            logger.info(f"   {key}: {value}")


class DataEnrichmentPipeline:
    """æ•°æ®ä¸°å¯ŒåŒ–ç®¡é“"""

    def __init__(self, config=None):
        self.config = config or {}
        self.stats = {
            "total_processed": 0,
            "enrichment_success": 0,
            "enrichment_failed": 0,
        }
        # æ˜¯å¦æ¨è¿Ÿåœ¨æœ¬é˜¶æ®µç”Ÿæˆ title_slugï¼ˆå•æ¬¡æå–ã€å•å‘ä¼ é€’ï¼‰
        self.defer_slug = False

        logger.info("ğŸ”§ DataEnrichmentPipeline åˆå§‹åŒ–å®Œæˆ")

    @classmethod
    def from_crawler(cls, crawler):
        config = {
            "enable_enrichment": crawler.settings.getbool(
                "ENABLE_DATA_ENRICHMENT", True
            ),
        }
        inst = cls(config)
        # å•æ¬¡æå–ã€å•å‘ä¼ é€’ï¼šæ¨è¿Ÿ slug ç”Ÿæˆåˆ°æ¸…æ´—åé˜¶æ®µ
        inst.defer_slug = crawler.settings.getbool("SINGLE_PASS_TITLE_FLOW", True)
        return inst

    def process_item(self, item, spider):
        """å¤„ç†æ•°æ®é¡¹"""
        self.stats["total_processed"] += 1
        adapter = ItemAdapter(item)

        try:
            # æ·»åŠ çˆ¬è™«ä¿¡æ¯
            adapter["spider_name"] = spider.name
            adapter["spider_version"] = getattr(spider, "version", "1.0")

            # æ·»åŠ æ—¶é—´æˆ³
            adapter["crawl_timestamp"] = self._get_current_time()

            # è®¡ç®—å†…å®¹æŒ‡çº¹
            content_fingerprint = self._calculate_content_fingerprint(adapter)
            adapter["content_fingerprint"] = content_fingerprint

            # æå–å…³é”®ä¿¡æ¯
            self._extract_key_info(adapter)

            # ç»Ÿä¸€ç”Ÿæˆæ–‡ç« æ ‡è¯†ï¼›è‹¥å¼€å¯å•æ¬¡æ•°æ®æµï¼Œåˆ™æ­¤é˜¶æ®µä¸ç”Ÿæˆ slugï¼ˆé¿å…é‡å¤æå–ï¼‰
            try:
                if self.defer_slug:
                    self._ensure_article_identity(adapter)
                    adapter.pop("title_slug", None)
                else:
                    self._ensure_article_identity(adapter)
            except Exception:
                pass

            # ç»Ÿä¸€è§„èŒƒåŒ–åª’ä½“é“¾æ¥ï¼ˆç»å¯¹åŒ–ã€åˆ—è¡¨åŒ–ã€å»ç©º/å»é‡ï¼‰
            try:
                self._normalize_media_urls(adapter)
            except Exception:
                pass

            # å†…å®¹ç±»å‹è¯†åˆ«
            try:
                adapter["content_type"] = self._classify_content_type(adapter)
            except Exception:
                # å¿½ç•¥åˆ†ç±»å¤±è´¥ï¼Œé»˜è®¤ rich_text
                if not adapter.get("content_type"):
                    adapter["content_type"] = "rich_text"

            # æ ‡å‡†åŒ–å­—æ®µå
            self._standardize_field_names(adapter)

            self.stats["enrichment_success"] += 1
            return item

        except Exception as e:
            self.stats["enrichment_failed"] += 1
            logger.error(f"æ•°æ®ä¸°å¯ŒåŒ–å¤±è´¥: {e}")
            return item

    def _calculate_content_fingerprint(self, adapter: ItemAdapter) -> str:
        """è®¡ç®—å†…å®¹æŒ‡çº¹"""
        import hashlib

        # ä½¿ç”¨URLå’Œå†…å®¹è®¡ç®—æŒ‡çº¹
        url = adapter.get("url", "")
        content = adapter.get("content", "")
        title = adapter.get("title", "")

        if isinstance(content, list):
            content = " ".join(str(c) for c in content)

        fingerprint_data = f"{url}|{title}|{content}"
        return hashlib.md5(fingerprint_data.encode("utf-8")).hexdigest()

    def _extract_key_info(self, adapter: ItemAdapter):
        """æå–å…³é”®ä¿¡æ¯"""
        content = adapter.get("content", "")
        if isinstance(content, list):
            content = " ".join(str(c) for c in content)

        content_str = str(content)

        # è®¡ç®—å†…å®¹é•¿åº¦
        adapter["content_length"] = len(content_str)

        # è®¡ç®—å­—æ•°ï¼ˆä¸­æ–‡ï¼‰
        import re

        chinese_chars = re.findall(r"[\u4e00-\u9fff]", content_str)
        adapter["chinese_char_count"] = len(chinese_chars)

        # æå–æ•°å­—ä¿¡æ¯
        numbers = re.findall(r"\d+", content_str)
        if numbers:
            adapter["numbers_found"] = [int(n) for n in numbers[:10]]  # æœ€å¤šä¿å­˜10ä¸ªæ•°å­—

    def _standardize_field_names(self, adapter: ItemAdapter):
        """æ ‡å‡†åŒ–å­—æ®µå"""
        # å­—æ®µåæ˜ å°„
        field_mapping = {
            "source_url": "url",
            "page_title": "title",
            "page_content": "content",
            "publish_time": "publish_date",
            "create_time": "crawl_timestamp",
        }

        # åº”ç”¨æ˜ å°„
        for old_name, new_name in field_mapping.items():
            if old_name in adapter and new_name not in adapter:
                adapter[new_name] = adapter[old_name]
                del adapter[old_name]

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime

        return datetime.now().isoformat()

    def get_stats(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()

    def _classify_content_type(self, adapter: ItemAdapter) -> str:
        """åŸºäºå›¾ç‰‡/æ–‡ä»¶é“¾æ¥ä¸æ–‡æœ¬é•¿åº¦çš„ç®€å•åˆ†ç±»ã€‚

        ä¼˜å…ˆå°Šé‡ä¸Šæ¸¸å·²è®¾ç½®çš„ content_typeï¼ˆä¾‹å¦‚ spider å¯¹ç›´é“¾æ–‡ä»¶åˆ¤å®šå‡ºçš„ docx/xlsx/zip ç­‰ï¼‰ã€‚
        è‹¥æ— æ˜¾å¼æŒ‡å®šï¼Œåˆ™è¿”å›: "pdf" | "image_gallery" | "rich_text"
        """
        try:
            # è‹¥ä¸Šæ¸¸ï¼ˆspider/æå–å™¨ï¼‰å·²è®¾ç½® content_typeï¼Œåˆ™ä¸è¦†å†™
            existing = adapter.get("content_type")
            if existing:
                return str(existing)

            image_urls = adapter.get("image_urls") or []
            file_urls = adapter.get("file_urls") or []
            chinese_count = adapter.get("chinese_char_count") or 0
            url = str(adapter.get("url", ""))

            # 1) æ–‡ä»¶ä¼˜å…ˆï¼šè‹¥å­˜åœ¨æ–‡ä»¶é“¾æ¥ï¼Œä½†æœªæŒ‡å®šç±»å‹ï¼Œå›é€€ä¸º pdfï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
            if file_urls:
                # è‹¥URLæ˜¾ç¤ºä¸ºpdfï¼Œæ ‡è®°pdfï¼Œå¦åˆ™é€šç”¨ 'file'
                try:
                    if any(str(u).lower().endswith(".pdf") for u in file_urls):
                        return "pdf"
                    return "file"
                except Exception:
                    return "file"
            if url.lower().endswith(".pdf"):
                return "pdf"

            # 2) çº¯å›¾ç‰‡ï¼šæœ‰å›¾ä¸”æ–‡æœ¬å¾ˆå°‘
            if image_urls and chinese_count < 20:
                return "image_gallery"

            # 3) é»˜è®¤å¯Œæ–‡æœ¬
            return "rich_text"
        except Exception:
            return "rich_text"

    def _normalize_media_urls(self, adapter: ItemAdapter) -> None:
        """ç¡®ä¿ image_urls/file_urls æ˜¯ç»å¯¹URLåˆ—è¡¨ï¼Œé¿å…ä¸‹è½½ç®¡é“æŠ¥ Missing schemeã€‚
        - å°†å­—ç¬¦ä¸²å½¢å¼çš„åˆ—è¡¨ï¼ˆå¦‚ "[...]") è½¬ä¸ºåˆ—è¡¨
        - ç›¸å¯¹è·¯å¾„åŸºäº adapter['response_meta']['url'] æˆ– adapter['url'] ç»å¯¹åŒ–
        - è¿‡æ»¤é http/https ä¸ç©ºå€¼
        - å»é‡å¹¶è®¾ç½® cover_image
        """
        import json
        import ast
        from urllib.parse import urljoin

        def ensure_list(val):
            if val is None:
                return []
            if isinstance(val, (list, tuple, set)):
                return list(val)
            if isinstance(val, str):
                s = val.strip()
                if s.startswith("[") and s.endswith("]"):
                    # å…ˆå°è¯• JSONï¼Œå†å›é€€ Python å­—é¢é‡è§£æ
                    for loader in (json.loads, ast.literal_eval):
                        try:
                            loaded = loader(s)
                            if isinstance(loaded, (list, tuple, set)):
                                return list(loaded)
                            return [str(loaded)]
                        except Exception:
                            continue
                    # éƒ½å¤±è´¥åˆ™ä½œä¸ºå•å€¼å¤„ç†
                    return [s]
                return [s]
            return [str(val)]

        def normalize_list(urls, base):
            out = []
            for u in ensure_list(urls):
                if not u:
                    continue
                u = str(u).strip().strip('"').strip("'")
                if not u:
                    continue
                abs_u = urljoin(base or "", u)
                if abs_u.startswith("http://") or abs_u.startswith("https://"):
                    out.append(abs_u)
            # å»é‡ä¿åº
            seen = set()
            result = []
            for u in out:
                if u not in seen:
                    seen.add(u)
                    result.append(u)
            return result

        base_url = (
            (adapter.get("response_meta") or {}).get("url")
            or adapter.get("url")
            or ""
        )

        adapter["image_urls"] = normalize_list(adapter.get("image_urls"), base_url)
        adapter["file_urls"] = normalize_list(adapter.get("file_urls"), base_url)

        # å°é¢å›¾ï¼šè‹¥æœªè®¾ç½®ä¸”å­˜åœ¨å›¾ç‰‡ï¼Œå–ç¬¬ä¸€å¼ 
        if adapter.get("image_urls") and not adapter.get("cover_image"):
            adapter["cover_image"] = adapter["image_urls"][0]


    def _ensure_article_identity(self, adapter: ItemAdapter) -> None:
        """ä¸ºèµ„æºä¸‹è½½ä¸å‘½åå‡†å¤‡ç¨³å®šæ ‡è¯†ä¸äººç±»å¯è¯»slugã€‚
        - article_id: å– url çš„ sha1 å‰16ä½
        - title_slug: æ ‡é¢˜å»éæ³•å­—ç¬¦ã€ç©ºç™½è½¬çŸ­æ¨ªçº¿ã€æœ€é•¿60
        """
        import hashlib, re
        url = str(adapter.get("url", ""))
        if url and not adapter.get("article_id"):
            adapter["article_id"] = hashlib.sha1(url.encode("utf-8")).hexdigest()[:16]
        title = str(adapter.get("title", ""))
        if not adapter.get("title_slug"):
            # æ¸…æ´—æ ‡é¢˜ï¼Œå»æ‰æ ·å¼ä¸²ä¸HTMLï¼Œä¼˜å…ˆä»é¦–ä¸ªä¸­æ–‡å­—ç¬¦å¼€å§‹
            clean_title = self._clean_title_for_slug(title)
            # å…è®¸ä¸­è‹±æ–‡ã€æ•°å­—ä¸è¿å­—ç¬¦ï¼›ç©ºç™½è½¬-ï¼Œæˆªæ–­é•¿åº¦
            slug = re.sub(r"[\s]+", "-", re.sub(r"[^\w\-\u4e00-\u9fff]", "", clean_title))
            slug = slug.strip("-")[:60]
            if slug:
                adapter["title_slug"] = slug


    def _clean_title_for_slug(self, title: str) -> str:
        """å»æ‰æ ·å¼ä¸²/HTMLæ ‡ç­¾ï¼Œå°½é‡ä»é¦–ä¸ªä¸­æ–‡å­—ç¬¦å¼€å§‹å–æ ‡é¢˜ã€‚
        é’ˆå¯¹ç±»ä¼¼ 'tdclasshanggao30zi18jiacualigncenterå¸‚ç–¾æ§ä¸­å¿ƒå¼€å±•...' çš„æƒ…å†µã€‚
        """
        import re
        if not title:
            return ""
        s = str(title)
        # å»æ‰ HTML æ ‡ç­¾
        s = re.sub(r"<[^>]+>", "", s)
        # å»æ‰å¸¸è§æ ·å¼ä¸²ï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼‰
        patterns = [
            r"^tdclasshanggao\w+",  # ç«™ç‚¹æ ·å¼å‰ç¼€
            r"^class\w+",
            r"^style\w+",
        ]
        for p in patterns:
            s = re.sub(p, "", s, flags=re.IGNORECASE)
        # è‹¥å­˜åœ¨ä¸­æ–‡ï¼Œæˆªæ–­åˆ°ç¬¬ä¸€ä¸ªä¸­æ–‡å­—ç¬¦å¼€å§‹
        m = re.search(r"[\u4e00-\u9fff]", s)
        if m:
            s = s[m.start():]
        return s.strip()

