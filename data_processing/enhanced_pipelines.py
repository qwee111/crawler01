# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„æ•°æ®å¤„ç†ç®¡é“

é›†æˆç¬¬ä¸‰é˜¶æ®µçš„æ‰€æœ‰æ•°æ®å¤„ç†åŠŸèƒ½
"""

import logging
from typing import Any, Dict, Optional

from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem

from .cleaner import CleaningPipeline
from .extractor import ExtractionConfigManager
from .quality_assessor import QualityMonitor
from .validator import ValidationPipeline

logger = logging.getLogger(__name__)


class EnhancedExtractionPipeline:
    """å¢å¼ºæ•°æ®æå–ç®¡é“"""

    def __init__(self, config_dir=None):
        # ä½¿ç”¨æ–°çš„é…ç½®ç®¡ç†å™¨
        from .extractor import ExtractionConfigManager

        self.extraction_manager = ExtractionConfigManager()
        self.config_dir = config_dir

        self.stats = {
            "total_processed": 0,
            "extraction_success": 0,
            "extraction_failed": 0,
            "items_dropped": 0,
        }

        logger.info("ğŸ”§ EnhancedExtractionPipeline åˆå§‹åŒ–å®Œæˆ")

    @classmethod
    def from_crawler(cls, crawler):
        config_dir = crawler.settings.get("EXTRACTION_CONFIG_DIR", "config/extraction")
        return cls(config_dir)

    def process_item(self, item, spider):
        """å¤„ç†æ•°æ®é¡¹"""
        self.stats["total_processed"] += 1
        adapter = ItemAdapter(item)

        logger.info(
            f"ğŸ”§ EnhancedExtractionPipeline å¤„ç†æ•°æ®é¡¹: {adapter.get('url', 'unknown')}"
        )

        try:
            # è·å–ç½‘ç«™åç§°
            site_name = adapter.get("site") or getattr(
                spider, "target_site", spider.name
            )
            logger.info(f"ğŸ¯ ä½¿ç”¨ç½‘ç«™é…ç½®: {site_name}")

            # ä»…å¯¹è¯¦æƒ…é¡µæ‰§è¡Œå¢å¼ºæå–ï¼Œåˆ—è¡¨é¡µç›´æ¥è·³è¿‡ï¼ˆé¿å…ç©ºå†…å®¹é€ æˆçš„è¯¯åˆ¤ï¼‰
            page_type = (
                adapter.get("page_type")
                or (adapter.get("page_analysis") or {}).get("page_type")
                or self._infer_page_type(adapter)
            )
            if page_type != "detail_page":
                logger.info(f"â­ï¸ éè¯¦æƒ…é¡µï¼Œè·³è¿‡å¢å¼ºæå–: {page_type}")
                # ä»…è·³è¿‡å¢å¼ºï¼Œä¸ä¸¢å¼ƒ item
                self.stats["items_skipped"] = self.stats.get("items_skipped", 0) + 1
                return item

            # åˆ›å»ºæ¨¡æ‹Ÿå“åº”å¯¹è±¡ï¼ˆå¸¦å®‰å…¨å›é€€ç­–ç•¥ï¼‰
            response = self._create_response_from_item(adapter)
            logger.info(f"ğŸ“„ åˆ›å»ºæ¨¡æ‹Ÿå“åº”å¯¹è±¡ï¼Œå†…å®¹é•¿åº¦: {len(response.text)}")

            # è‹¥å†…å®¹ä»ä¸ºç©ºï¼Œåˆ™æ”¾å¼ƒå¢å¼ºæå–ï¼Œé¿å…è¦†ç›–å·²æœ‰å­—æ®µ
            if not response.text:
                logger.info("â­ï¸ æ¨¡æ‹Ÿå“åº”æ— å†…å®¹ï¼Œè·³è¿‡å¢å¼ºæå–")
                return item

            # ä½¿ç”¨é…ç½®åŒ–æå–å™¨é‡æ–°æå–æ•°æ®
            extracted_data = self.extraction_manager.extract_data(response, site_name)
            logger.info(f"ğŸ“Š é…ç½®åŒ–æå–ç»“æœ: {len(extracted_data)} ä¸ªå­—æ®µ")

            # åˆå¹¶æå–çš„æ•°æ®ï¼ˆä»…æ›´æ–°éç©ºå€¼ï¼Œå¹¶åšå­—æ®µåå…¼å®¹æ˜ å°„ï¼‰
            updated_fields = 0
            for key, value in extracted_data.items():
                if value is not None:
                    adapter[key] = value
                    updated_fields += 1

            # å…¼å®¹æ˜ å°„ï¼šarticle_title/article_content â†’ title/content
            if (
                adapter.get("title") is None
                and adapter.get("article_title") is not None
            ):
                adapter["title"] = adapter.get("article_title")
                updated_fields += 1
            content_val = adapter.get("content") or adapter.get("article_content")
            if isinstance(content_val, list):
                content_val = " ".join(
                    [str(x).strip() for x in content_val if str(x).strip()]
                )
            if adapter.get("content") is None and content_val:
                adapter["content"] = content_val
                updated_fields += 1

            # æ›´æ–°å†…å®¹ç»Ÿè®¡ï¼ˆé¿å…ä¸ DataEnrichmentPipeline é‡å¤ï¼‰
            if adapter.get("content"):
                try:
                    txt = adapter.get("content")
                    if "content_length" not in adapter:
                        adapter["content_length"] = len(txt)
                    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°
                    import re

                    if "chinese_char_count" not in adapter:
                        adapter["chinese_char_count"] = len(
                            re.findall(r"[\u4e00-\u9fff]", txt)
                        )
                except Exception:
                    pass

            logger.info(f"ğŸ“ æ›´æ–°äº† {updated_fields} ä¸ªå­—æ®µ")

            # æ·»åŠ æå–å…ƒæ•°æ®
            adapter["_extraction_metadata"] = {
                "extractor_used": site_name,
                "extraction_method": "enhanced",
                "fields_extracted": len(extracted_data),
            }

            self.stats["extraction_success"] += 1
            logger.info(f"âœ… å¢å¼ºæå–æˆåŠŸ: {adapter.get('url', 'unknown')}")

            return item

        except Exception as e:
            self.stats["extraction_failed"] += 1
            logger.error(f"âŒ å¢å¼ºæå–å¤±è´¥: {e}")
            import traceback

            logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

            # ä¿ç•™åŸå§‹æ•°æ®ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
            adapter["_extraction_error"] = str(e)
            return item

    def _create_response_from_item(self, adapter: ItemAdapter):
        """ä»æ•°æ®é¡¹åˆ›å»ºå“åº”å¯¹è±¡ï¼ˆå¸¦å®‰å…¨å›é€€ï¼‰"""

        class MockResponse:
            def __init__(self, url, text, status=200):
                self.url = url
                self.text = text
                self.status = status
                self.body = text.encode("utf-8") if text else b""
                self.headers = {}

            def json(self):
                import json

                return json.loads(self.text)

        url = adapter.get("url", "")
        # ä¼˜å…ˆé¡ºåºï¼šraw_html > content > article_content > full_content > text
        candidates = [
            adapter.get("raw_html"),
            adapter.get("content"),
            adapter.get("article_content"),
            adapter.get("full_content"),
            adapter.get("text"),
        ]
        content = None
        for val in candidates:
            if val:
                content = val
                break

        # åˆ—è¡¨/å¤šæ®µå†…å®¹åˆå¹¶
        if isinstance(content, list):
            content = " ".join([str(c).strip() for c in content if str(c).strip()])

        status = adapter.get("status") or adapter.get("status_code") or 200

        return MockResponse(url, str(content or ""), status)

    def _infer_page_type(self, adapter: ItemAdapter) -> str:
        """æ ¹æ®URLä¸å­—æ®µå¯å‘å¼åˆ¤æ–­é¡µé¢ç±»å‹"""
        try:
            url = (adapter.get("url") or "").lower()
            # æ˜æ˜¾çš„URLæ¨¡å¼
            if any(kw in url for kw in ["list", "index", "category", "åˆ—è¡¨"]):
                return "list_page"
            import re

            if re.search(r"/\d+\.s?html$", url) or any(
                kw in url for kw in ["detail", "article"]
            ):
                return "detail_page"
            # æ ¹æ®å­—æ®µç‰¹å¾
            items_val = adapter.get("items")
            if items_val:
                if isinstance(items_val, list) and len(items_val) >= 3:
                    return "list_page"
                if (
                    isinstance(items_val, str)
                    and "[" in items_val
                    and "title" in items_val
                ):
                    return "list_page"
            content_val = adapter.get("content") or adapter.get("article_content")
            if content_val:
                if isinstance(content_val, str) and len(content_val.strip()) > 50:
                    return "detail_page"
                if (
                    isinstance(content_val, list)
                    and len(" ".join(map(str, content_val))) > 50
                ):
                    return "detail_page"
        except Exception:
            pass
        return "unknown_page"

    def get_stats(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()


class ComprehensiveDataPipeline:
    """ç»¼åˆæ•°æ®å¤„ç†ç®¡é“"""

    def __init__(self, config=None):
        self.config = config or {}

        # åˆå§‹åŒ–å­ç®¡é“
        try:
            from .cleaner import CleaningPipeline
            from .quality_assessor import QualityMonitor
            from .validator import ValidationPipeline

            self.cleaning_pipeline = CleaningPipeline()
            self.validation_pipeline = ValidationPipeline()
            self.quality_monitor = QualityMonitor()

            logger.info("âœ… æ‰€æœ‰å­ç®¡é“åˆå§‹åŒ–æˆåŠŸ")

        except ImportError as e:
            logger.warning(f"âš ï¸ éƒ¨åˆ†å­ç®¡é“åˆå§‹åŒ–å¤±è´¥: {e}")
            # åˆ›å»ºç®€åŒ–ç‰ˆæœ¬
            self.cleaning_pipeline = None
            self.validation_pipeline = None
            self.quality_monitor = None

        self.stats = {
            "total_processed": 0,
            "cleaning_success": 0,
            "validation_success": 0,
            "quality_assessed": 0,
            "items_dropped": 0,
        }

        logger.info("ğŸ”§ ComprehensiveDataPipeline åˆå§‹åŒ–å®Œæˆ")

    @classmethod
    def from_crawler(cls, crawler):
        config = {
            "enable_cleaning": crawler.settings.getbool("ENABLE_DATA_CLEANING", True),
            "enable_validation": crawler.settings.getbool(
                "ENABLE_DATA_VALIDATION", True
            ),
            "enable_quality_assessment": crawler.settings.getbool(
                "ENABLE_QUALITY_ASSESSMENT", True
            ),
            "drop_invalid_items": crawler.settings.getbool("DROP_INVALID_ITEMS", False),
            "min_quality_score": crawler.settings.getfloat("MIN_QUALITY_SCORE", 0.0),
        }
        return cls(config)

    def process_item(self, item, spider):
        """å¤„ç†æ•°æ®é¡¹"""
        self.stats["total_processed"] += 1
        adapter = ItemAdapter(item)

        try:
            # 1. æ•°æ®æ¸…æ´—
            if self.config.get("enable_cleaning", True) and self.cleaning_pipeline:
                item = self.cleaning_pipeline.process_item(item, spider)
                adapter = ItemAdapter(item)
                self.stats["cleaning_success"] += 1
                logger.debug(f"æ•°æ®æ¸…æ´—å®Œæˆ: {adapter.get('url', 'unknown')}")

            # 2. æ•°æ®éªŒè¯
            if self.config.get("enable_validation", True) and self.validation_pipeline:
                item = self.validation_pipeline.process_item(item, spider)
                adapter = ItemAdapter(item)
                validation_result = adapter.get("_validation", {})

                if validation_result.get("is_valid", True):
                    self.stats["validation_success"] += 1
                else:
                    logger.warning(f"æ•°æ®éªŒè¯å¤±è´¥: {validation_result.get('errors', {})}")

                    # å¦‚æœé…ç½®ä¸ºä¸¢å¼ƒæ— æ•ˆæ•°æ®
                    if self.config.get("drop_invalid_items", False):
                        self.stats["items_dropped"] += 1
                        raise DropItem(f"æ•°æ®éªŒè¯å¤±è´¥: {validation_result.get('errors', {})}")

            # 3. è´¨é‡è¯„ä¼°
            if (
                self.config.get("enable_quality_assessment", True)
                and self.quality_monitor
            ):
                item = self.quality_monitor.monitor_item(item)
                adapter = ItemAdapter(item)
                quality_report = adapter.get("_quality_report", {})
                quality_score = quality_report.get("overall_score", 0.0)

                self.stats["quality_assessed"] += 1

                # æ£€æŸ¥æœ€ä½è´¨é‡è¦æ±‚
                min_quality_score = self.config.get("min_quality_score", 0.0)
                if quality_score < min_quality_score:
                    self.stats["items_dropped"] += 1
                    raise DropItem(f"æ•°æ®è´¨é‡è¿‡ä½: {quality_score} < {min_quality_score}")

                logger.debug(f"è´¨é‡è¯„ä¼°å®Œæˆ: {quality_score}")

            # 4. æ·»åŠ å¤„ç†å…ƒæ•°æ®
            adapter["_processing_metadata"] = {
                "processed_at": self._get_current_time(),
                "pipeline_version": "3.0",
                "processing_stages": self._get_processing_stages(),
            }

            return item

        except DropItem:
            raise
        except Exception as e:
            logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return item

    def _get_processing_stages(self):
        """è·å–å¤„ç†é˜¶æ®µä¿¡æ¯"""
        stages = []
        if self.config.get("enable_cleaning", True):
            stages.append("cleaning")
        if self.config.get("enable_validation", True):
            stages.append("validation")
        if self.config.get("enable_quality_assessment", True):
            stages.append("quality_assessment")
        return stages

    def _get_current_time(self):
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime

        return datetime.now().isoformat()

    def close_spider(self, spider):
        """çˆ¬è™«å…³é—­æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("ğŸ“Š ComprehensiveDataPipeline ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in self.stats.items():
            logger.info(f"   {key}: {value}")


class DataEnrichmentPipeline:
    """æ•°æ®ä¸°å¯ŒåŒ–ç®¡é“"""

    def __init__(self, config=None):
        self.config = config or {}
        self.stats = {
            "total_processed": 0,
            "enrichment_success": 0,
            "enrichment_failed": 0,
        }

        logger.info("ğŸ”§ DataEnrichmentPipeline åˆå§‹åŒ–å®Œæˆ")

    @classmethod
    def from_crawler(cls, crawler):
        config = {
            "enable_enrichment": crawler.settings.getbool(
                "ENABLE_DATA_ENRICHMENT", True
            ),
        }
        return cls(config)

    def process_item(self, item, spider):
        """å¤„ç†æ•°æ®é¡¹"""
        self.stats["total_processed"] += 1
        adapter = ItemAdapter(item)

        try:
            # æ·»åŠ çˆ¬è™«ä¿¡æ¯
            adapter["spider_name"] = spider.name
            adapter["spider_version"] = getattr(spider, "version", "1.0")

            # æ·»åŠ æ—¶é—´æˆ³
            adapter["crawl_timestamp"] = self._get_current_time()

            # è®¡ç®—å†…å®¹æŒ‡çº¹
            content_fingerprint = self._calculate_content_fingerprint(adapter)
            adapter["content_fingerprint"] = content_fingerprint

            # æå–å…³é”®ä¿¡æ¯
            self._extract_key_info(adapter)

            # æ ‡å‡†åŒ–å­—æ®µå
            self._standardize_field_names(adapter)

            self.stats["enrichment_success"] += 1
            return item

        except Exception as e:
            self.stats["enrichment_failed"] += 1
            logger.error(f"æ•°æ®ä¸°å¯ŒåŒ–å¤±è´¥: {e}")
            return item

    def _calculate_content_fingerprint(self, adapter: ItemAdapter) -> str:
        """è®¡ç®—å†…å®¹æŒ‡çº¹"""
        import hashlib

        # ä½¿ç”¨URLå’Œå†…å®¹è®¡ç®—æŒ‡çº¹
        url = adapter.get("url", "")
        content = adapter.get("content", "")
        title = adapter.get("title", "")

        if isinstance(content, list):
            content = " ".join(str(c) for c in content)

        fingerprint_data = f"{url}|{title}|{content}"
        return hashlib.md5(fingerprint_data.encode("utf-8")).hexdigest()

    def _extract_key_info(self, adapter: ItemAdapter):
        """æå–å…³é”®ä¿¡æ¯"""
        content = adapter.get("content", "")
        if isinstance(content, list):
            content = " ".join(str(c) for c in content)

        content_str = str(content)

        # è®¡ç®—å†…å®¹é•¿åº¦
        adapter["content_length"] = len(content_str)

        # è®¡ç®—å­—æ•°ï¼ˆä¸­æ–‡ï¼‰
        import re

        chinese_chars = re.findall(r"[\u4e00-\u9fff]", content_str)
        adapter["chinese_char_count"] = len(chinese_chars)

        # æå–æ•°å­—ä¿¡æ¯
        numbers = re.findall(r"\d+", content_str)
        if numbers:
            adapter["numbers_found"] = [int(n) for n in numbers[:10]]  # æœ€å¤šä¿å­˜10ä¸ªæ•°å­—

    def _standardize_field_names(self, adapter: ItemAdapter):
        """æ ‡å‡†åŒ–å­—æ®µå"""
        # å­—æ®µåæ˜ å°„
        field_mapping = {
            "source_url": "url",
            "page_title": "title",
            "page_content": "content",
            "publish_time": "publish_date",
            "create_time": "crawl_timestamp",
        }

        # åº”ç”¨æ˜ å°„
        for old_name, new_name in field_mapping.items():
            if old_name in adapter and new_name not in adapter:
                adapter[new_name] = adapter[old_name]
                del adapter[old_name]

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime

        return datetime.now().isoformat()

    def get_stats(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
