#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘ç«™è®¿é—®å’Œç»“æ„åˆ†æå·¥å…·

å…ˆç¡®ä¿èƒ½å¤Ÿè®¿é—®ç›®æ ‡ç½‘ç«™ï¼Œè·å–æºç ï¼Œç„¶ååˆ†æé¡µé¢ç»“æ„ç”Ÿæˆæå–è§„åˆ™
"""

import json
import logging
import os
import sys
import time
from pathlib import Path
from urllib.parse import urljoin, urlparse

import requests

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    from bs4 import BeautifulSoup

    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False
    logger.warning("BeautifulSoupæœªå®‰è£…ï¼Œå°†è·³è¿‡HTMLåˆ†æåŠŸèƒ½")

try:
    from lxml import html

    LXML_AVAILABLE = True
except ImportError:
    LXML_AVAILABLE = False
    logger.warning("lxmlæœªå®‰è£…ï¼Œå°†è·³è¿‡XPathåˆ†æåŠŸèƒ½")


class WebsiteAnalyzer:
    """ç½‘ç«™åˆ†æå™¨"""

    def __init__(self, base_url):
        self.base_url = base_url
        self.domain = urlparse(base_url).netloc
        self.session = requests.Session()
        self.results = {
            "base_url": base_url,
            "domain": self.domain,
            "access_tests": [],
            "page_analysis": {},
            "extraction_rules": {},
            "recommendations": [],
        }

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(f"analysis_{self.domain.replace('.', '_')}")
        self.output_dir.mkdir(exist_ok=True)

        logger.info(f"åˆå§‹åŒ–ç½‘ç«™åˆ†æå™¨: {base_url}")

    def test_access_methods(self):
        """æµ‹è¯•å¤šç§è®¿é—®æ–¹å¼"""
        logger.info("ğŸŒ æµ‹è¯•ç½‘ç«™è®¿é—®æ–¹å¼...")

        # ä¸åŒçš„User-Agent
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]

        # æµ‹è¯•URLå˜ä½“
        test_urls = [
            self.base_url,
            self.base_url.replace("https://", "http://"),
            self.base_url.replace("http://", "https://"),
        ]

        # å»é‡
        test_urls = list(set(test_urls))

        for url in test_urls:
            for i, ua in enumerate(user_agents):
                test_name = f"{url} (UA{i+1})"
                logger.info(f"   æµ‹è¯•: {test_name}")

                try:
                    headers = {
                        "User-Agent": ua,
                        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                        "Accept-Encoding": "gzip, deflate, br",
                        "Connection": "keep-alive",
                        "Upgrade-Insecure-Requests": "1",
                        "Sec-Fetch-Dest": "document",
                        "Sec-Fetch-Mode": "navigate",
                        "Sec-Fetch-Site": "none",
                        "Cache-Control": "max-age=0",
                    }

                    start_time = time.time()
                    response = self.session.get(
                        url,
                        headers=headers,
                        timeout=30,
                        verify=False,  # å¿½ç•¥SSLè¯ä¹¦é—®é¢˜
                        allow_redirects=True,
                    )
                    end_time = time.time()

                    test_result = {
                        "url": url,
                        "user_agent": ua,
                        "status_code": response.status_code,
                        "response_time": round(end_time - start_time, 2),
                        "content_length": len(response.content),
                        "content_type": response.headers.get("Content-Type", ""),
                        "final_url": response.url,
                        "redirects": len(response.history),
                        "success": response.status_code == 200
                        and len(response.content) > 1000,
                        "error": None,
                    }

                    logger.info(
                        f"     âœ… çŠ¶æ€ç : {response.status_code}, å¤§å°: {len(response.content)} å­—èŠ‚, æ—¶é—´: {test_result['response_time']}ç§’"
                    )

                    # å¦‚æœæˆåŠŸï¼Œä¿å­˜å†…å®¹
                    if test_result["success"]:
                        self.save_page_content(response, f"success_{i+1}")
                        test_result["content_saved"] = True

                        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æˆåŠŸï¼Œè¿›è¡Œè¯¦ç»†åˆ†æ
                        if not any(
                            t.get("analyzed") for t in self.results["access_tests"]
                        ):
                            self.analyze_page_structure(response)
                            test_result["analyzed"] = True

                    self.results["access_tests"].append(test_result)

                    # å¦‚æœæˆåŠŸäº†ï¼Œå¯ä»¥è·³è¿‡å…¶ä»–User-Agentæµ‹è¯•
                    if test_result["success"]:
                        logger.info(f"   âœ… è®¿é—®æˆåŠŸï¼Œè·³è¿‡å…¶ä»–User-Agentæµ‹è¯•")
                        return True

                except requests.exceptions.Timeout:
                    error_msg = "è¯·æ±‚è¶…æ—¶"
                    logger.warning(f"     â° {error_msg}")
                except requests.exceptions.ConnectionError as e:
                    error_msg = f"è¿æ¥é”™è¯¯: {str(e)[:100]}"
                    logger.warning(f"     ğŸ”Œ {error_msg}")
                except requests.exceptions.SSLError as e:
                    error_msg = f"SSLé”™è¯¯: {str(e)[:100]}"
                    logger.warning(f"     ğŸ”’ {error_msg}")
                except Exception as e:
                    error_msg = f"å…¶ä»–é”™è¯¯: {str(e)[:100]}"
                    logger.warning(f"     âŒ {error_msg}")

                # æ·»åŠ å¤±è´¥è®°å½•
                self.results["access_tests"].append(
                    {"url": url, "user_agent": ua, "success": False, "error": error_msg}
                )

                # æ·»åŠ å»¶è¿Ÿé¿å…è§¦å‘åçˆ¬è™«
                time.sleep(2)

        return False

    def save_page_content(self, response, suffix=""):
        """ä¿å­˜é¡µé¢å†…å®¹"""
        # ä¿å­˜HTMLæºç 
        html_file = self.output_dir / f"page_source_{suffix}.html"
        with open(html_file, "w", encoding="utf-8") as f:
            f.write(response.text)

        # ä¿å­˜å“åº”å¤´
        headers_file = self.output_dir / f"response_headers_{suffix}.json"
        with open(headers_file, "w", encoding="utf-8") as f:
            json.dump(dict(response.headers), f, indent=2, ensure_ascii=False)

        logger.info(f"   ğŸ’¾ é¡µé¢å†…å®¹å·²ä¿å­˜: {html_file}")

    def analyze_page_structure(self, response):
        """åˆ†æé¡µé¢ç»“æ„"""
        logger.info("ğŸ” åˆ†æé¡µé¢ç»“æ„...")

        html_content = response.text

        analysis = {
            "url": response.url,
            "title": "",
            "meta_info": {},
            "structure_analysis": {},
            "potential_selectors": {},
        }

        if BS4_AVAILABLE:
            soup = BeautifulSoup(html_content, "html.parser")

            # åŸºæœ¬ä¿¡æ¯
            if soup.title:
                analysis["title"] = soup.title.get_text().strip()
                logger.info(f"   ğŸ“ é¡µé¢æ ‡é¢˜: {analysis['title']}")

            # Metaä¿¡æ¯
            for meta in soup.find_all("meta"):
                name = (
                    meta.get("name") or meta.get("property") or meta.get("http-equiv")
                )
                content = meta.get("content")
                if name and content:
                    analysis["meta_info"][name] = content

            # ç»“æ„åˆ†æ
            analysis["structure_analysis"] = {
                "total_elements": len(soup.find_all()),
                "div_count": len(soup.find_all("div")),
                "p_count": len(soup.find_all("p")),
                "a_count": len(soup.find_all("a")),
                "img_count": len(soup.find_all("img")),
                "h1_count": len(soup.find_all("h1")),
                "h2_count": len(soup.find_all("h2")),
                "h3_count": len(soup.find_all("h3")),
            }

            # å¯»æ‰¾æ½œåœ¨çš„å†…å®¹é€‰æ‹©å™¨
            self.find_content_selectors(soup, analysis)

        self.results["page_analysis"] = analysis

        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = self.output_dir / "page_analysis.json"
        with open(analysis_file, "w", encoding="utf-8") as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)

        logger.info(f"   ğŸ“Š é¡µé¢åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜: {analysis_file}")

    def find_content_selectors(self, soup, analysis):
        """å¯»æ‰¾å†…å®¹é€‰æ‹©å™¨"""
        logger.info("ğŸ¯ å¯»æ‰¾æ½œåœ¨çš„å†…å®¹é€‰æ‹©å™¨...")

        selectors = {}

        # æ ‡é¢˜é€‰æ‹©å™¨
        title_candidates = []
        for tag in ["h1", "h2", "h3"]:
            elements = soup.find_all(tag)
            for elem in elements:
                text = elem.get_text().strip()
                if len(text) > 5 and len(text) < 200:  # åˆç†çš„æ ‡é¢˜é•¿åº¦
                    selector_info = {
                        "tag": tag,
                        "text": text[:50] + "..." if len(text) > 50 else text,
                        "class": elem.get("class"),
                        "id": elem.get("id"),
                    }
                    title_candidates.append(selector_info)

        selectors["title_candidates"] = title_candidates[:5]  # åªä¿ç•™å‰5ä¸ª

        # å†…å®¹é€‰æ‹©å™¨
        content_candidates = []
        for class_name in [
            "content",
            "article",
            "main",
            "text",
            "body",
            "article-content",
            "post-content",
        ]:
            elements = soup.find_all(
                attrs={"class": lambda x: x and class_name in " ".join(x).lower()}
            )
            for elem in elements:
                text = elem.get_text().strip()
                if len(text) > 100:  # å†…å®¹åº”è¯¥æ¯”è¾ƒé•¿
                    selector_info = {
                        "class": elem.get("class"),
                        "id": elem.get("id"),
                        "tag": elem.name,
                        "text_length": len(text),
                        "text_preview": text[:100] + "..." if len(text) > 100 else text,
                    }
                    content_candidates.append(selector_info)

        selectors["content_candidates"] = content_candidates[:5]

        # æ—¥æœŸé€‰æ‹©å™¨
        date_candidates = []
        import re

        date_pattern = r"\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}"

        for elem in soup.find_all(text=re.compile(date_pattern)):
            parent = elem.parent
            if parent:
                selector_info = {
                    "tag": parent.name,
                    "class": parent.get("class"),
                    "id": parent.get("id"),
                    "text": elem.strip(),
                }
                date_candidates.append(selector_info)

        selectors["date_candidates"] = date_candidates[:5]

        analysis["potential_selectors"] = selectors

        # è¾“å‡ºå‘ç°çš„é€‰æ‹©å™¨
        logger.info(f"   ğŸ“ æ‰¾åˆ° {len(title_candidates)} ä¸ªæ ‡é¢˜å€™é€‰")
        logger.info(f"   ğŸ“„ æ‰¾åˆ° {len(content_candidates)} ä¸ªå†…å®¹å€™é€‰")
        logger.info(f"   ğŸ“… æ‰¾åˆ° {len(date_candidates)} ä¸ªæ—¥æœŸå€™é€‰")

    def generate_extraction_rules(self):
        """ç”Ÿæˆæå–è§„åˆ™"""
        logger.info("âš™ï¸ ç”Ÿæˆæå–è§„åˆ™...")

        if not self.results["page_analysis"]:
            logger.warning("   âš ï¸ æ²¡æœ‰é¡µé¢åˆ†ææ•°æ®ï¼Œæ— æ³•ç”Ÿæˆè§„åˆ™")
            return

        selectors = self.results["page_analysis"].get("potential_selectors", {})
        rules = {}

        # æ ‡é¢˜è§„åˆ™
        title_candidates = selectors.get("title_candidates", [])
        if title_candidates:
            title_xpaths = []
            for candidate in title_candidates[:3]:  # å–å‰3ä¸ª
                if candidate.get("class"):
                    class_str = " ".join(candidate["class"])
                    title_xpaths.append(
                        f"//{candidate['tag']}[@class='{class_str}']//text()"
                    )
                elif candidate.get("id"):
                    title_xpaths.append(
                        f"//{candidate['tag']}[@id='{candidate['id']}']//text()"
                    )
                else:
                    title_xpaths.append(f"//{candidate['tag']}//text()")

            rules["title"] = {
                "method": "xpath",
                "selector": " | ".join(title_xpaths),
                "type": "string",
                "required": True,
                "description": "é¡µé¢æ ‡é¢˜",
            }

        # å†…å®¹è§„åˆ™
        content_candidates = selectors.get("content_candidates", [])
        if content_candidates:
            content_xpaths = []
            for candidate in content_candidates[:3]:
                if candidate.get("class"):
                    class_str = " ".join(candidate["class"])
                    content_xpaths.append(f"//div[@class='{class_str}']//text()")
                elif candidate.get("id"):
                    content_xpaths.append(f"//div[@id='{candidate['id']}']//text()")

            if content_xpaths:
                rules["content"] = {
                    "method": "xpath",
                    "selector": " | ".join(content_xpaths),
                    "type": "string",
                    "multiple": True,
                    "required": True,
                    "description": "é¡µé¢å†…å®¹",
                }

        # æ—¥æœŸè§„åˆ™
        date_candidates = selectors.get("date_candidates", [])
        if date_candidates:
            date_xpaths = []
            for candidate in date_candidates[:3]:
                if candidate.get("class"):
                    class_str = " ".join(candidate["class"])
                    date_xpaths.append(f"//*[@class='{class_str}']//text()")
                elif candidate.get("id"):
                    date_xpaths.append(f"//*[@id='{candidate['id']}']//text()")

            if date_xpaths:
                rules["publish_date"] = {
                    "method": "xpath",
                    "selector": " | ".join(date_xpaths),
                    "type": "date",
                    "description": "å‘å¸ƒæ—¥æœŸ",
                }

        self.results["extraction_rules"] = rules

        # ä¿å­˜è§„åˆ™
        rules_file = self.output_dir / "extraction_rules.yaml"
        import yaml

        with open(rules_file, "w", encoding="utf-8") as f:
            yaml.dump(
                {"fields": rules}, f, default_flow_style=False, allow_unicode=True
            )

        logger.info(f"   âœ… æå–è§„åˆ™å·²ç”Ÿæˆ: {rules_file}")
        logger.info(f"   ğŸ“Š ç”Ÿæˆäº† {len(rules)} ä¸ªå­—æ®µè§„åˆ™")

    def test_extraction_rules(self):
        """æµ‹è¯•æå–è§„åˆ™"""
        logger.info("ğŸ§ª æµ‹è¯•æå–è§„åˆ™...")

        if not self.results["extraction_rules"]:
            logger.warning("   âš ï¸ æ²¡æœ‰æå–è§„åˆ™ï¼Œè·³è¿‡æµ‹è¯•")
            return

        # è·å–ä¿å­˜çš„HTMLæ–‡ä»¶
        html_files = list(self.output_dir.glob("page_source_*.html"))
        if not html_files:
            logger.warning("   âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä¿å­˜çš„HTMLæ–‡ä»¶")
            return

        html_file = html_files[0]
        with open(html_file, "r", encoding="utf-8") as f:
            html_content = f.read()

        if LXML_AVAILABLE:
            tree = html.fromstring(html_content)

            test_results = {}
            for field_name, rule in self.results["extraction_rules"].items():
                try:
                    selector = rule["selector"]
                    elements = tree.xpath(selector)

                    if rule.get("multiple"):
                        result = [elem.strip() for elem in elements if elem.strip()]
                    else:
                        result = elements[0].strip() if elements else None

                    test_results[field_name] = {
                        "success": bool(result),
                        "result": result,
                        "count": len(elements)
                        if isinstance(elements, list)
                        else (1 if result else 0),
                    }

                    if result:
                        preview = (
                            str(result)[:100] + "..."
                            if len(str(result)) > 100
                            else str(result)
                        )
                        logger.info(f"   âœ… {field_name}: {preview}")
                    else:
                        logger.warning(f"   âŒ {field_name}: æœªæå–åˆ°æ•°æ®")

                except Exception as e:
                    test_results[field_name] = {"success": False, "error": str(e)}
                    logger.error(f"   âŒ {field_name}: æå–å¤±è´¥ - {e}")

            # ä¿å­˜æµ‹è¯•ç»“æœ
            test_file = self.output_dir / "extraction_test_results.json"
            with open(test_file, "w", encoding="utf-8") as f:
                json.dump(test_results, f, indent=2, ensure_ascii=False)

            logger.info(f"   ğŸ“Š æå–æµ‹è¯•å®Œæˆ: {test_file}")

    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")

        # ç»Ÿè®¡æˆåŠŸçš„è®¿é—®æµ‹è¯•
        successful_tests = [t for t in self.results["access_tests"] if t.get("success")]

        report = {
            "website": self.base_url,
            "analysis_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "summary": {
                "total_access_tests": len(self.results["access_tests"]),
                "successful_tests": len(successful_tests),
                "access_success_rate": len(successful_tests)
                / len(self.results["access_tests"])
                if self.results["access_tests"]
                else 0,
                "page_analyzed": bool(self.results["page_analysis"]),
                "rules_generated": len(self.results["extraction_rules"]),
            },
            "recommendations": [],
        }

        # ç”Ÿæˆå»ºè®®
        if successful_tests:
            report["recommendations"].append("âœ… ç½‘ç«™å¯ä»¥æ­£å¸¸è®¿é—®")
            best_test = max(successful_tests, key=lambda x: x.get("content_length", 0))
            report["recommendations"].append(f"ğŸ¯ æ¨èä½¿ç”¨çš„è®¿é—®æ–¹å¼: {best_test['url']}")
        else:
            report["recommendations"].append("âŒ ç½‘ç«™è®¿é—®å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–åçˆ¬è™«ç­–ç•¥")

        if self.results["extraction_rules"]:
            report["recommendations"].append(
                f"âš™ï¸ å·²ç”Ÿæˆ {len(self.results['extraction_rules'])} ä¸ªæå–è§„åˆ™"
            )
        else:
            report["recommendations"].append("âš ï¸ æœªèƒ½ç”Ÿæˆæå–è§„åˆ™ï¼Œéœ€è¦æ‰‹åŠ¨åˆ†æé¡µé¢ç»“æ„")

        # ä¿å­˜å®Œæ•´æŠ¥å‘Š
        report_file = self.output_dir / "analysis_report.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(
                {**report, "detailed_results": self.results},
                f,
                indent=2,
                ensure_ascii=False,
            )

        logger.info(f"ğŸ“Š åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

        # è¾“å‡ºæ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“‹ ç½‘ç«™åˆ†ææŠ¥å‘Šæ‘˜è¦")
        print("=" * 60)
        print(f"ğŸŒ ç›®æ ‡ç½‘ç«™: {self.base_url}")
        print(
            f"ğŸ“Š è®¿é—®æµ‹è¯•: {report['summary']['successful_tests']}/{report['summary']['total_access_tests']} æˆåŠŸ"
        )
        print(f"ğŸ“„ é¡µé¢åˆ†æ: {'âœ… å®Œæˆ' if report['summary']['page_analyzed'] else 'âŒ å¤±è´¥'}")
        print(f"âš™ï¸ æå–è§„åˆ™: {report['summary']['rules_generated']} ä¸ª")
        print("\nğŸ’¡ å»ºè®®:")
        for rec in report["recommendations"]:
            print(f"   {rec}")
        print(f"\nğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {self.output_dir}")

        return report


def main():
    """ä¸»å‡½æ•°"""
    # ç›®æ ‡ç½‘ç«™
    target_url = "https://www.bjcdc.org/"

    print("ğŸ¯ ç½‘ç«™è®¿é—®å’Œç»“æ„åˆ†æå·¥å…·")
    print("=" * 60)
    print(f"ğŸŒ ç›®æ ‡ç½‘ç«™: {target_url}")
    print()

    # åˆ›å»ºåˆ†æå™¨
    analyzer = WebsiteAnalyzer(target_url)

    try:
        # 1. æµ‹è¯•è®¿é—®
        access_success = analyzer.test_access_methods()

        if access_success:
            # 2. ç”Ÿæˆæå–è§„åˆ™
            analyzer.generate_extraction_rules()

            # 3. æµ‹è¯•æå–è§„åˆ™
            analyzer.test_extraction_rules()

        # 4. ç”ŸæˆæŠ¥å‘Š
        analyzer.generate_report()

        return access_success

    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­åˆ†æ")
        return False
    except Exception as e:
        logger.error(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
