#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®ç®¡é“æ£€æŸ¥è„šæœ¬
"""

import os
from datetime import datetime

import pymongo


def check_data_pipeline():
    """æ£€æŸ¥æ•°æ®ç®¡é“å’Œæ•°æ®ä¿å­˜æªæ–½"""

    # è¿æ¥åˆ°MongoDB
    mongo_uri = "mongodb://admin:password123@localhost:27017/"
    print(f"è¿æ¥MongoDB: {mongo_uri}")
    try:
        client = pymongo.MongoClient(mongo_uri)
        db = client["crawler_db"]
        print("MongoDBè¿æ¥æˆåŠŸ")

        print("ğŸ” æ•°æ®ç®¡é“æ£€æŸ¥æŠ¥å‘Š")
        print("=" * 60)
        print()

        # 1. æ£€æŸ¥æ‰€æœ‰é›†åˆ
        collections = db.list_collection_names()
        print("ğŸ“Š æ•°æ®åº“ä¸­çš„é›†åˆ:")
        total_records = 0
        for i, collection_name in enumerate(collections, 1):
            collection = db[collection_name]
            count = collection.count_documents({})
            total_records += count
            print(f"  {i}. {collection_name}: {count:,} æ¡è®°å½•")

        print(f"ğŸ“ˆ æ€»è®°å½•æ•°: {total_records:,}")
        print()

        # 2. æ£€æŸ¥adaptive_dataé›†åˆè¯¦æƒ…
        if "adaptive_data" in collections:
            adaptive_collection = db["adaptive_data"]
            count = adaptive_collection.count_documents({})
            print("ğŸ¯ adaptive_dataé›†åˆè¯¦ç»†ä¿¡æ¯:")
            print(f"   ğŸ“Š æ€»è®°å½•æ•°: {count:,}")

            # æ£€æŸ¥æœ€æ–°è®°å½•
            latest_doc = adaptive_collection.find_one(sort=[("_id", -1)])
            if latest_doc:
                print(f'   ğŸ“… æœ€æ–°è®°å½•ID: {str(latest_doc["_id"])}')
                print(f'   ğŸŒ æœ€æ–°URL: {latest_doc.get("url", "N/A")}')
                title = latest_doc.get("title", "N/A")
                if len(title) > 50:
                    title = title[:50] + "..."
                print(f"   ğŸ“ æ ‡é¢˜: {title}")

                # æ£€æŸ¥å­—æ®µå®Œæ•´æ€§
                required_fields = ["url", "title", "content", "crawl_timestamp"]
                missing_fields = [
                    field for field in required_fields if field not in latest_doc
                ]
                if missing_fields:
                    print(f"   âš ï¸ ç¼ºå¤±å­—æ®µ: {missing_fields}")
                else:
                    print("   âœ… å¿…éœ€å­—æ®µå®Œæ•´")

                # æ˜¾ç¤ºæ‰€æœ‰å­—æ®µ
                print(f"   ğŸ“‹ å­—æ®µåˆ—è¡¨: {list(latest_doc.keys())}")

            print()
            print("ğŸ“‹ æ•°æ®è´¨é‡æ£€æŸ¥:")

            # æ£€æŸ¥ç©ºæ ‡é¢˜
            empty_title = adaptive_collection.count_documents(
                {"title": {"$in": ["", None]}}
            )
            print(f"   ğŸ“ ç©ºæ ‡é¢˜è®°å½•: {empty_title}")

            # æ£€æŸ¥ç©ºå†…å®¹
            empty_content = adaptive_collection.count_documents(
                {"content": {"$in": ["", None]}}
            )
            print(f"   ğŸ“„ ç©ºå†…å®¹è®°å½•: {empty_content}")

            # æ£€æŸ¥é‡å¤URL
            pipeline = [
                {"$group": {"_id": "$url", "count": {"$sum": 1}}},
                {"$match": {"count": {"$gt": 1}}},
                {"$count": "duplicates"},
            ]
            duplicates = list(adaptive_collection.aggregate(pipeline))
            duplicate_count = duplicates[0]["duplicates"] if duplicates else 0
            print(f"   ğŸ”„ é‡å¤URLæ•°é‡: {duplicate_count}")

            # æ£€æŸ¥æœ€è¿‘çš„è®°å½•
            recent_count = adaptive_collection.count_documents(
                {
                    "crawl_timestamp": {
                        "$gte": datetime.now().replace(
                            hour=0, minute=0, second=0, microsecond=0
                        )
                    }
                }
            )
            print(f"   ğŸ“… ä»Šæ—¥æ–°å¢è®°å½•: {recent_count}")

        else:
            print("âŒ adaptive_dataé›†åˆä¸å­˜åœ¨!")

        print()
        print("ğŸ”§ æ•°æ®ç®¡é“çŠ¶æ€: âœ… æ­£å¸¸è¿è¡Œ")

        client.close()

    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")


if __name__ == "__main__":
    check_data_pipeline()
