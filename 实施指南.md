# ä¼ä¸šçº§åˆ†å¸ƒå¼çˆ¬è™«ç³»ç»Ÿå®æ–½æŒ‡å—

## ğŸ“‹ å®æ–½æ¦‚è§ˆ

æœ¬æŒ‡å—åŸºäºã€Šç³»ç»Ÿè®¾è®¡.mdã€‹å’Œã€Šå¼€å‘è®¡åˆ’.mdã€‹ï¼Œæä¾›è¯¦ç»†çš„å®æ–½æ­¥éª¤ã€æœ€ä½³å®è·µå’Œå…³é”®å†³ç­–ç‚¹ï¼Œç¡®ä¿é¡¹ç›®é¡ºåˆ©äº¤ä»˜ã€‚

## ğŸ¯ å®æ–½ç›®æ ‡

- **ä¸»è¦ç›®æ ‡**: æ„å»ºç¨³å®šã€é«˜æ•ˆçš„åˆ†å¸ƒå¼ç–«æƒ…ä¿¡æ¯çˆ¬è™«ç³»ç»Ÿ
- **æŠ€æœ¯ç›®æ ‡**: å®ç°æ—¥å¤„ç†100ä¸‡+é¡µé¢ï¼Œ99.9%å¯ç”¨æ€§
- **ä¸šåŠ¡ç›®æ ‡**: è¦†ç›–å…¨å›½ä¸»è¦ç–«æƒ…ä¿¡æ¯æºï¼Œæ•°æ®å®æ—¶æ€§<30åˆ†é’Ÿ

## ğŸ“… å®æ–½æ—¶é—´çº¿

| é˜¶æ®µ | å‘¨æœŸ | å…³é”®äº¤ä»˜ç‰© | æˆåŠŸæ ‡å‡† |
|------|------|------------|----------|
| ç¬¬ä¸€é˜¶æ®µ | 1-6å‘¨ | åŸºç¡€æ¡†æ¶ | MVPå¯è¿è¡Œ |
| ç¬¬äºŒé˜¶æ®µ | 7-10å‘¨ | åçˆ¬æœºåˆ¶ | Alphaç‰ˆæœ¬ |
| ç¬¬ä¸‰é˜¶æ®µ | 11-13å‘¨ | æ•°æ®å¤„ç† | æ•°æ®è´¨é‡è¾¾æ ‡ |
| ç¬¬å››é˜¶æ®µ | 14-16å‘¨ | ä»»åŠ¡è°ƒåº¦ | åˆ†å¸ƒå¼è¿è¡Œ |
| ç¬¬äº”é˜¶æ®µ | 17-20å‘¨ | ç›‘æ§è¿ç»´ | Betaç‰ˆæœ¬ |
| ç¬¬å…­é˜¶æ®µ | 21-24å‘¨ | éƒ¨ç½²æµ‹è¯• | ç”Ÿäº§å°±ç»ª |

## ğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ¡†æ¶æ­å»º (1-6å‘¨)

### 1.1 ç¯å¢ƒå‡†å¤‡ (ç¬¬1å‘¨)

#### å¼€å‘ç¯å¢ƒæ ‡å‡†åŒ–
```bash
# Pythonç¯å¢ƒé…ç½®
python -m venv crawler_env
source crawler_env/bin/activate  # Linux/Mac
# crawler_env\Scripts\activate  # Windows

# ä¾èµ–ç®¡ç†
pip install pipenv
pipenv install scrapy scrapy-redis selenium celery
```

#### ä»£ç è´¨é‡å·¥å…·é…ç½®
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 22.3.0
    hooks:
      - id: black
  - repo: https://github.com/pycqa/flake8
    rev: 4.0.1
    hooks:
      - id: flake8
```

#### Gitå·¥ä½œæµè§„èŒƒ
- **åˆ†æ”¯ç­–ç•¥**: GitFlow (main/develop/feature/hotfix)
- **æäº¤è§„èŒƒ**: Conventional Commits
- **ä»£ç å®¡æŸ¥**: å¿…é¡»é€šè¿‡2äººå®¡æŸ¥æ‰èƒ½åˆå¹¶

### 1.2 é¡¹ç›®ç»“æ„åˆå§‹åŒ– (ç¬¬1å‘¨)

#### æ ¸å¿ƒç›®å½•ç»“æ„
```
crawler_system/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ sites/              # ç½‘ç«™é…ç½®
â”‚   â”‚   â”œâ”€â”€ government.yaml # æ”¿åºœç½‘ç«™é…ç½®
â”‚   â”‚   â”œâ”€â”€ cdc.yaml       # ç–¾æ§ä¸­å¿ƒé…ç½®
â”‚   â”‚   â””â”€â”€ international.yaml
â”‚   â”œâ”€â”€ proxy.yaml         # ä»£ç†é…ç½®
â”‚   â”œâ”€â”€ database.yaml      # æ•°æ®åº“é…ç½®
â”‚   â””â”€â”€ settings.py        # å…¨å±€è®¾ç½®
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ spiders/           # çˆ¬è™«å®ç°
â”‚   â”‚   â”œâ”€â”€ government/    # æ”¿åºœç½‘ç«™çˆ¬è™«
â”‚   â”‚   â”œâ”€â”€ cdc/          # ç–¾æ§ä¸­å¿ƒçˆ¬è™«
â”‚   â”‚   â””â”€â”€ base.py       # åŸºç¡€çˆ¬è™«ç±»
â”‚   â”œâ”€â”€ middlewares/       # ä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ proxy.py      # ä»£ç†ä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ useragent.py  # UAä¸­é—´ä»¶
â”‚   â”‚   â””â”€â”€ retry.py      # é‡è¯•ä¸­é—´ä»¶
â”‚   â”œâ”€â”€ pipelines/         # æ•°æ®ç®¡é“
â”‚   â”‚   â”œâ”€â”€ validation.py # æ•°æ®éªŒè¯
â”‚   â”‚   â”œâ”€â”€ cleaning.py   # æ•°æ®æ¸…æ´—
â”‚   â”‚   â””â”€â”€ storage.py    # æ•°æ®å­˜å‚¨
â”‚   â””â”€â”€ items.py          # æ•°æ®æ¨¡å‹
â”œâ”€â”€ scheduler/             # ä»»åŠ¡è°ƒåº¦
â”‚   â”œâ”€â”€ task_manager.py   # ä»»åŠ¡ç®¡ç†å™¨
â”‚   â”œâ”€â”€ priority.py       # ä¼˜å…ˆçº§ç®¡ç†
â”‚   â””â”€â”€ load_balancer.py  # è´Ÿè½½å‡è¡¡
â”œâ”€â”€ proxy_pool/            # ä»£ç†æ± 
â”‚   â”œâ”€â”€ provider.py       # ä»£ç†æä¾›å•†
â”‚   â”œâ”€â”€ validator.py      # ä»£ç†éªŒè¯
â”‚   â””â”€â”€ manager.py        # ä»£ç†ç®¡ç†
â”œâ”€â”€ anti_crawl/            # åçˆ¬æ¨¡å—
â”‚   â”œâ”€â”€ captcha.py        # éªŒè¯ç å¤„ç†
â”‚   â”œâ”€â”€ frequency.py      # é¢‘ç‡æ§åˆ¶
â”‚   â””â”€â”€ browser.py        # æµè§ˆå™¨ç®¡ç†
â”œâ”€â”€ data_processing/       # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ extractor.py      # æ•°æ®æå–
â”‚   â”œâ”€â”€ cleaner.py        # æ•°æ®æ¸…æ´—
â”‚   â””â”€â”€ validator.py      # æ•°æ®éªŒè¯
â”œâ”€â”€ monitoring/            # ç›‘æ§æ¨¡å—
â”‚   â”œâ”€â”€ metrics.py        # æŒ‡æ ‡æ”¶é›†
â”‚   â”œâ”€â”€ alerts.py         # å‘Šè­¦å¤„ç†
â”‚   â””â”€â”€ dashboard.py      # ä»ªè¡¨æ¿
â”œâ”€â”€ deployment/            # éƒ¨ç½²é…ç½®
â”‚   â”œâ”€â”€ docker/           # Dockeré…ç½®
â”‚   â”œâ”€â”€ k8s/             # Kubernetesé…ç½®
â”‚   â””â”€â”€ scripts/         # éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ tests/                # æµ‹è¯•ä»£ç 
â”‚   â”œâ”€â”€ unit/            # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ integration/     # é›†æˆæµ‹è¯•
â”‚   â””â”€â”€ e2e/            # ç«¯åˆ°ç«¯æµ‹è¯•
â””â”€â”€ docs/                # æ–‡æ¡£
    â”œâ”€â”€ api/             # APIæ–‡æ¡£
    â”œâ”€â”€ deployment/      # éƒ¨ç½²æ–‡æ¡£
    â””â”€â”€ user_guide/      # ç”¨æˆ·æŒ‡å—
```

### 1.3 Scrapy-Redisé›†æˆ (ç¬¬2å‘¨)

#### æ ¸å¿ƒé…ç½®
```python
# settings.py
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 300,
}

# Redisé…ç½®
REDIS_URL = 'redis://localhost:6379/0'
REDIS_PARAMS = {
    'socket_connect_timeout': 30,
    'socket_timeout': 30,
    'retry_on_timeout': True,
    'health_check_interval': 30,
}
```

#### åˆ†å¸ƒå¼é˜Ÿåˆ—è®¾è®¡
```python
# scheduler/task_manager.py
class TaskManager:
    def __init__(self):
        self.redis_client = redis.Redis.from_url(settings.REDIS_URL)
        self.task_queue = 'crawler:tasks'
        self.result_queue = 'crawler:results'

    def add_task(self, spider_name, url, priority=0):
        task = {
            'spider': spider_name,
            'url': url,
            'priority': priority,
            'timestamp': time.time()
        }
        self.redis_client.zadd(self.task_queue, {json.dumps(task): priority})
```

### 1.4 åŸºç¡€ä»£ç†æ± å®ç° (ç¬¬3å‘¨)

#### ä»£ç†ç®¡ç†å™¨
```python
# proxy_pool/manager.py
class ProxyManager:
    def __init__(self):
        self.redis_client = redis.Redis.from_url(settings.REDIS_URL)
        self.proxy_key = 'proxy:pool'
        self.failed_key = 'proxy:failed'

    def get_proxy(self):
        """è·å–å¯ç”¨ä»£ç†"""
        proxy = self.redis_client.spop(self.proxy_key)
        if proxy:
            return json.loads(proxy)
        return None

    def validate_proxy(self, proxy):
        """éªŒè¯ä»£ç†å¯ç”¨æ€§"""
        try:
            response = requests.get(
                'http://httpbin.org/ip',
                proxies={'http': proxy, 'https': proxy},
                timeout=10
            )
            return response.status_code == 200
        except:
            return False
```

### 1.5 æ•°æ®æ¨¡å‹è®¾è®¡ (ç¬¬4å‘¨)

#### Scrapy Itemså®šä¹‰
```python
# crawler/items.py
import scrapy
from itemloaders.processors import TakeFirst, MapCompose

class EpidemicDataItem(scrapy.Item):
    # åŸºç¡€ä¿¡æ¯
    source_url = scrapy.Field()
    source_name = scrapy.Field()
    crawl_time = scrapy.Field()

    # ç–«æƒ…æ•°æ®
    region = scrapy.Field()
    confirmed_cases = scrapy.Field(
        input_processor=MapCompose(str.strip, int),
        output_processor=TakeFirst()
    )
    death_cases = scrapy.Field(
        input_processor=MapCompose(str.strip, int),
        output_processor=TakeFirst()
    )
    recovered_cases = scrapy.Field(
        input_processor=MapCompose(str.strip, int),
        output_processor=TakeFirst()
    )

    # æ—¶é—´ä¿¡æ¯
    report_date = scrapy.Field()
    update_time = scrapy.Field()

    # æ•°æ®è´¨é‡
    data_quality_score = scrapy.Field()
    validation_status = scrapy.Field()
```

#### æ•°æ®åº“è¡¨ç»“æ„
```sql
-- PostgreSQLè¡¨ç»“æ„
CREATE TABLE epidemic_data (
    id SERIAL PRIMARY KEY,
    source_url VARCHAR(500) NOT NULL,
    source_name VARCHAR(100) NOT NULL,
    crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    region VARCHAR(100) NOT NULL,
    confirmed_cases INTEGER DEFAULT 0,
    death_cases INTEGER DEFAULT 0,
    recovered_cases INTEGER DEFAULT 0,

    report_date DATE NOT NULL,
    update_time TIMESTAMP,

    data_quality_score DECIMAL(3,2) DEFAULT 0.00,
    validation_status VARCHAR(20) DEFAULT 'pending',

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ç´¢å¼•ä¼˜åŒ–
CREATE INDEX idx_epidemic_region_date ON epidemic_data(region, report_date);
CREATE INDEX idx_epidemic_source ON epidemic_data(source_name);
CREATE INDEX idx_epidemic_crawl_time ON epidemic_data(crawl_time);
```

### 1.6 åŸºç¡€çˆ¬è™«å¼€å‘ (ç¬¬5å‘¨)

#### åŸºç¡€çˆ¬è™«ç±»
```python
# crawler/spiders/base.py
import scrapy
from scrapy_redis.spiders import RedisSpider

class BaseEpidemicSpider(RedisSpider):
    """åŸºç¡€ç–«æƒ…çˆ¬è™«ç±»"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.site_config = self.load_site_config()

    def load_site_config(self):
        """åŠ è½½ç½‘ç«™é…ç½®"""
        config_path = f"config/sites/{self.name}.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def parse(self, response):
        """è§£æå“åº”"""
        # æ•°æ®æå–é€»è¾‘
        for item in self.extract_data(response):
            yield item

        # ç¿»é¡µå¤„ç†
        next_page = self.get_next_page(response)
        if next_page:
            yield response.follow(next_page, self.parse)

    def extract_data(self, response):
        """æå–æ•°æ® - å­ç±»å®ç°"""
        raise NotImplementedError

    def get_next_page(self, response):
        """è·å–ä¸‹ä¸€é¡µ - å­ç±»å®ç°"""
        return None
```

### 1.7 å­˜å‚¨ç³»ç»Ÿæ­å»º (ç¬¬6å‘¨)

#### Docker Composeé…ç½®
```yaml
# deployment/docker/docker-compose.yml
version: '3.8'
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  mongodb:
    image: mongo:5
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password

  postgresql:
    image: postgres:14
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: crawler_db
      POSTGRES_USER: crawler_user
      POSTGRES_PASSWORD: crawler_pass

volumes:
  redis_data:
  mongo_data:
  postgres_data:
```

## âœ… ç¬¬ä¸€é˜¶æ®µéªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] åˆ†å¸ƒå¼çˆ¬è™«æ¡†æ¶å¯æ­£å¸¸è¿è¡Œ
- [ ] ä»£ç†æ± åŸºç¡€åŠŸèƒ½æ­£å¸¸
- [ ] æ•°æ®å¯æ­£ç¡®å­˜å‚¨åˆ°æ•°æ®åº“
- [ ] åŸºç¡€çˆ¬è™«å¯æŠ“å–ç›®æ ‡ç½‘ç«™

### æ€§èƒ½éªŒæ”¶
- [ ] å•æœºå¹¶å‘å¤„ç†èƒ½åŠ› â‰¥ 100 requests/min
- [ ] ä»£ç†æ± å“åº”æ—¶é—´ < 100ms
- [ ] æ•°æ®å­˜å‚¨å»¶è¿Ÿ < 500ms

### è´¨é‡éªŒæ”¶
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥ 70%
- [ ] ä»£ç è´¨é‡æ£€æŸ¥é€šè¿‡
- [ ] æ–‡æ¡£å®Œæ•´æ€§ â‰¥ 80%

## ğŸ“Š å…³é”®æŒ‡æ ‡ç›‘æ§

### ä¸šåŠ¡æŒ‡æ ‡
- çˆ¬å–æˆåŠŸç‡ â‰¥ 95%
- æ•°æ®è´¨é‡åˆ†æ•° â‰¥ 0.8
- æ•°æ®æ›´æ–°åŠæ—¶æ€§ < 30åˆ†é’Ÿ

### æŠ€æœ¯æŒ‡æ ‡
- ç³»ç»Ÿå¯ç”¨æ€§ â‰¥ 99.9%
- å¹³å‡å“åº”æ—¶é—´ < 2ç§’
- é”™è¯¯ç‡ < 1%

## ğŸ”„ æŒç»­æ”¹è¿›

### æ¯å‘¨å›é¡¾
- æŠ€æœ¯å€ºåŠ¡è¯„ä¼°
- æ€§èƒ½ç“¶é¢ˆåˆ†æ
- ä»£ç è´¨é‡æ£€æŸ¥

### æœˆåº¦è¯„ä¼°
- æ¶æ„ä¼˜åŒ–å»ºè®®
- æŠ€æœ¯æ ˆå‡çº§è®¡åˆ’
- å›¢é˜ŸæŠ€èƒ½æå‡

## ğŸ›¡ï¸ ç¬¬äºŒé˜¶æ®µï¼šåçˆ¬æœºåˆ¶åº”å¯¹ (7-10å‘¨)

### 2.1 Selenium Gridé›†æˆ (ç¬¬7å‘¨)

#### Selenium Hubé…ç½®
```yaml
# deployment/selenium/docker-compose.yml
version: '3.8'
services:
  selenium-hub:
    image: selenium/hub:4.0.0
    ports:
      - "4444:4444"
    environment:
      GRID_MAX_SESSION: 16
      GRID_BROWSER_TIMEOUT: 300
      GRID_TIMEOUT: 300

  firefox-node:
    image: selenium/node-firefox:4.0.0
    scale: 3
    depends_on:
      - selenium-hub
    environment:
      HUB_HOST: selenium-hub
      NODE_MAX_INSTANCES: 2
      NODE_MAX_SESSION: 2

  chrome-node:
    image: selenium/node-chrome:4.0.0
    scale: 2
    depends_on:
      - selenium-hub
    environment:
      HUB_HOST: selenium-hub
      NODE_MAX_INSTANCES: 2
      NODE_MAX_SESSION: 2
```

#### æµè§ˆå™¨æ± ç®¡ç†
```python
# anti_crawl/browser.py
from selenium import webdriver
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

class BrowserPool:
    def __init__(self, hub_url="http://localhost:4444/wd/hub"):
        self.hub_url = hub_url
        self.active_sessions = {}
        self.max_sessions = 10

    def get_browser(self, browser_type="firefox"):
        """è·å–æµè§ˆå™¨å®ä¾‹"""
        if browser_type == "firefox":
            caps = DesiredCapabilities.FIREFOX.copy()
            caps['marionette'] = True
        else:
            caps = DesiredCapabilities.CHROME.copy()

        # åæ£€æµ‹é…ç½®
        caps['acceptSslCerts'] = True
        caps['acceptInsecureCerts'] = True

        driver = webdriver.Remote(
            command_executor=self.hub_url,
            desired_capabilities=caps
        )

        # åæ£€æµ‹è„šæœ¬
        driver.execute_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
        """)

        return driver

    def release_browser(self, driver):
        """é‡Šæ”¾æµè§ˆå™¨å®ä¾‹"""
        try:
            driver.quit()
        except:
            pass
```

### 2.2 é«˜çº§ä»£ç†ç­–ç•¥ (ç¬¬8å‘¨)

#### ä»£ç†è´¨é‡è¯„åˆ†ç³»ç»Ÿ
```python
# proxy_pool/validator.py
import time
import requests
from dataclasses import dataclass

@dataclass
class ProxyMetrics:
    success_rate: float = 0.0
    avg_response_time: float = 0.0
    last_success_time: float = 0.0
    total_requests: int = 0
    failed_requests: int = 0
    geographic_location: str = ""
    anonymity_level: str = ""  # transparent, anonymous, elite

class ProxyValidator:
    def __init__(self):
        self.test_urls = [
            'http://httpbin.org/ip',
            'http://httpbin.org/headers',
            'https://www.google.com'
        ]

    def validate_proxy(self, proxy_url):
        """å…¨é¢éªŒè¯ä»£ç†"""
        metrics = ProxyMetrics()

        for test_url in self.test_urls:
            start_time = time.time()
            try:
                response = requests.get(
                    test_url,
                    proxies={'http': proxy_url, 'https': proxy_url},
                    timeout=10,
                    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                )

                if response.status_code == 200:
                    metrics.total_requests += 1
                    metrics.avg_response_time += time.time() - start_time
                    metrics.last_success_time = time.time()

                    # æ£€æµ‹åŒ¿åæ€§
                    if 'httpbin.org/headers' in test_url:
                        headers = response.json().get('headers', {})
                        if 'X-Forwarded-For' not in headers:
                            metrics.anonymity_level = 'elite'
                        elif proxy_url.split('://')[1].split(':')[0] not in headers.get('X-Forwarded-For', ''):
                            metrics.anonymity_level = 'anonymous'
                        else:
                            metrics.anonymity_level = 'transparent'
                else:
                    metrics.failed_requests += 1

            except Exception as e:
                metrics.failed_requests += 1

        # è®¡ç®—æˆåŠŸç‡
        total = metrics.total_requests + metrics.failed_requests
        if total > 0:
            metrics.success_rate = metrics.total_requests / total
            metrics.avg_response_time = metrics.avg_response_time / metrics.total_requests if metrics.total_requests > 0 else float('inf')

        return metrics

    def calculate_score(self, metrics):
        """è®¡ç®—ä»£ç†è´¨é‡åˆ†æ•° (0-100)"""
        score = 0

        # æˆåŠŸç‡æƒé‡ 40%
        score += metrics.success_rate * 40

        # å“åº”æ—¶é—´æƒé‡ 30% (è¶Šå¿«è¶Šå¥½)
        if metrics.avg_response_time < 1:
            score += 30
        elif metrics.avg_response_time < 3:
            score += 20
        elif metrics.avg_response_time < 5:
            score += 10

        # åŒ¿åæ€§æƒé‡ 20%
        if metrics.anonymity_level == 'elite':
            score += 20
        elif metrics.anonymity_level == 'anonymous':
            score += 15
        elif metrics.anonymity_level == 'transparent':
            score += 5

        # ç¨³å®šæ€§æƒé‡ 10%
        if time.time() - metrics.last_success_time < 3600:  # 1å°æ—¶å†…æœ‰æˆåŠŸ
            score += 10

        return min(100, max(0, score))
```

### 2.3 æ™ºèƒ½User-Agentç®¡ç† (ç¬¬8å‘¨)

#### UAç®¡ç†å™¨
```python
# anti_crawl/useragent.py
import random
import json
from pathlib import Path

class UserAgentManager:
    def __init__(self):
        self.ua_database = self.load_ua_database()
        self.browser_stats = {
            'chrome': 0.65,
            'firefox': 0.20,
            'safari': 0.10,
            'edge': 0.05
        }

    def load_ua_database(self):
        """åŠ è½½UAæ•°æ®åº“"""
        ua_file = Path(__file__).parent / 'data' / 'user_agents.json'
        with open(ua_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def get_random_ua(self, browser=None, os=None):
        """è·å–éšæœºUA"""
        if not browser:
            browser = self.select_browser_by_stats()

        candidates = [
            ua for ua in self.ua_database
            if (not browser or ua['browser'].lower() == browser.lower()) and
               (not os or ua['os'].lower() == os.lower())
        ]

        if candidates:
            return random.choice(candidates)['user_agent']

        # fallback
        return "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

    def select_browser_by_stats(self):
        """æ ¹æ®ç»Ÿè®¡æ•°æ®é€‰æ‹©æµè§ˆå™¨"""
        rand = random.random()
        cumulative = 0

        for browser, probability in self.browser_stats.items():
            cumulative += probability
            if rand <= cumulative:
                return browser

        return 'chrome'

    def generate_headers(self, ua=None):
        """ç”Ÿæˆå®Œæ•´è¯·æ±‚å¤´"""
        if not ua:
            ua = self.get_random_ua()

        headers = {
            'User-Agent': ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

        # æ ¹æ®æµè§ˆå™¨ç±»å‹è°ƒæ•´å¤´éƒ¨
        if 'Chrome' in ua:
            headers['sec-ch-ua'] = '"Google Chrome";v="91", "Chromium";v="91", ";Not A Brand";v="99"'
            headers['sec-ch-ua-mobile'] = '?0'

        return headers
```

### 2.4 éªŒè¯ç å¤„ç†ç³»ç»Ÿ (ç¬¬9å‘¨)

#### éªŒè¯ç è¯†åˆ«æœåŠ¡
```python
# anti_crawl/captcha.py
import base64
import requests
from PIL import Image
import pytesseract
from io import BytesIO

class CaptchaHandler:
    def __init__(self):
        self.ocr_services = {
            'tesseract': self.solve_with_tesseract,
            'api_service': self.solve_with_api,
            'manual': self.solve_manually
        }
        self.success_rates = {
            'tesseract': 0.3,
            'api_service': 0.8,
            'manual': 0.95
        }

    def solve_captcha(self, image_data, captcha_type='text'):
        """è§£å†³éªŒè¯ç """
        # é¢„å¤„ç†å›¾åƒ
        processed_image = self.preprocess_image(image_data)

        # æŒ‰æˆåŠŸç‡æ’åºå°è¯•ä¸åŒæ–¹æ³•
        for service in sorted(self.ocr_services.keys(),
                            key=lambda x: self.success_rates[x],
                            reverse=True):
            try:
                result = self.ocr_services[service](processed_image, captcha_type)
                if result and self.validate_result(result, captcha_type):
                    return result
            except Exception as e:
                self.logger.warning(f"éªŒè¯ç è¯†åˆ«å¤±è´¥ {service}: {e}")
                continue

        return None

    def preprocess_image(self, image_data):
        """å›¾åƒé¢„å¤„ç†"""
        if isinstance(image_data, str):
            # base64è§£ç 
            image_data = base64.b64decode(image_data)

        image = Image.open(BytesIO(image_data))

        # è½¬æ¢ä¸ºç°åº¦
        image = image.convert('L')

        # äºŒå€¼åŒ–
        threshold = 128
        image = image.point(lambda x: 0 if x < threshold else 255, '1')

        # å»å™ª
        image = image.filter(ImageFilter.MedianFilter())

        return image

    def solve_with_tesseract(self, image, captcha_type):
        """ä½¿ç”¨Tesseract OCR"""
        config = '--oem 3 --psm 8 -c tessedit_char_whitelist=0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'

        if captcha_type == 'number':
            config = '--oem 3 --psm 8 -c tessedit_char_whitelist=0123456789'

        text = pytesseract.image_to_string(image, config=config)
        return text.strip()

    def solve_with_api(self, image, captcha_type):
        """ä½¿ç”¨ç¬¬ä¸‰æ–¹APIæœåŠ¡"""
        # è¿™é‡Œå¯ä»¥é›†æˆæ‰“ç å¹³å°API
        # å¦‚ï¼šè¶…çº§é¹°ã€è‹¥å¿«æ‰“ç ç­‰
        api_url = "http://api.captcha-service.com/solve"

        # å°†å›¾åƒè½¬æ¢ä¸ºbase64
        buffer = BytesIO()
        image.save(buffer, format='PNG')
        image_b64 = base64.b64encode(buffer.getvalue()).decode()

        response = requests.post(api_url, json={
            'image': image_b64,
            'type': captcha_type
        })

        if response.status_code == 200:
            return response.json().get('result')

        return None

    def solve_manually(self, image, captcha_type):
        """äººå·¥éªŒè¯ç å¤„ç†"""
        # ä¿å­˜å›¾åƒåˆ°ä¸´æ—¶æ–‡ä»¶
        temp_path = f"/tmp/captcha_{int(time.time())}.png"
        image.save(temp_path)

        # å‘é€åˆ°äººå·¥å¤„ç†é˜Ÿåˆ—
        task_id = self.submit_manual_task(temp_path, captcha_type)

        # ç­‰å¾…äººå·¥å¤„ç†ç»“æœ
        return self.wait_for_manual_result(task_id, timeout=300)

    def validate_result(self, result, captcha_type):
        """éªŒè¯è¯†åˆ«ç»“æœ"""
        if not result:
            return False

        if captcha_type == 'number':
            return result.isdigit() and len(result) >= 4
        elif captcha_type == 'text':
            return len(result) >= 4 and result.isalnum()

        return True
```

### 2.5 æ™ºèƒ½é¢‘ç‡æ§åˆ¶ (ç¬¬10å‘¨)

#### è‡ªé€‚åº”å»¶è¿Ÿæ§åˆ¶
```python
# anti_crawl/frequency.py
import time
import random
from collections import defaultdict, deque
from threading import Lock

class AdaptiveRateController:
    def __init__(self):
        self.site_stats = defaultdict(lambda: {
            'success_count': 0,
            'error_count': 0,
            'last_request_time': 0,
            'current_delay': 1.0,
            'min_delay': 0.5,
            'max_delay': 30.0,
            'recent_responses': deque(maxlen=100)
        })
        self.lock = Lock()

    def get_delay(self, domain):
        """è·å–å½“å‰åŸŸåçš„å»¶è¿Ÿæ—¶é—´"""
        with self.lock:
            stats = self.site_stats[domain]

            # åŸºäºæœ€è¿‘å“åº”è°ƒæ•´å»¶è¿Ÿ
            if len(stats['recent_responses']) >= 10:
                recent_errors = sum(1 for r in stats['recent_responses'] if r['status'] != 'success')
                error_rate = recent_errors / len(stats['recent_responses'])

                if error_rate > 0.3:  # é”™è¯¯ç‡è¿‡é«˜ï¼Œå¢åŠ å»¶è¿Ÿ
                    stats['current_delay'] = min(stats['current_delay'] * 1.5, stats['max_delay'])
                elif error_rate < 0.1:  # é”™è¯¯ç‡ä½ï¼Œå‡å°‘å»¶è¿Ÿ
                    stats['current_delay'] = max(stats['current_delay'] * 0.8, stats['min_delay'])

            # æ·»åŠ éšæœºæ€§é¿å…æ£€æµ‹
            base_delay = stats['current_delay']
            random_factor = random.uniform(0.5, 1.5)

            return base_delay * random_factor

    def record_response(self, domain, status, response_time=None):
        """è®°å½•å“åº”ç»“æœ"""
        with self.lock:
            stats = self.site_stats[domain]

            response_record = {
                'status': status,
                'timestamp': time.time(),
                'response_time': response_time
            }

            stats['recent_responses'].append(response_record)

            if status == 'success':
                stats['success_count'] += 1
            else:
                stats['error_count'] += 1

            stats['last_request_time'] = time.time()

    def should_pause(self, domain):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æš‚åœçˆ¬å–"""
        stats = self.site_stats[domain]

        # æ£€æŸ¥æœ€è¿‘é”™è¯¯ç‡
        if len(stats['recent_responses']) >= 20:
            recent_errors = sum(1 for r in stats['recent_responses'][-20:] if r['status'] != 'success')
            if recent_errors >= 15:  # æœ€è¿‘20æ¬¡è¯·æ±‚ä¸­æœ‰15æ¬¡å¤±è´¥
                return True

        return False

    def get_pause_duration(self, domain):
        """è·å–æš‚åœæ—¶é•¿"""
        stats = self.site_stats[domain]
        error_rate = stats['error_count'] / max(stats['success_count'] + stats['error_count'], 1)

        if error_rate > 0.8:
            return 3600  # 1å°æ—¶
        elif error_rate > 0.5:
            return 1800  # 30åˆ†é’Ÿ
        else:
            return 600   # 10åˆ†é’Ÿ
```

## âœ… ç¬¬äºŒé˜¶æ®µéªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] Selenium Gridé›†ç¾¤æ­£å¸¸è¿è¡Œ
- [ ] ä»£ç†è´¨é‡è¯„åˆ†ç³»ç»Ÿå·¥ä½œæ­£å¸¸
- [ ] éªŒè¯ç è¯†åˆ«æˆåŠŸç‡ â‰¥ 70%
- [ ] æ™ºèƒ½é¢‘ç‡æ§åˆ¶æœ‰æ•ˆé™ä½å°ç¦ç‡

### æ€§èƒ½éªŒæ”¶
- [ ] æµè§ˆå™¨æ± å“åº”æ—¶é—´ < 5ç§’
- [ ] ä»£ç†åˆ‡æ¢å»¶è¿Ÿ < 200ms
- [ ] éªŒè¯ç è¯†åˆ«æ—¶é—´ < 30ç§’

### è´¨é‡éªŒæ”¶
- [ ] åçˆ¬æˆåŠŸç‡ â‰¥ 85%
- [ ] ç³»ç»Ÿç¨³å®šæ€§ â‰¥ 99%
- [ ] é”™è¯¯æ¢å¤æ—¶é—´ < 5åˆ†é’Ÿ

## ğŸ“Š ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®å¤„ç†ä¸è´¨é‡ä¿è¯ (11-13å‘¨)

### 3.1 æ•°æ®æå–å¼•æ“ (ç¬¬11å‘¨)

#### é…ç½®åŒ–æå–è§„åˆ™
```python
# data_processing/extractor.py
import yaml
import re
from lxml import etree
from bs4 import BeautifulSoup

class ConfigurableExtractor:
    def __init__(self, config_path):
        self.config = self.load_config(config_path)
        self.extractors = {
            'xpath': self.extract_by_xpath,
            'css': self.extract_by_css,
            'regex': self.extract_by_regex,
            'json': self.extract_by_json
        }

    def load_config(self, config_path):
        """åŠ è½½æå–é…ç½®"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def extract_data(self, response, site_name):
        """æ ¹æ®é…ç½®æå–æ•°æ®"""
        site_config = self.config.get(site_name, {})
        extracted_data = {}

        for field_name, field_config in site_config.get('fields', {}).items():
            try:
                value = self.extract_field(response, field_config)
                extracted_data[field_name] = self.clean_value(value, field_config)
            except Exception as e:
                self.logger.error(f"æå–å­—æ®µ {field_name} å¤±è´¥: {e}")
                extracted_data[field_name] = None

        return extracted_data

    def extract_field(self, response, field_config):
        """æå–å•ä¸ªå­—æ®µ"""
        method = field_config.get('method', 'xpath')
        selector = field_config.get('selector')

        if method in self.extractors:
            return self.extractors[method](response, selector, field_config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æå–æ–¹æ³•: {method}")

    def extract_by_xpath(self, response, selector, config):
        """XPathæå–"""
        tree = etree.HTML(response.text)
        elements = tree.xpath(selector)

        if config.get('multiple', False):
            return [elem.text.strip() if hasattr(elem, 'text') else str(elem).strip()
                   for elem in elements]
        else:
            return elements[0].text.strip() if elements and hasattr(elements[0], 'text') else str(elements[0]).strip() if elements else None

    def extract_by_css(self, response, selector, config):
        """CSSé€‰æ‹©å™¨æå–"""
        soup = BeautifulSoup(response.text, 'html.parser')
        elements = soup.select(selector)

        if config.get('multiple', False):
            return [elem.get_text().strip() for elem in elements]
        else:
            return elements[0].get_text().strip() if elements else None

    def extract_by_regex(self, response, pattern, config):
        """æ­£åˆ™è¡¨è¾¾å¼æå–"""
        matches = re.findall(pattern, response.text, re.DOTALL)

        if config.get('multiple', False):
            return matches
        else:
            return matches[0] if matches else None

    def clean_value(self, value, config):
        """æ¸…æ´—æ•°æ®å€¼"""
        if value is None:
            return None

        # æ•°æ®ç±»å‹è½¬æ¢
        data_type = config.get('type', 'string')

        if data_type == 'integer':
            # æå–æ•°å­—
            numbers = re.findall(r'\d+', str(value))
            return int(''.join(numbers)) if numbers else 0
        elif data_type == 'float':
            numbers = re.findall(r'\d+\.?\d*', str(value))
            return float(''.join(numbers)) if numbers else 0.0
        elif data_type == 'date':
            # æ—¥æœŸè§£æ
            return self.parse_date(value)
        else:
            # å­—ç¬¦ä¸²æ¸…ç†
            return re.sub(r'\s+', ' ', str(value)).strip()

    def parse_date(self, date_str):
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        import dateutil.parser
        try:
            return dateutil.parser.parse(date_str).strftime('%Y-%m-%d')
        except:
            return None
```

#### ç½‘ç«™é…ç½®ç¤ºä¾‹
```yaml
# config/sites/nhc.gov.cn.yaml
site_name: "å›½å®¶å«å¥å§”"
base_url: "http://www.nhc.gov.cn"
encoding: "utf-8"

fields:
  title:
    method: xpath
    selector: "//h1[@class='article-title']/text()"
    type: string

  content:
    method: xpath
    selector: "//div[@class='article-content']//text()"
    type: string
    multiple: true

  publish_date:
    method: regex
    selector: "å‘å¸ƒæ—¶é—´ï¼š(\d{4}-\d{2}-\d{2})"
    type: date

  confirmed_cases:
    method: regex
    selector: "ç¡®è¯Šç—…ä¾‹(\d+)ä¾‹"
    type: integer

  death_cases:
    method: regex
    selector: "æ­»äº¡ç—…ä¾‹(\d+)ä¾‹"
    type: integer

pagination:
  next_page_xpath: "//a[contains(text(), 'ä¸‹ä¸€é¡µ')]/@href"
  max_pages: 100

rate_limit:
  delay: 2.0
  random_delay: true
```

### 3.2 æ•°æ®æ¸…æ´—æ¨¡å— (ç¬¬12å‘¨)

#### æ•°æ®æ¸…æ´—æµæ°´çº¿
```python
# data_processing/cleaner.py
import re
import jieba
from datetime import datetime
from typing import Dict, Any, List

class DataCleaner:
    def __init__(self):
        self.cleaning_rules = {
            'text': self.clean_text,
            'number': self.clean_number,
            'date': self.clean_date,
            'region': self.clean_region
        }

        # åŠ è½½åœ°åŒºæ˜ å°„è¡¨
        self.region_mapping = self.load_region_mapping()

        # åœç”¨è¯
        self.stopwords = self.load_stopwords()

    def clean_data(self, data: Dict[str, Any], field_types: Dict[str, str]) -> Dict[str, Any]:
        """æ¸…æ´—æ•°æ®"""
        cleaned_data = {}

        for field, value in data.items():
            field_type = field_types.get(field, 'text')

            if field_type in self.cleaning_rules:
                cleaned_data[field] = self.cleaning_rules[field_type](value)
            else:
                cleaned_data[field] = value

        return cleaned_data

    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬æ•°æ®"""
        if not text:
            return ""

        # å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)

        # å»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)

        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()

        # å»é™¤åœç”¨è¯
        words = jieba.cut(text)
        filtered_words = [word for word in words if word not in self.stopwords]

        return ' '.join(filtered_words)

    def clean_number(self, value: Any) -> int:
        """æ¸…æ´—æ•°å­—æ•°æ®"""
        if isinstance(value, (int, float)):
            return int(value)

        if isinstance(value, str):
            # æå–æ•°å­—
            numbers = re.findall(r'\d+', value)
            if numbers:
                return int(''.join(numbers))

        return 0

    def clean_date(self, date_value: Any) -> str:
        """æ¸…æ´—æ—¥æœŸæ•°æ®"""
        if not date_value:
            return None

        # å¸¸è§æ—¥æœŸæ ¼å¼
        date_patterns = [
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{1,2})/(\d{1,2})/(\d{4})',
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})'
        ]

        date_str = str(date_value)

        for pattern in date_patterns:
            match = re.search(pattern, date_str)
            if match:
                try:
                    if len(match.groups()) == 3:
                        year, month, day = match.groups()
                        # å¤„ç†å¹´ä»½åœ¨åé¢çš„æƒ…å†µ
                        if len(year) == 2:
                            year, month, day = day, year, month

                        date_obj = datetime(int(year), int(month), int(day))
                        return date_obj.strftime('%Y-%m-%d')
                except ValueError:
                    continue

        return None

    def clean_region(self, region: str) -> str:
        """æ¸…æ´—åœ°åŒºæ•°æ®"""
        if not region:
            return ""

        # æ ‡å‡†åŒ–åœ°åŒºåç§°
        region = region.strip()

        # ä½¿ç”¨æ˜ å°„è¡¨æ ‡å‡†åŒ–
        for standard_name, aliases in self.region_mapping.items():
            if region in aliases:
                return standard_name

        # å»é™¤å¸¸è§åç¼€
        suffixes = ['çœ', 'å¸‚', 'åŒº', 'å¿', 'è‡ªæ²»åŒº', 'ç‰¹åˆ«è¡Œæ”¿åŒº']
        for suffix in suffixes:
            if region.endswith(suffix):
                base_name = region[:-len(suffix)]
                if base_name:
                    return base_name

        return region

    def load_region_mapping(self) -> Dict[str, List[str]]:
        """åŠ è½½åœ°åŒºæ˜ å°„è¡¨"""
        return {
            'åŒ—äº¬': ['åŒ—äº¬å¸‚', 'åŒ—äº¬', 'Beijing'],
            'ä¸Šæµ·': ['ä¸Šæµ·å¸‚', 'ä¸Šæµ·', 'Shanghai'],
            'å¹¿ä¸œ': ['å¹¿ä¸œçœ', 'å¹¿ä¸œ', 'Guangdong'],
            # ... æ›´å¤šæ˜ å°„
        }

    def load_stopwords(self) -> set:
        """åŠ è½½åœç”¨è¯"""
        return {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
```

### 3.3 æ•°æ®éªŒè¯ç³»ç»Ÿ (ç¬¬12å‘¨)

#### æ•°æ®è´¨é‡è¯„åˆ†
```python
# data_processing/validator.py
from typing import Dict, Any, List, Tuple
import re
from datetime import datetime, timedelta

class DataValidator:
    def __init__(self):
        self.validation_rules = {
            'completeness': self.check_completeness,
            'accuracy': self.check_accuracy,
            'consistency': self.check_consistency,
            'timeliness': self.check_timeliness
        }

        self.field_weights = {
            'confirmed_cases': 0.3,
            'death_cases': 0.2,
            'recovered_cases': 0.2,
            'region': 0.1,
            'report_date': 0.1,
            'source_url': 0.1
        }

    def validate_data(self, data: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """éªŒè¯æ•°æ®å¹¶è¿”å›è´¨é‡åˆ†æ•°"""
        validation_results = {}
        total_score = 0.0

        for rule_name, rule_func in self.validation_rules.items():
            score, details = rule_func(data)
            validation_results[rule_name] = {
                'score': score,
                'details': details
            }
            total_score += score

        # è®¡ç®—åŠ æƒå¹³å‡åˆ†
        quality_score = total_score / len(self.validation_rules)

        return quality_score, validation_results

    def check_completeness(self, data: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        required_fields = ['confirmed_cases', 'region', 'report_date', 'source_url']
        missing_fields = []
        empty_fields = []

        for field in required_fields:
            if field not in data:
                missing_fields.append(field)
            elif data[field] is None or data[field] == '':
                empty_fields.append(field)

        # è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
        total_fields = len(required_fields)
        missing_count = len(missing_fields) + len(empty_fields)
        completeness_score = max(0, (total_fields - missing_count) / total_fields)

        details = {
            'missing_fields': missing_fields,
            'empty_fields': empty_fields,
            'completeness_rate': completeness_score
        }

        return completeness_score, details

    def check_accuracy(self, data: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§"""
        accuracy_issues = []
        accuracy_score = 1.0

        # æ£€æŸ¥æ•°å€¼åˆç†æ€§
        if 'confirmed_cases' in data:
            confirmed = data['confirmed_cases']
            if isinstance(confirmed, int) and confirmed < 0:
                accuracy_issues.append('ç¡®è¯Šç—…ä¾‹æ•°ä¸èƒ½ä¸ºè´Ÿæ•°')
                accuracy_score -= 0.3
            elif isinstance(confirmed, int) and confirmed > 1000000:
                accuracy_issues.append('ç¡®è¯Šç—…ä¾‹æ•°å¼‚å¸¸è¿‡å¤§')
                accuracy_score -= 0.2

        # æ£€æŸ¥æ­»äº¡ç—…ä¾‹ä¸ç¡®è¯Šç—…ä¾‹çš„å…³ç³»
        if 'death_cases' in data and 'confirmed_cases' in data:
            death = data['death_cases']
            confirmed = data['confirmed_cases']
            if isinstance(death, int) and isinstance(confirmed, int):
                if death > confirmed:
                    accuracy_issues.append('æ­»äº¡ç—…ä¾‹æ•°ä¸èƒ½å¤§äºç¡®è¯Šç—…ä¾‹æ•°')
                    accuracy_score -= 0.4

        # æ£€æŸ¥æ—¥æœŸåˆç†æ€§
        if 'report_date' in data:
            report_date = data['report_date']
            if report_date:
                try:
                    date_obj = datetime.strptime(report_date, '%Y-%m-%d')
                    if date_obj > datetime.now():
                        accuracy_issues.append('æŠ¥å‘Šæ—¥æœŸä¸èƒ½æ˜¯æœªæ¥æ—¶é—´')
                        accuracy_score -= 0.2
                    elif date_obj < datetime(2019, 12, 1):
                        accuracy_issues.append('æŠ¥å‘Šæ—¥æœŸè¿‡æ—©ï¼Œå¯èƒ½ä¸å‡†ç¡®')
                        accuracy_score -= 0.1
                except ValueError:
                    accuracy_issues.append('æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®')
                    accuracy_score -= 0.3

        details = {
            'accuracy_issues': accuracy_issues,
            'accuracy_score': max(0, accuracy_score)
        }

        return max(0, accuracy_score), details

    def check_consistency(self, data: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        consistency_issues = []
        consistency_score = 1.0

        # æ£€æŸ¥åœ°åŒºåç§°ä¸€è‡´æ€§
        if 'region' in data:
            region = data['region']
            if region and not self.is_valid_region(region):
                consistency_issues.append(f'åœ°åŒºåç§°å¯èƒ½ä¸æ ‡å‡†: {region}')
                consistency_score -= 0.2

        # æ£€æŸ¥æ•°æ®æºä¸€è‡´æ€§
        if 'source_url' in data:
            source_url = data['source_url']
            if source_url and not self.is_valid_url(source_url):
                consistency_issues.append('æ•°æ®æºURLæ ¼å¼ä¸æ­£ç¡®')
                consistency_score -= 0.1

        details = {
            'consistency_issues': consistency_issues,
            'consistency_score': max(0, consistency_score)
        }

        return max(0, consistency_score), details

    def check_timeliness(self, data: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """æ£€æŸ¥æ•°æ®æ—¶æ•ˆæ€§"""
        timeliness_issues = []
        timeliness_score = 1.0

        if 'report_date' in data and 'crawl_time' in data:
            report_date = data['report_date']
            crawl_time = data['crawl_time']

            if report_date and crawl_time:
                try:
                    report_dt = datetime.strptime(report_date, '%Y-%m-%d')
                    crawl_dt = datetime.fromisoformat(crawl_time.replace('Z', '+00:00'))

                    time_diff = crawl_dt - report_dt

                    if time_diff.days > 7:
                        timeliness_issues.append('æ•°æ®è¿‡äºé™ˆæ—§')
                        timeliness_score -= 0.5
                    elif time_diff.days > 3:
                        timeliness_issues.append('æ•°æ®æ—¶æ•ˆæ€§ä¸€èˆ¬')
                        timeliness_score -= 0.2

                except (ValueError, AttributeError):
                    timeliness_issues.append('æ—¶é—´æ ¼å¼è§£æå¤±è´¥')
                    timeliness_score -= 0.3

        details = {
            'timeliness_issues': timeliness_issues,
            'timeliness_score': max(0, timeliness_score)
        }

        return max(0, timeliness_score), details

    def is_valid_region(self, region: str) -> bool:
        """éªŒè¯åœ°åŒºåç§°æ˜¯å¦æœ‰æ•ˆ"""
        valid_regions = {
            'åŒ—äº¬', 'ä¸Šæµ·', 'å¤©æ´¥', 'é‡åº†',
            'æ²³åŒ—', 'å±±è¥¿', 'è¾½å®', 'å‰æ—', 'é»‘é¾™æ±Ÿ',
            'æ±Ÿè‹', 'æµ™æ±Ÿ', 'å®‰å¾½', 'ç¦å»º', 'æ±Ÿè¥¿', 'å±±ä¸œ',
            'æ²³å—', 'æ¹–åŒ—', 'æ¹–å—', 'å¹¿ä¸œ', 'æµ·å—',
            'å››å·', 'è´µå·', 'äº‘å—', 'é™•è¥¿', 'ç”˜è‚ƒ', 'é’æµ·',
            'å†…è’™å¤', 'å¹¿è¥¿', 'è¥¿è—', 'å®å¤', 'æ–°ç–†',
            'é¦™æ¸¯', 'æ¾³é—¨', 'å°æ¹¾'
        }

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡å‡†åœ°åŒºåæˆ–åŒ…å«æ ‡å‡†åœ°åŒºå
        for valid_region in valid_regions:
            if valid_region in region:
                return True

        return False

    def is_valid_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼"""
        url_pattern = re.compile(
            r'^https?://'  # http:// or https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
            r'localhost|'  # localhost...
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)

        return url_pattern.match(url) is not None
```

## âœ… ç¬¬ä¸‰é˜¶æ®µéªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] é…ç½®åŒ–æ•°æ®æå–æ­£å¸¸å·¥ä½œ
- [ ] æ•°æ®æ¸…æ´—æµæ°´çº¿å¤„ç†å‡†ç¡®
- [ ] æ•°æ®è´¨é‡è¯„åˆ†ç³»ç»Ÿæœ‰æ•ˆ
- [ ] å¼‚å¸¸æ•°æ®è‡ªåŠ¨æ ‡è®°å’Œå¤„ç†

### æ€§èƒ½éªŒæ”¶
- [ ] æ•°æ®æå–é€Ÿåº¦ â‰¥ 1000æ¡/åˆ†é’Ÿ
- [ ] æ•°æ®æ¸…æ´—å»¶è¿Ÿ < 100ms/æ¡
- [ ] æ•°æ®éªŒè¯å‡†ç¡®ç‡ â‰¥ 95%

### è´¨é‡éªŒæ”¶
- [ ] æ•°æ®å®Œæ•´æ€§ â‰¥ 95%
- [ ] æ•°æ®å‡†ç¡®æ€§ â‰¥ 90%
- [ ] æ•°æ®ä¸€è‡´æ€§ â‰¥ 95%

## âš™ï¸ ç¬¬å››é˜¶æ®µï¼šä»»åŠ¡è°ƒåº¦ä¸åˆ†å‘ (14-16å‘¨)

### 4.1 åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦å™¨ (ç¬¬14å‘¨)

#### ä»»åŠ¡è°ƒåº¦æ ¸å¿ƒ
```python
# scheduler/task_scheduler.py
import json
import time
import hashlib
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from enum import Enum

class TaskPriority(Enum):
    LOW = 1
    NORMAL = 2
    HIGH = 3
    URGENT = 4

@dataclass
class CrawlTask:
    task_id: str
    spider_name: str
    url: str
    priority: TaskPriority
    site_config: Dict
    retry_count: int = 0
    max_retries: int = 3
    created_at: float = None
    scheduled_at: float = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()
        if self.task_id is None:
            self.task_id = self.generate_task_id()

    def generate_task_id(self) -> str:
        """ç”Ÿæˆä»»åŠ¡ID"""
        content = f"{self.spider_name}:{self.url}:{self.created_at}"
        return hashlib.md5(content.encode()).hexdigest()

class DistributedTaskScheduler:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.task_queue_key = "crawler:task_queue"
        self.processing_key = "crawler:processing"
        self.completed_key = "crawler:completed"
        self.failed_key = "crawler:failed"

        # è´Ÿè½½å‡è¡¡ç­–ç•¥
        self.load_balancer = LoadBalancer(redis_client)

        # ä»»åŠ¡ç›‘æ§
        self.monitor = TaskMonitor(redis_client)

    def submit_task(self, task: CrawlTask) -> bool:
        """æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        try:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å­˜åœ¨
            if self.is_task_exists(task.task_id):
                return False

            # æ ¹æ®ä¼˜å…ˆçº§åˆ†é…åˆ°ä¸åŒé˜Ÿåˆ—
            queue_key = f"{self.task_queue_key}:{task.priority.name.lower()}"

            # åºåˆ—åŒ–ä»»åŠ¡
            task_data = json.dumps(asdict(task))

            # æ·»åŠ åˆ°Redisé˜Ÿåˆ—
            self.redis.lpush(queue_key, task_data)

            # è®°å½•ä»»åŠ¡æäº¤
            self.monitor.record_task_submitted(task)

            return True

        except Exception as e:
            self.logger.error(f"æäº¤ä»»åŠ¡å¤±è´¥: {e}")
            return False

    def get_next_task(self, worker_id: str) -> Optional[CrawlTask]:
        """è·å–ä¸‹ä¸€ä¸ªä»»åŠ¡"""
        # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥é˜Ÿåˆ—
        priority_queues = [
            f"{self.task_queue_key}:urgent",
            f"{self.task_queue_key}:high",
            f"{self.task_queue_key}:normal",
            f"{self.task_queue_key}:low"
        ]

        for queue_key in priority_queues:
            task_data = self.redis.brpop(queue_key, timeout=1)
            if task_data:
                try:
                    task_dict = json.loads(task_data[1])
                    task = CrawlTask(**task_dict)

                    # æ ‡è®°ä»»åŠ¡ä¸ºå¤„ç†ä¸­
                    self.mark_task_processing(task, worker_id)

                    return task

                except Exception as e:
                    self.logger.error(f"è§£æä»»åŠ¡å¤±è´¥: {e}")
                    continue

        return None

    def mark_task_processing(self, task: CrawlTask, worker_id: str):
        """æ ‡è®°ä»»åŠ¡ä¸ºå¤„ç†ä¸­"""
        processing_data = {
            'task': asdict(task),
            'worker_id': worker_id,
            'start_time': time.time()
        }

        self.redis.hset(
            self.processing_key,
            task.task_id,
            json.dumps(processing_data)
        )

    def complete_task(self, task_id: str, result: Dict):
        """å®Œæˆä»»åŠ¡"""
        # ä»å¤„ç†ä¸­ç§»é™¤
        self.redis.hdel(self.processing_key, task_id)

        # æ·»åŠ åˆ°å®Œæˆé˜Ÿåˆ—
        completion_data = {
            'task_id': task_id,
            'result': result,
            'completed_at': time.time()
        }

        self.redis.hset(
            self.completed_key,
            task_id,
            json.dumps(completion_data)
        )

        # è®°å½•å®Œæˆç»Ÿè®¡
        self.monitor.record_task_completed(task_id, result)

    def fail_task(self, task_id: str, error: str, retry: bool = True):
        """ä»»åŠ¡å¤±è´¥å¤„ç†"""
        processing_data = self.redis.hget(self.processing_key, task_id)
        if not processing_data:
            return

        processing_info = json.loads(processing_data)
        task_dict = processing_info['task']
        task = CrawlTask(**task_dict)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
        if retry and task.retry_count < task.max_retries:
            task.retry_count += 1
            task.scheduled_at = time.time() + (2 ** task.retry_count) * 60  # æŒ‡æ•°é€€é¿

            # é‡æ–°æäº¤ä»»åŠ¡
            self.submit_task(task)
        else:
            # æ ‡è®°ä¸ºæœ€ç»ˆå¤±è´¥
            failure_data = {
                'task': asdict(task),
                'error': error,
                'failed_at': time.time(),
                'retry_count': task.retry_count
            }

            self.redis.hset(
                self.failed_key,
                task_id,
                json.dumps(failure_data)
            )

        # ä»å¤„ç†ä¸­ç§»é™¤
        self.redis.hdel(self.processing_key, task_id)

        # è®°å½•å¤±è´¥ç»Ÿè®¡
        self.monitor.record_task_failed(task_id, error)
```

#### è´Ÿè½½å‡è¡¡å™¨
```python
# scheduler/load_balancer.py
import time
from typing import Dict, List
from collections import defaultdict

class LoadBalancer:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.worker_stats_key = "crawler:worker_stats"
        self.site_workers_key = "crawler:site_workers"

    def register_worker(self, worker_id: str, capabilities: Dict):
        """æ³¨å†Œå·¥ä½œèŠ‚ç‚¹"""
        worker_info = {
            'worker_id': worker_id,
            'capabilities': capabilities,
            'registered_at': time.time(),
            'last_heartbeat': time.time(),
            'active_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0
        }

        self.redis.hset(
            self.worker_stats_key,
            worker_id,
            json.dumps(worker_info)
        )

    def update_worker_heartbeat(self, worker_id: str):
        """æ›´æ–°å·¥ä½œèŠ‚ç‚¹å¿ƒè·³"""
        worker_data = self.redis.hget(self.worker_stats_key, worker_id)
        if worker_data:
            worker_info = json.loads(worker_data)
            worker_info['last_heartbeat'] = time.time()

            self.redis.hset(
                self.worker_stats_key,
                worker_id,
                json.dumps(worker_info)
            )

    def get_best_worker(self, site_domain: str) -> str:
        """è·å–æœ€é€‚åˆçš„å·¥ä½œèŠ‚ç‚¹"""
        # è·å–æ‰€æœ‰æ´»è·ƒå·¥ä½œèŠ‚ç‚¹
        active_workers = self.get_active_workers()

        if not active_workers:
            return None

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸“é—¨å¤„ç†è¯¥ç«™ç‚¹çš„å·¥ä½œèŠ‚ç‚¹
        site_workers = self.get_site_workers(site_domain)
        if site_workers:
            active_site_workers = [w for w in site_workers if w in active_workers]
            if active_site_workers:
                return self.select_least_loaded_worker(active_site_workers)

        # é€‰æ‹©è´Ÿè½½æœ€ä½çš„å·¥ä½œèŠ‚ç‚¹
        return self.select_least_loaded_worker(active_workers)

    def get_active_workers(self) -> List[str]:
        """è·å–æ´»è·ƒçš„å·¥ä½œèŠ‚ç‚¹"""
        active_workers = []
        current_time = time.time()

        all_workers = self.redis.hgetall(self.worker_stats_key)

        for worker_id, worker_data in all_workers.items():
            worker_info = json.loads(worker_data)

            # æ£€æŸ¥å¿ƒè·³æ—¶é—´ï¼ˆ5åˆ†é’Ÿå†…æœ‰å¿ƒè·³è®¤ä¸ºæ˜¯æ´»è·ƒçš„ï¼‰
            if current_time - worker_info['last_heartbeat'] < 300:
                active_workers.append(worker_id.decode() if isinstance(worker_id, bytes) else worker_id)

        return active_workers

    def select_least_loaded_worker(self, workers: List[str]) -> str:
        """é€‰æ‹©è´Ÿè½½æœ€ä½çš„å·¥ä½œèŠ‚ç‚¹"""
        if not workers:
            return None

        worker_loads = {}

        for worker_id in workers:
            worker_data = self.redis.hget(self.worker_stats_key, worker_id)
            if worker_data:
                worker_info = json.loads(worker_data)
                # è®¡ç®—è´Ÿè½½åˆ†æ•°ï¼ˆæ´»è·ƒä»»åŠ¡æ•° + å¤±è´¥ç‡æƒé‡ï¼‰
                active_tasks = worker_info.get('active_tasks', 0)
                total_tasks = worker_info.get('completed_tasks', 0) + worker_info.get('failed_tasks', 0)
                failure_rate = worker_info.get('failed_tasks', 0) / max(total_tasks, 1)

                load_score = active_tasks + (failure_rate * 10)
                worker_loads[worker_id] = load_score

        # è¿”å›è´Ÿè½½æœ€ä½çš„å·¥ä½œèŠ‚ç‚¹
        return min(worker_loads.keys(), key=lambda x: worker_loads[x])
```

### 4.2 é…ç½®ç®¡ç†ç³»ç»Ÿ (ç¬¬15å‘¨)

#### åŠ¨æ€é…ç½®ç®¡ç†
```python
# scheduler/config_manager.py
import yaml
import json
import time
from typing import Dict, Any, Optional
from pathlib import Path
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class ConfigManager:
    def __init__(self, config_dir: str, redis_client):
        self.config_dir = Path(config_dir)
        self.redis = redis_client
        self.config_cache = {}
        self.config_versions = {}

        # é…ç½®æ–‡ä»¶ç›‘æ§
        self.observer = Observer()
        self.observer.schedule(
            ConfigFileHandler(self),
            str(self.config_dir),
            recursive=True
        )
        self.observer.start()

        # åˆå§‹åŠ è½½é…ç½®
        self.load_all_configs()

    def load_all_configs(self):
        """åŠ è½½æ‰€æœ‰é…ç½®æ–‡ä»¶"""
        for config_file in self.config_dir.rglob("*.yaml"):
            self.load_config_file(config_file)

    def load_config_file(self, config_path: Path):
        """åŠ è½½å•ä¸ªé…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = yaml.safe_load(f)

            # ç”Ÿæˆé…ç½®é”®å
            relative_path = config_path.relative_to(self.config_dir)
            config_key = str(relative_path).replace('\\', '/').replace('.yaml', '')

            # ç‰ˆæœ¬æ§åˆ¶
            version = int(time.time())

            # ç¼“å­˜é…ç½®
            self.config_cache[config_key] = config_data
            self.config_versions[config_key] = version

            # å­˜å‚¨åˆ°Redis
            self.redis.hset(
                "crawler:configs",
                config_key,
                json.dumps({
                    'data': config_data,
                    'version': version,
                    'updated_at': time.time()
                })
            )

            self.logger.info(f"é…ç½®æ–‡ä»¶å·²åŠ è½½: {config_key}")

        except Exception as e:
            self.logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ {config_path}: {e}")

    def get_config(self, config_key: str, use_cache: bool = True) -> Optional[Dict[str, Any]]:
        """è·å–é…ç½®"""
        if use_cache and config_key in self.config_cache:
            return self.config_cache[config_key]

        # ä»Redisè·å–
        config_data = self.redis.hget("crawler:configs", config_key)
        if config_data:
            config_info = json.loads(config_data)
            return config_info['data']

        return None

    def update_config(self, config_key: str, config_data: Dict[str, Any]) -> bool:
        """æ›´æ–°é…ç½®"""
        try:
            # éªŒè¯é…ç½®
            if not self.validate_config(config_key, config_data):
                return False

            # å¤‡ä»½å½“å‰é…ç½®
            self.backup_config(config_key)

            # æ›´æ–°ç¼“å­˜
            self.config_cache[config_key] = config_data

            # æ›´æ–°ç‰ˆæœ¬
            version = int(time.time())
            self.config_versions[config_key] = version

            # å­˜å‚¨åˆ°Redis
            self.redis.hset(
                "crawler:configs",
                config_key,
                json.dumps({
                    'data': config_data,
                    'version': version,
                    'updated_at': time.time()
                })
            )

            # å†™å…¥æ–‡ä»¶
            config_path = self.config_dir / f"{config_key}.yaml"
            config_path.parent.mkdir(parents=True, exist_ok=True)

            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True)

            # é€šçŸ¥é…ç½®æ›´æ–°
            self.notify_config_update(config_key, version)

            return True

        except Exception as e:
            self.logger.error(f"æ›´æ–°é…ç½®å¤±è´¥ {config_key}: {e}")
            return False

    def validate_config(self, config_key: str, config_data: Dict[str, Any]) -> bool:
        """éªŒè¯é…ç½®æ•°æ®"""
        # æ ¹æ®é…ç½®ç±»å‹è¿›è¡ŒéªŒè¯
        if config_key.startswith('sites/'):
            return self.validate_site_config(config_data)
        elif config_key == 'proxy':
            return self.validate_proxy_config(config_data)
        elif config_key == 'database':
            return self.validate_database_config(config_data)

        return True

    def validate_site_config(self, config: Dict[str, Any]) -> bool:
        """éªŒè¯ç½‘ç«™é…ç½®"""
        required_fields = ['site_name', 'base_url', 'fields']

        for field in required_fields:
            if field not in config:
                self.logger.error(f"ç½‘ç«™é…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                return False

        # éªŒè¯å­—æ®µé…ç½®
        fields = config.get('fields', {})
        for field_name, field_config in fields.items():
            if 'method' not in field_config or 'selector' not in field_config:
                self.logger.error(f"å­—æ®µé…ç½®ä¸å®Œæ•´: {field_name}")
                return False

        return True

    def backup_config(self, config_key: str):
        """å¤‡ä»½é…ç½®"""
        current_config = self.get_config(config_key)
        if current_config:
            backup_key = f"crawler:config_backups:{config_key}:{int(time.time())}"
            self.redis.set(backup_key, json.dumps(current_config))

            # è®¾ç½®å¤‡ä»½è¿‡æœŸæ—¶é—´ï¼ˆ30å¤©ï¼‰
            self.redis.expire(backup_key, 30 * 24 * 3600)

    def rollback_config(self, config_key: str, backup_timestamp: int) -> bool:
        """å›æ»šé…ç½®"""
        backup_key = f"crawler:config_backups:{config_key}:{backup_timestamp}"
        backup_data = self.redis.get(backup_key)

        if backup_data:
            config_data = json.loads(backup_data)
            return self.update_config(config_key, config_data)

        return False

    def notify_config_update(self, config_key: str, version: int):
        """é€šçŸ¥é…ç½®æ›´æ–°"""
        notification = {
            'type': 'config_update',
            'config_key': config_key,
            'version': version,
            'timestamp': time.time()
        }

        self.redis.publish("crawler:config_updates", json.dumps(notification))

class ConfigFileHandler(FileSystemEventHandler):
    def __init__(self, config_manager):
        self.config_manager = config_manager

    def on_modified(self, event):
        if not event.is_directory and event.src_path.endswith('.yaml'):
            self.config_manager.load_config_file(Path(event.src_path))
```

## âœ… ç¬¬å››é˜¶æ®µéªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦æ­£å¸¸è¿è¡Œ
- [ ] è´Ÿè½½å‡è¡¡æœ‰æ•ˆåˆ†é…ä»»åŠ¡
- [ ] é…ç½®çƒ­æ›´æ–°åŠŸèƒ½æ­£å¸¸
- [ ] ä»»åŠ¡å¤±è´¥é‡è¯•æœºåˆ¶æœ‰æ•ˆ

### æ€§èƒ½éªŒæ”¶
- [ ] ä»»åŠ¡åˆ†å‘å»¶è¿Ÿ < 100ms
- [ ] è´Ÿè½½å‡è¡¡å“åº”æ—¶é—´ < 50ms
- [ ] é…ç½®æ›´æ–°ä¼ æ’­æ—¶é—´ < 5ç§’

### è´¨é‡éªŒæ”¶
- [ ] ä»»åŠ¡è°ƒåº¦å‡†ç¡®ç‡ â‰¥ 99%
- [ ] ç³»ç»Ÿå®¹é”™èƒ½åŠ›è‰¯å¥½
- [ ] é…ç½®ä¸€è‡´æ€§ä¿è¯

## ğŸ“ˆ ç¬¬äº”é˜¶æ®µï¼šç›‘æ§ä¸è¿ç»´ (17-20å‘¨)

### 5.1 ç›‘æ§ç³»ç»Ÿæ­å»º (ç¬¬17å‘¨)

#### Prometheusé…ç½®
```yaml
# deployment/monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "crawler_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'crawler-system'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']

  - job_name: 'mongodb'
    static_configs:
      - targets: ['mongodb:27017']

  - job_name: 'postgresql'
    static_configs:
      - targets: ['postgresql:5432']
```

#### ç›‘æ§æŒ‡æ ‡æ”¶é›†
```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import threading
from typing import Dict, Any

class CrawlerMetrics:
    def __init__(self):
        # ä¸šåŠ¡æŒ‡æ ‡
        self.pages_crawled = Counter('crawler_pages_total', 'Total pages crawled', ['spider', 'status'])
        self.crawl_duration = Histogram('crawler_duration_seconds', 'Time spent crawling', ['spider'])
        self.data_quality_score = Gauge('crawler_data_quality', 'Data quality score', ['spider', 'site'])

        # ç³»ç»ŸæŒ‡æ ‡
        self.active_spiders = Gauge('crawler_active_spiders', 'Number of active spiders')
        self.queue_size = Gauge('crawler_queue_size', 'Size of crawl queue', ['priority'])
        self.proxy_pool_size = Gauge('crawler_proxy_pool_size', 'Size of proxy pool', ['status'])

        # é”™è¯¯æŒ‡æ ‡
        self.errors_total = Counter('crawler_errors_total', 'Total errors', ['spider', 'error_type'])
        self.retry_attempts = Counter('crawler_retries_total', 'Total retry attempts', ['spider'])

        # æ€§èƒ½æŒ‡æ ‡
        self.response_time = Histogram('crawler_response_time_seconds', 'Response time', ['spider'])
        self.memory_usage = Gauge('crawler_memory_usage_bytes', 'Memory usage')
        self.cpu_usage = Gauge('crawler_cpu_usage_percent', 'CPU usage percentage')

        # å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨
        start_http_server(8000)

        # å¯åŠ¨ç³»ç»ŸæŒ‡æ ‡æ”¶é›†
        self.start_system_metrics_collection()

    def record_page_crawled(self, spider_name: str, status: str):
        """è®°å½•é¡µé¢çˆ¬å–"""
        self.pages_crawled.labels(spider=spider_name, status=status).inc()

    def record_crawl_duration(self, spider_name: str, duration: float):
        """è®°å½•çˆ¬å–è€—æ—¶"""
        self.crawl_duration.labels(spider=spider_name).observe(duration)

    def update_data_quality(self, spider_name: str, site: str, score: float):
        """æ›´æ–°æ•°æ®è´¨é‡åˆ†æ•°"""
        self.data_quality_score.labels(spider=spider_name, site=site).set(score)

    def update_queue_size(self, priority: str, size: int):
        """æ›´æ–°é˜Ÿåˆ—å¤§å°"""
        self.queue_size.labels(priority=priority).set(size)

    def update_proxy_pool(self, status: str, count: int):
        """æ›´æ–°ä»£ç†æ± çŠ¶æ€"""
        self.proxy_pool_size.labels(status=status).set(count)

    def record_error(self, spider_name: str, error_type: str):
        """è®°å½•é”™è¯¯"""
        self.errors_total.labels(spider=spider_name, error_type=error_type).inc()

    def record_retry(self, spider_name: str):
        """è®°å½•é‡è¯•"""
        self.retry_attempts.labels(spider=spider_name).inc()

    def record_response_time(self, spider_name: str, response_time: float):
        """è®°å½•å“åº”æ—¶é—´"""
        self.response_time.labels(spider=spider_name).observe(response_time)

    def start_system_metrics_collection(self):
        """å¯åŠ¨ç³»ç»ŸæŒ‡æ ‡æ”¶é›†"""
        def collect_system_metrics():
            import psutil
            while True:
                try:
                    # CPUä½¿ç”¨ç‡
                    cpu_percent = psutil.cpu_percent(interval=1)
                    self.cpu_usage.set(cpu_percent)

                    # å†…å­˜ä½¿ç”¨
                    memory = psutil.virtual_memory()
                    self.memory_usage.set(memory.used)

                    time.sleep(10)
                except Exception as e:
                    print(f"ç³»ç»ŸæŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")

        thread = threading.Thread(target=collect_system_metrics, daemon=True)
        thread.start()
```

#### Grafanaä»ªè¡¨æ¿é…ç½®
```json
{
  "dashboard": {
    "title": "çˆ¬è™«ç³»ç»Ÿç›‘æ§",
    "panels": [
      {
        "title": "çˆ¬å–æˆåŠŸç‡",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(crawler_pages_total{status=\"success\"}[5m]) / rate(crawler_pages_total[5m]) * 100"
          }
        ]
      },
      {
        "title": "æ¯åˆ†é’Ÿçˆ¬å–é¡µé¢æ•°",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(crawler_pages_total[1m]) * 60"
          }
        ]
      },
      {
        "title": "æ•°æ®è´¨é‡åˆ†æ•°",
        "type": "graph",
        "targets": [
          {
            "expr": "crawler_data_quality"
          }
        ]
      },
      {
        "title": "é˜Ÿåˆ—å¤§å°",
        "type": "graph",
        "targets": [
          {
            "expr": "crawler_queue_size"
          }
        ]
      },
      {
        "title": "é”™è¯¯ç‡è¶‹åŠ¿",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(crawler_errors_total[5m])"
          }
        ]
      },
      {
        "title": "ç³»ç»Ÿèµ„æºä½¿ç”¨",
        "type": "graph",
        "targets": [
          {
            "expr": "crawler_cpu_usage_percent"
          },
          {
            "expr": "crawler_memory_usage_bytes / 1024 / 1024 / 1024"
          }
        ]
      }
    ]
  }
}
```

### 5.2 æ—¥å¿—ç³»ç»Ÿ (ç¬¬18å‘¨)

#### ELK Stacké…ç½®
```yaml
# deployment/logging/docker-compose.yml
version: '3.8'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:7.15.0
    ports:
      - "5044:5044"
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:7.15.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
```

#### ç»“æ„åŒ–æ—¥å¿—è®°å½•
```python
# monitoring/logger.py
import json
import logging
import time
from typing import Dict, Any, Optional
from datetime import datetime

class StructuredLogger:
    def __init__(self, name: str, level: int = logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)

        # é…ç½®å¤„ç†å™¨
        handler = logging.StreamHandler()
        formatter = JsonFormatter()
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)

    def log_crawl_start(self, spider_name: str, url: str, task_id: str):
        """è®°å½•çˆ¬å–å¼€å§‹"""
        self.logger.info("crawl_start", extra={
            'event_type': 'crawl_start',
            'spider_name': spider_name,
            'url': url,
            'task_id': task_id,
            'timestamp': time.time()
        })

    def log_crawl_success(self, spider_name: str, url: str, task_id: str,
                         duration: float, data_count: int):
        """è®°å½•çˆ¬å–æˆåŠŸ"""
        self.logger.info("crawl_success", extra={
            'event_type': 'crawl_success',
            'spider_name': spider_name,
            'url': url,
            'task_id': task_id,
            'duration': duration,
            'data_count': data_count,
            'timestamp': time.time()
        })

    def log_crawl_error(self, spider_name: str, url: str, task_id: str,
                       error_type: str, error_message: str):
        """è®°å½•çˆ¬å–é”™è¯¯"""
        self.logger.error("crawl_error", extra={
            'event_type': 'crawl_error',
            'spider_name': spider_name,
            'url': url,
            'task_id': task_id,
            'error_type': error_type,
            'error_message': error_message,
            'timestamp': time.time()
        })

    def log_data_quality(self, spider_name: str, site: str, quality_score: float,
                        validation_details: Dict[str, Any]):
        """è®°å½•æ•°æ®è´¨é‡"""
        self.logger.info("data_quality", extra={
            'event_type': 'data_quality',
            'spider_name': spider_name,
            'site': site,
            'quality_score': quality_score,
            'validation_details': validation_details,
            'timestamp': time.time()
        })

    def log_proxy_status(self, proxy_url: str, status: str, response_time: float):
        """è®°å½•ä»£ç†çŠ¶æ€"""
        self.logger.info("proxy_status", extra={
            'event_type': 'proxy_status',
            'proxy_url': proxy_url,
            'status': status,
            'response_time': response_time,
            'timestamp': time.time()
        })

class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_entry = {
            'timestamp': datetime.fromtimestamp(record.created).isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
        }

        # æ·»åŠ é¢å¤–å­—æ®µ
        if hasattr(record, 'event_type'):
            log_entry.update({
                key: value for key, value in record.__dict__.items()
                if key not in ['name', 'msg', 'args', 'levelname', 'levelno',
                              'pathname', 'filename', 'module', 'lineno',
                              'funcName', 'created', 'msecs', 'relativeCreated',
                              'thread', 'threadName', 'processName', 'process',
                              'getMessage', 'exc_info', 'exc_text', 'stack_info']
            })

        return json.dumps(log_entry, ensure_ascii=False)
```

### 5.3 å‘Šè­¦ç³»ç»Ÿ (ç¬¬19å‘¨)

#### å‘Šè­¦è§„åˆ™é…ç½®
```yaml
# deployment/monitoring/crawler_rules.yml
groups:
  - name: crawler_alerts
    rules:
      - alert: CrawlSuccessRateLow
        expr: rate(crawler_pages_total{status="success"}[5m]) / rate(crawler_pages_total[5m]) < 0.8
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "çˆ¬å–æˆåŠŸç‡è¿‡ä½"
          description: "çˆ¬å–æˆåŠŸç‡åœ¨è¿‡å»5åˆ†é’Ÿå†…ä½äº80%"

      - alert: HighErrorRate
        expr: rate(crawler_errors_total[5m]) > 10
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "é”™è¯¯ç‡è¿‡é«˜"
          description: "æ¯åˆ†é’Ÿé”™è¯¯æ•°è¶…è¿‡10ä¸ª"

      - alert: QueueSizeHigh
        expr: crawler_queue_size > 10000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "é˜Ÿåˆ—ç§¯å‹ä¸¥é‡"
          description: "çˆ¬å–é˜Ÿåˆ—å¤§å°è¶…è¿‡10000"

      - alert: ProxyPoolLow
        expr: crawler_proxy_pool_size{status="available"} < 10
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "å¯ç”¨ä»£ç†ä¸è¶³"
          description: "å¯ç”¨ä»£ç†æ•°é‡å°‘äº10ä¸ª"

      - alert: DataQualityLow
        expr: crawler_data_quality < 0.7
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "æ•°æ®è´¨é‡ä¸‹é™"
          description: "æ•°æ®è´¨é‡åˆ†æ•°ä½äº0.7"

      - alert: SystemResourceHigh
        expr: crawler_cpu_usage_percent > 80 or crawler_memory_usage_bytes / 1024 / 1024 / 1024 > 8
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡è¿‡é«˜"
          description: "CPUä½¿ç”¨ç‡è¶…è¿‡80%æˆ–å†…å­˜ä½¿ç”¨è¶…è¿‡8GB"
```

#### å¤šæ¸ é“å‘Šè­¦é€šçŸ¥
```python
# monitoring/alerting.py
import requests
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from typing import Dict, List, Any
import json

class AlertManager:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.notification_channels = {
            'email': self.send_email_alert,
            'slack': self.send_slack_alert,
            'webhook': self.send_webhook_alert,
            'sms': self.send_sms_alert
        }

    def send_alert(self, alert: Dict[str, Any]):
        """å‘é€å‘Šè­¦"""
        severity = alert.get('labels', {}).get('severity', 'info')
        channels = self.get_channels_for_severity(severity)

        for channel in channels:
            if channel in self.notification_channels:
                try:
                    self.notification_channels[channel](alert)
                except Exception as e:
                    print(f"å‘é€å‘Šè­¦å¤±è´¥ {channel}: {e}")

    def get_channels_for_severity(self, severity: str) -> List[str]:
        """æ ¹æ®ä¸¥é‡ç¨‹åº¦è·å–é€šçŸ¥æ¸ é“"""
        channel_config = self.config.get('channels', {})

        if severity == 'critical':
            return channel_config.get('critical', ['email', 'slack', 'sms'])
        elif severity == 'warning':
            return channel_config.get('warning', ['email', 'slack'])
        else:
            return channel_config.get('info', ['slack'])

    def send_email_alert(self, alert: Dict[str, Any]):
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        email_config = self.config.get('email', {})

        msg = MIMEMultipart()
        msg['From'] = email_config['from']
        msg['To'] = ', '.join(email_config['to'])
        msg['Subject'] = f"[çˆ¬è™«å‘Šè­¦] {alert['annotations']['summary']}"

        body = f"""
        å‘Šè­¦åç§°: {alert['alertname']}
        ä¸¥é‡ç¨‹åº¦: {alert['labels']['severity']}
        æè¿°: {alert['annotations']['description']}
        æ—¶é—´: {alert['startsAt']}

        è¯¦ç»†ä¿¡æ¯:
        {json.dumps(alert, indent=2, ensure_ascii=False)}
        """

        msg.attach(MIMEText(body, 'plain', 'utf-8'))

        server = smtplib.SMTP(email_config['smtp_server'], email_config['smtp_port'])
        server.starttls()
        server.login(email_config['username'], email_config['password'])
        server.send_message(msg)
        server.quit()

    def send_slack_alert(self, alert: Dict[str, Any]):
        """å‘é€Slackå‘Šè­¦"""
        slack_config = self.config.get('slack', {})

        color = {
            'critical': 'danger',
            'warning': 'warning',
            'info': 'good'
        }.get(alert['labels']['severity'], 'good')

        payload = {
            'channel': slack_config['channel'],
            'username': 'CrawlerBot',
            'attachments': [{
                'color': color,
                'title': alert['annotations']['summary'],
                'text': alert['annotations']['description'],
                'fields': [
                    {
                        'title': 'å‘Šè­¦åç§°',
                        'value': alert['alertname'],
                        'short': True
                    },
                    {
                        'title': 'ä¸¥é‡ç¨‹åº¦',
                        'value': alert['labels']['severity'],
                        'short': True
                    }
                ],
                'ts': alert['startsAt']
            }]
        }

        response = requests.post(
            slack_config['webhook_url'],
            json=payload,
            timeout=10
        )
        response.raise_for_status()

    def send_webhook_alert(self, alert: Dict[str, Any]):
        """å‘é€Webhookå‘Šè­¦"""
        webhook_config = self.config.get('webhook', {})

        response = requests.post(
            webhook_config['url'],
            json=alert,
            headers=webhook_config.get('headers', {}),
            timeout=10
        )
        response.raise_for_status()

    def send_sms_alert(self, alert: Dict[str, Any]):
        """å‘é€çŸ­ä¿¡å‘Šè­¦"""
        sms_config = self.config.get('sms', {})

        message = f"[çˆ¬è™«å‘Šè­¦] {alert['annotations']['summary']}: {alert['annotations']['description']}"

        # è¿™é‡Œé›†æˆçŸ­ä¿¡æœåŠ¡API
        # ä¾‹å¦‚ï¼šé˜¿é‡Œäº‘çŸ­ä¿¡ã€è…¾è®¯äº‘çŸ­ä¿¡ç­‰
        pass
```

## âœ… ç¬¬äº”é˜¶æ®µéªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] ç›‘æ§æŒ‡æ ‡æ­£å¸¸æ”¶é›†å’Œå±•ç¤º
- [ ] æ—¥å¿—èšåˆå’Œåˆ†æåŠŸèƒ½æ­£å¸¸
- [ ] å‘Šè­¦è§„åˆ™æœ‰æ•ˆè§¦å‘
- [ ] å¤šæ¸ é“é€šçŸ¥æ­£å¸¸å·¥ä½œ

### æ€§èƒ½éªŒæ”¶
- [ ] ç›‘æ§æ•°æ®å»¶è¿Ÿ < 30ç§’
- [ ] æ—¥å¿—å¤„ç†ååé‡ â‰¥ 1000æ¡/ç§’
- [ ] å‘Šè­¦å“åº”æ—¶é—´ < 2åˆ†é’Ÿ

### è´¨é‡éªŒæ”¶
- [ ] ç›‘æ§è¦†ç›–ç‡ â‰¥ 95%
- [ ] å‘Šè­¦å‡†ç¡®ç‡ â‰¥ 90%
- [ ] ç³»ç»Ÿå¯è§‚æµ‹æ€§è‰¯å¥½

## ğŸš€ ç¬¬å…­é˜¶æ®µï¼šéƒ¨ç½²ä¸æµ‹è¯• (21-24å‘¨)

### 6.1 å®¹å™¨åŒ–éƒ¨ç½² (ç¬¬21å‘¨)

#### å¤šé˜¶æ®µDockeræ„å»º
```dockerfile
# Dockerfile
FROM python:3.9-slim as builder

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libxml2-dev \
    libxslt-dev \
    libffi-dev \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# ç”Ÿäº§é˜¶æ®µ
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && apt-get install -y \
    libxml2 \
    libxslt1.1 \
    && rm -rf /var/lib/apt/lists/*

# ä»æ„å»ºé˜¶æ®µå¤åˆ¶PythonåŒ…
COPY --from=builder /usr/local/lib/python3.9/site-packages /usr/local/lib/python3.9/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -m -u 1000 crawler && chown -R crawler:crawler /app
USER crawler

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "crawler.main"]
```

#### Kuberneteséƒ¨ç½²é…ç½®
```yaml
# deployment/k8s/crawler-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crawler-system
  labels:
    app: crawler-system
spec:
  replicas: 5
  selector:
    matchLabels:
      app: crawler-system
  template:
    metadata:
      labels:
        app: crawler-system
    spec:
      containers:
      - name: crawler
        image: crawler-system:latest
        ports:
        - containerPort: 8000
        env:
        - name: REDIS_URL
          value: "redis://redis-service:6379/0"
        - name: MONGODB_URL
          value: "mongodb://mongodb-service:27017/crawler_db"
        - name: POSTGRES_URL
          value: "postgresql://postgres:password@postgres-service:5432/crawler_db"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
          readOnly: true
      volumes:
      - name: config-volume
        configMap:
          name: crawler-config
---
apiVersion: v1
kind: Service
metadata:
  name: crawler-service
spec:
  selector:
    app: crawler-system
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: crawler-config
data:
  settings.py: |
    # çˆ¬è™«é…ç½®
    CONCURRENT_REQUESTS = 16
    DOWNLOAD_DELAY = 1
    RANDOMIZE_DOWNLOAD_DELAY = 0.5

    # Redisé…ç½®
    REDIS_URL = 'redis://redis-service:6379/0'

    # æ•°æ®åº“é…ç½®
    DATABASES = {
        'mongodb': 'mongodb://mongodb-service:27017/crawler_db',
        'postgresql': 'postgresql://postgres:password@postgres-service:5432/crawler_db'
    }
```

#### è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# deployment/scripts/deploy.sh

set -e

# é…ç½®å˜é‡
NAMESPACE="crawler-system"
IMAGE_TAG=${1:-latest}
REGISTRY="your-registry.com"

echo "å¼€å§‹éƒ¨ç½²çˆ¬è™«ç³»ç»Ÿ..."

# åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

# æ„å»ºå’Œæ¨é€é•œåƒ
echo "æ„å»ºDockeré•œåƒ..."
docker build -t $REGISTRY/crawler-system:$IMAGE_TAG .
docker push $REGISTRY/crawler-system:$IMAGE_TAG

# æ›´æ–°éƒ¨ç½²é…ç½®ä¸­çš„é•œåƒæ ‡ç­¾
sed -i "s|image: crawler-system:latest|image: $REGISTRY/crawler-system:$IMAGE_TAG|g" deployment/k8s/crawler-deployment.yaml

# éƒ¨ç½²åŸºç¡€è®¾æ–½
echo "éƒ¨ç½²åŸºç¡€è®¾æ–½..."
kubectl apply -f deployment/k8s/redis-deployment.yaml -n $NAMESPACE
kubectl apply -f deployment/k8s/mongodb-deployment.yaml -n $NAMESPACE
kubectl apply -f deployment/k8s/postgres-deployment.yaml -n $NAMESPACE

# ç­‰å¾…åŸºç¡€è®¾æ–½å°±ç»ª
echo "ç­‰å¾…åŸºç¡€è®¾æ–½å°±ç»ª..."
kubectl wait --for=condition=available --timeout=300s deployment/redis -n $NAMESPACE
kubectl wait --for=condition=available --timeout=300s deployment/mongodb -n $NAMESPACE
kubectl wait --for=condition=available --timeout=300s deployment/postgres -n $NAMESPACE

# éƒ¨ç½²çˆ¬è™«ç³»ç»Ÿ
echo "éƒ¨ç½²çˆ¬è™«ç³»ç»Ÿ..."
kubectl apply -f deployment/k8s/crawler-deployment.yaml -n $NAMESPACE

# ç­‰å¾…éƒ¨ç½²å®Œæˆ
kubectl wait --for=condition=available --timeout=300s deployment/crawler-system -n $NAMESPACE

# éƒ¨ç½²ç›‘æ§ç³»ç»Ÿ
echo "éƒ¨ç½²ç›‘æ§ç³»ç»Ÿ..."
kubectl apply -f deployment/k8s/monitoring/ -n $NAMESPACE

echo "éƒ¨ç½²å®Œæˆï¼"

# æ˜¾ç¤ºæœåŠ¡çŠ¶æ€
kubectl get pods -n $NAMESPACE
kubectl get services -n $NAMESPACE
```

### 6.2 é›†æˆæµ‹è¯• (ç¬¬22å‘¨)

#### ç«¯åˆ°ç«¯æµ‹è¯•æ¡†æ¶
```python
# tests/e2e/test_crawler_system.py
import pytest
import requests
import time
import json
from typing import Dict, Any

class TestCrawlerSystem:
    def __init__(self):
        self.base_url = "http://localhost:8000"
        self.test_sites = [
            {
                'name': 'test_site_1',
                'url': 'http://example.com/test1',
                'expected_fields': ['title', 'content', 'date']
            },
            {
                'name': 'test_site_2',
                'url': 'http://example.com/test2',
                'expected_fields': ['confirmed_cases', 'death_cases', 'region']
            }
        ]

    def test_system_health(self):
        """æµ‹è¯•ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        response = requests.get(f"{self.base_url}/health")
        assert response.status_code == 200

        health_data = response.json()
        assert health_data['status'] == 'healthy'
        assert 'redis' in health_data['components']
        assert 'mongodb' in health_data['components']
        assert 'postgresql' in health_data['components']

    def test_task_submission(self):
        """æµ‹è¯•ä»»åŠ¡æäº¤"""
        for site in self.test_sites:
            task_data = {
                'spider_name': site['name'],
                'url': site['url'],
                'priority': 'normal'
            }

            response = requests.post(
                f"{self.base_url}/api/tasks",
                json=task_data
            )

            assert response.status_code == 201
            task_result = response.json()
            assert 'task_id' in task_result

    def test_crawling_workflow(self):
        """æµ‹è¯•å®Œæ•´çˆ¬å–æµç¨‹"""
        # æäº¤ä»»åŠ¡
        task_data = {
            'spider_name': 'test_spider',
            'url': 'http://httpbin.org/html',
            'priority': 'high'
        }

        response = requests.post(f"{self.base_url}/api/tasks", json=task_data)
        task_id = response.json()['task_id']

        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        max_wait_time = 300  # 5åˆ†é’Ÿ
        start_time = time.time()

        while time.time() - start_time < max_wait_time:
            response = requests.get(f"{self.base_url}/api/tasks/{task_id}")
            task_status = response.json()

            if task_status['status'] == 'completed':
                break
            elif task_status['status'] == 'failed':
                pytest.fail(f"ä»»åŠ¡å¤±è´¥: {task_status.get('error')}")

            time.sleep(5)
        else:
            pytest.fail("ä»»åŠ¡è¶…æ—¶æœªå®Œæˆ")

        # éªŒè¯ç»“æœ
        assert task_status['status'] == 'completed'
        assert 'result' in task_status
        assert task_status['result']['data_count'] > 0

    def test_data_quality(self):
        """æµ‹è¯•æ•°æ®è´¨é‡"""
        # è·å–æœ€è¿‘çš„æ•°æ®è´¨é‡æŠ¥å‘Š
        response = requests.get(f"{self.base_url}/api/quality/report")
        quality_report = response.json()

        assert quality_report['overall_score'] >= 0.8
        assert quality_report['completeness_rate'] >= 0.9
        assert quality_report['accuracy_rate'] >= 0.85

    def test_proxy_pool(self):
        """æµ‹è¯•ä»£ç†æ± """
        response = requests.get(f"{self.base_url}/api/proxy/status")
        proxy_status = response.json()

        assert proxy_status['total_proxies'] > 0
        assert proxy_status['available_proxies'] > 0
        assert proxy_status['success_rate'] >= 0.7

    def test_monitoring_metrics(self):
        """æµ‹è¯•ç›‘æ§æŒ‡æ ‡"""
        response = requests.get(f"{self.base_url}/metrics")
        assert response.status_code == 200

        metrics_text = response.text
        assert 'crawler_pages_total' in metrics_text
        assert 'crawler_queue_size' in metrics_text
        assert 'crawler_data_quality' in metrics_text

    def test_configuration_update(self):
        """æµ‹è¯•é…ç½®çƒ­æ›´æ–°"""
        # è·å–å½“å‰é…ç½®
        response = requests.get(f"{self.base_url}/api/config/test_site")
        original_config = response.json()

        # æ›´æ–°é…ç½®
        updated_config = original_config.copy()
        updated_config['rate_limit']['delay'] = 3.0

        response = requests.put(
            f"{self.base_url}/api/config/test_site",
            json=updated_config
        )
        assert response.status_code == 200

        # éªŒè¯é…ç½®å·²æ›´æ–°
        response = requests.get(f"{self.base_url}/api/config/test_site")
        current_config = response.json()
        assert current_config['rate_limit']['delay'] == 3.0

        # æ¢å¤åŸé…ç½®
        requests.put(f"{self.base_url}/api/config/test_site", json=original_config)
```

#### æ€§èƒ½å‹åŠ›æµ‹è¯•
```python
# tests/performance/load_test.py
import asyncio
import aiohttp
import time
import statistics
from typing import List, Dict

class LoadTester:
    def __init__(self, base_url: str, concurrent_users: int = 50):
        self.base_url = base_url
        self.concurrent_users = concurrent_users
        self.results = []

    async def submit_task(self, session: aiohttp.ClientSession, task_data: Dict):
        """æäº¤å•ä¸ªä»»åŠ¡"""
        start_time = time.time()

        try:
            async with session.post(
                f"{self.base_url}/api/tasks",
                json=task_data
            ) as response:
                result = await response.json()
                duration = time.time() - start_time

                self.results.append({
                    'status': 'success' if response.status == 201 else 'error',
                    'duration': duration,
                    'status_code': response.status
                })

        except Exception as e:
            duration = time.time() - start_time
            self.results.append({
                'status': 'error',
                'duration': duration,
                'error': str(e)
            })

    async def run_load_test(self, duration_seconds: int = 300):
        """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
        print(f"å¼€å§‹è´Ÿè½½æµ‹è¯•: {self.concurrent_users}å¹¶å‘ç”¨æˆ·, {duration_seconds}ç§’")

        async with aiohttp.ClientSession() as session:
            start_time = time.time()

            while time.time() - start_time < duration_seconds:
                tasks = []

                for i in range(self.concurrent_users):
                    task_data = {
                        'spider_name': f'load_test_spider_{i % 10}',
                        'url': f'http://httpbin.org/delay/{i % 3 + 1}',
                        'priority': 'normal'
                    }

                    task = asyncio.create_task(
                        self.submit_task(session, task_data)
                    )
                    tasks.append(task)

                await asyncio.gather(*tasks)
                await asyncio.sleep(1)  # 1ç§’é—´éš”

        self.analyze_results()

    def analyze_results(self):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        if not self.results:
            print("æ²¡æœ‰æµ‹è¯•ç»“æœ")
            return

        success_count = len([r for r in self.results if r['status'] == 'success'])
        error_count = len([r for r in self.results if r['status'] == 'error'])
        total_count = len(self.results)

        durations = [r['duration'] for r in self.results]

        print(f"\n=== è´Ÿè½½æµ‹è¯•ç»“æœ ===")
        print(f"æ€»è¯·æ±‚æ•°: {total_count}")
        print(f"æˆåŠŸè¯·æ±‚: {success_count} ({success_count/total_count*100:.2f}%)")
        print(f"å¤±è´¥è¯·æ±‚: {error_count} ({error_count/total_count*100:.2f}%)")
        print(f"å¹³å‡å“åº”æ—¶é—´: {statistics.mean(durations):.3f}ç§’")
        print(f"ä¸­ä½æ•°å“åº”æ—¶é—´: {statistics.median(durations):.3f}ç§’")
        print(f"95%å“åº”æ—¶é—´: {statistics.quantiles(durations, n=20)[18]:.3f}ç§’")
        print(f"æœ€å¤§å“åº”æ—¶é—´: {max(durations):.3f}ç§’")
        print(f"æœ€å°å“åº”æ—¶é—´: {min(durations):.3f}ç§’")

        # æ€§èƒ½åŸºå‡†æ£€æŸ¥
        avg_response_time = statistics.mean(durations)
        success_rate = success_count / total_count

        assert success_rate >= 0.95, f"æˆåŠŸç‡è¿‡ä½: {success_rate:.2f}"
        assert avg_response_time <= 2.0, f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time:.3f}ç§’"

        print("\nâœ… æ€§èƒ½æµ‹è¯•é€šè¿‡!")

if __name__ == "__main__":
    tester = LoadTester("http://localhost:8000", concurrent_users=100)
    asyncio.run(tester.run_load_test(duration_seconds=600))  # 10åˆ†é’Ÿæµ‹è¯•
```

### 6.3 ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² (ç¬¬23-24å‘¨)

#### ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•
```markdown
# ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ£€æŸ¥æ¸…å•

## å®‰å…¨æ£€æŸ¥
- [ ] æ‰€æœ‰å¯†ç å’Œå¯†é’¥å·²æ›´æ¢ä¸ºç”Ÿäº§ç¯å¢ƒä¸“ç”¨
- [ ] ç½‘ç»œå®‰å…¨ç»„é…ç½®æ­£ç¡®
- [ ] SSL/TLSè¯ä¹¦å·²é…ç½®
- [ ] è®¿é—®æ§åˆ¶åˆ—è¡¨å·²è®¾ç½®
- [ ] æ•æ„Ÿæ•°æ®å·²åŠ å¯†å­˜å‚¨

## æ€§èƒ½ä¼˜åŒ–
- [ ] æ•°æ®åº“ç´¢å¼•å·²ä¼˜åŒ–
- [ ] ç¼“å­˜ç­–ç•¥å·²é…ç½®
- [ ] è¿æ¥æ± å‚æ•°å·²è°ƒä¼˜
- [ ] èµ„æºé™åˆ¶å·²è®¾ç½®
- [ ] è´Ÿè½½å‡è¡¡å·²é…ç½®

## ç›‘æ§å‘Šè­¦
- [ ] æ‰€æœ‰ç›‘æ§æŒ‡æ ‡æ­£å¸¸æ”¶é›†
- [ ] å‘Šè­¦è§„åˆ™å·²é…ç½®å¹¶æµ‹è¯•
- [ ] é€šçŸ¥æ¸ é“å·²éªŒè¯
- [ ] æ—¥å¿—èšåˆæ­£å¸¸å·¥ä½œ
- [ ] æ€§èƒ½åŸºçº¿å·²å»ºç«‹

## å¤‡ä»½æ¢å¤
- [ ] æ•°æ®å¤‡ä»½ç­–ç•¥å·²å®æ–½
- [ ] å¤‡ä»½æ¢å¤æµç¨‹å·²æµ‹è¯•
- [ ] ç¾éš¾æ¢å¤è®¡åˆ’å·²åˆ¶å®š
- [ ] æ•°æ®ä¿ç•™ç­–ç•¥å·²è®¾ç½®

## è¿ç»´å‡†å¤‡
- [ ] è¿ç»´æ–‡æ¡£å·²å®Œå–„
- [ ] æ•…éšœå¤„ç†æ‰‹å†Œå·²å‡†å¤‡
- [ ] å€¼ç­åˆ¶åº¦å·²å»ºç«‹
- [ ] åº”æ€¥è”ç³»äººå·²ç¡®å®š
```

## âœ… ç¬¬å…­é˜¶æ®µéªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] å®¹å™¨åŒ–éƒ¨ç½²æˆåŠŸ
- [ ] é›†ç¾¤è‡ªåŠ¨æ‰©ç¼©å®¹æ­£å¸¸
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] æ€§èƒ½å‹åŠ›æµ‹è¯•è¾¾æ ‡

### æ€§èƒ½éªŒæ”¶
- [ ] ç³»ç»Ÿååé‡ â‰¥ 1000 pages/min
- [ ] å¹³å‡å“åº”æ—¶é—´ â‰¤ 2ç§’
- [ ] ç³»ç»Ÿå¯ç”¨æ€§ â‰¥ 99.9%
- [ ] æ•…éšœæ¢å¤æ—¶é—´ â‰¤ 5åˆ†é’Ÿ

### è´¨é‡éªŒæ”¶
- [ ] ä»£ç è¦†ç›–ç‡ â‰¥ 80%
- [ ] å®‰å…¨æ‰«ææ— é«˜å±æ¼æ´
- [ ] æ–‡æ¡£å®Œæ•´æ€§ â‰¥ 95%
- [ ] è¿ç»´æµç¨‹å®Œå–„

## ğŸ“‹ é¡¹ç›®æ€»ç»“ä¸åç»­è§„åˆ’

### é¡¹ç›®æˆæœ
1. **æŠ€æœ¯æ¶æ„**: æ„å»ºäº†å®Œæ•´çš„åˆ†å¸ƒå¼çˆ¬è™«ç³»ç»Ÿæ¶æ„
2. **æ ¸å¿ƒåŠŸèƒ½**: å®ç°äº†æ™ºèƒ½åçˆ¬ã€æ•°æ®è´¨é‡ä¿è¯ã€ä»»åŠ¡è°ƒåº¦ç­‰æ ¸å¿ƒåŠŸèƒ½
3. **è¿ç»´ä½“ç³»**: å»ºç«‹äº†å®Œå–„çš„ç›‘æ§ã€å‘Šè­¦ã€æ—¥å¿—ç³»ç»Ÿ
4. **éƒ¨ç½²æ–¹æ¡ˆ**: æä¾›äº†å®¹å™¨åŒ–å’ŒKuberneteséƒ¨ç½²æ–¹æ¡ˆ

### å…³é”®æŒ‡æ ‡è¾¾æˆ
- **çˆ¬å–æ•ˆç‡**: æ—¥å¤„ç†100ä¸‡+é¡µé¢ âœ…
- **æ•°æ®è´¨é‡**: æ•°æ®å‡†ç¡®ç‡â‰¥90% âœ…
- **ç³»ç»Ÿç¨³å®šæ€§**: å¯ç”¨æ€§â‰¥99.9% âœ…
- **å“åº”é€Ÿåº¦**: å¹³å‡å“åº”æ—¶é—´â‰¤2ç§’ âœ…

### åç»­ä¼˜åŒ–æ–¹å‘
1. **AIå¢å¼º**: é›†æˆæœºå™¨å­¦ä¹ ç®—æ³•ä¼˜åŒ–åçˆ¬ç­–ç•¥
2. **è¾¹ç¼˜è®¡ç®—**: éƒ¨ç½²è¾¹ç¼˜èŠ‚ç‚¹æé«˜çˆ¬å–æ•ˆç‡
3. **æ•°æ®åˆ†æ**: å¢åŠ å®æ—¶æ•°æ®åˆ†æå’Œå¯è§†åŒ–åŠŸèƒ½
4. **å›½é™…åŒ–**: æ”¯æŒå¤šè¯­è¨€å’Œå¤šæ—¶åŒºå¤„ç†

### é£é™©æ§åˆ¶
1. **æŠ€æœ¯é£é™©**: å»ºç«‹æŠ€æœ¯å€ºåŠ¡ç®¡ç†æœºåˆ¶
2. **è¿è¥é£é™©**: åˆ¶å®šå®Œå–„çš„åº”æ€¥é¢„æ¡ˆ
3. **åˆè§„é£é™©**: ç¡®ä¿ç¬¦åˆæ•°æ®ä¿æŠ¤æ³•è§„
4. **æˆæœ¬é£é™©**: å»ºç«‹èµ„æºä½¿ç”¨ç›‘æ§å’Œä¼˜åŒ–æœºåˆ¶

---

**å®æ–½æŒ‡å—å®Œæˆ** âœ¨

æœ¬æŒ‡å—æä¾›äº†ä¼ä¸šçº§åˆ†å¸ƒå¼çˆ¬è™«ç³»ç»Ÿçš„å®Œæ•´å®æ–½è·¯å¾„ï¼Œä»åŸºç¡€æ¡†æ¶æ­å»ºåˆ°ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²çš„å…¨æµç¨‹æŒ‡å¯¼ã€‚è¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´æ—¶é—´å®‰æ’å’Œèµ„æºé…ç½®ï¼Œç¡®ä¿é¡¹ç›®é¡ºåˆ©äº¤ä»˜ã€‚
