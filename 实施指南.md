# 企业级分布式爬虫系统实施指南

## 📋 实施概览

本指南基于《系统设计.md》和《开发计划.md》，提供详细的实施步骤、最佳实践和关键决策点，确保项目顺利交付。

## 🎯 实施目标

- **主要目标**: 构建稳定、高效的分布式疫情信息爬虫系统
- **技术目标**: 实现日处理100万+页面，99.9%可用性
- **业务目标**: 覆盖全国主要疫情信息源，数据实时性<30分钟

## 📅 实施时间线

| 阶段 | 周期 | 关键交付物 | 成功标准 |
|------|------|------------|----------|
| 第一阶段 | 1-6周 | 基础框架 | MVP可运行 |
| 第二阶段 | 7-10周 | 反爬机制 | Alpha版本 |
| 第三阶段 | 11-13周 | 数据处理 | 数据质量达标 |
| 第四阶段 | 14-16周 | 任务调度 | 分布式运行 |
| 第五阶段 | 17-20周 | 监控运维 | Beta版本 |
| 第六阶段 | 21-24周 | 部署测试 | 生产就绪 |

## 🚀 第一阶段：基础框架搭建 (1-6周)

### 1.1 环境准备 (第1周)

#### 开发环境标准化
```bash
# Python环境配置
python -m venv crawler_env
source crawler_env/bin/activate  # Linux/Mac
# crawler_env\Scripts\activate  # Windows

# 依赖管理
pip install pipenv
pipenv install scrapy scrapy-redis selenium celery
```

#### 代码质量工具配置
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 22.3.0
    hooks:
      - id: black
  - repo: https://github.com/pycqa/flake8
    rev: 4.0.1
    hooks:
      - id: flake8
```

#### Git工作流规范
- **分支策略**: GitFlow (main/develop/feature/hotfix)
- **提交规范**: Conventional Commits
- **代码审查**: 必须通过2人审查才能合并

### 1.2 项目结构初始化 (第1周)

#### 核心目录结构
```
crawler_system/
├── config/
│   ├── sites/              # 网站配置
│   │   ├── government.yaml # 政府网站配置
│   │   ├── cdc.yaml       # 疾控中心配置
│   │   └── international.yaml
│   ├── proxy.yaml         # 代理配置
│   ├── database.yaml      # 数据库配置
│   └── settings.py        # 全局设置
├── crawler/
│   ├── spiders/           # 爬虫实现
│   │   ├── government/    # 政府网站爬虫
│   │   ├── cdc/          # 疾控中心爬虫
│   │   └── base.py       # 基础爬虫类
│   ├── middlewares/       # 中间件
│   │   ├── proxy.py      # 代理中间件
│   │   ├── useragent.py  # UA中间件
│   │   └── retry.py      # 重试中间件
│   ├── pipelines/         # 数据管道
│   │   ├── validation.py # 数据验证
│   │   ├── cleaning.py   # 数据清洗
│   │   └── storage.py    # 数据存储
│   └── items.py          # 数据模型
├── scheduler/             # 任务调度
│   ├── task_manager.py   # 任务管理器
│   ├── priority.py       # 优先级管理
│   └── load_balancer.py  # 负载均衡
├── proxy_pool/            # 代理池
│   ├── provider.py       # 代理提供商
│   ├── validator.py      # 代理验证
│   └── manager.py        # 代理管理
├── anti_crawl/            # 反爬模块
│   ├── captcha.py        # 验证码处理
│   ├── frequency.py      # 频率控制
│   └── browser.py        # 浏览器管理
├── data_processing/       # 数据处理
│   ├── extractor.py      # 数据提取
│   ├── cleaner.py        # 数据清洗
│   └── validator.py      # 数据验证
├── monitoring/            # 监控模块
│   ├── metrics.py        # 指标收集
│   ├── alerts.py         # 告警处理
│   └── dashboard.py      # 仪表板
├── deployment/            # 部署配置
│   ├── docker/           # Docker配置
│   ├── k8s/             # Kubernetes配置
│   └── scripts/         # 部署脚本
├── tests/                # 测试代码
│   ├── unit/            # 单元测试
│   ├── integration/     # 集成测试
│   └── e2e/            # 端到端测试
└── docs/                # 文档
    ├── api/             # API文档
    ├── deployment/      # 部署文档
    └── user_guide/      # 用户指南
```

### 1.3 Scrapy-Redis集成 (第2周)

#### 核心配置
```python
# settings.py
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 300,
}

# Redis配置
REDIS_URL = 'redis://localhost:6379/0'
REDIS_PARAMS = {
    'socket_connect_timeout': 30,
    'socket_timeout': 30,
    'retry_on_timeout': True,
    'health_check_interval': 30,
}
```

#### 分布式队列设计
```python
# scheduler/task_manager.py
class TaskManager:
    def __init__(self):
        self.redis_client = redis.Redis.from_url(settings.REDIS_URL)
        self.task_queue = 'crawler:tasks'
        self.result_queue = 'crawler:results'

    def add_task(self, spider_name, url, priority=0):
        task = {
            'spider': spider_name,
            'url': url,
            'priority': priority,
            'timestamp': time.time()
        }
        self.redis_client.zadd(self.task_queue, {json.dumps(task): priority})
```

### 1.4 基础代理池实现 (第3周)

#### 代理管理器
```python
# proxy_pool/manager.py
class ProxyManager:
    def __init__(self):
        self.redis_client = redis.Redis.from_url(settings.REDIS_URL)
        self.proxy_key = 'proxy:pool'
        self.failed_key = 'proxy:failed'

    def get_proxy(self):
        """获取可用代理"""
        proxy = self.redis_client.spop(self.proxy_key)
        if proxy:
            return json.loads(proxy)
        return None

    def validate_proxy(self, proxy):
        """验证代理可用性"""
        try:
            response = requests.get(
                'http://httpbin.org/ip',
                proxies={'http': proxy, 'https': proxy},
                timeout=10
            )
            return response.status_code == 200
        except:
            return False
```

### 1.5 数据模型设计 (第4周)

#### Scrapy Items定义
```python
# crawler/items.py
import scrapy
from itemloaders.processors import TakeFirst, MapCompose

class EpidemicDataItem(scrapy.Item):
    # 基础信息
    source_url = scrapy.Field()
    source_name = scrapy.Field()
    crawl_time = scrapy.Field()

    # 疫情数据
    region = scrapy.Field()
    confirmed_cases = scrapy.Field(
        input_processor=MapCompose(str.strip, int),
        output_processor=TakeFirst()
    )
    death_cases = scrapy.Field(
        input_processor=MapCompose(str.strip, int),
        output_processor=TakeFirst()
    )
    recovered_cases = scrapy.Field(
        input_processor=MapCompose(str.strip, int),
        output_processor=TakeFirst()
    )

    # 时间信息
    report_date = scrapy.Field()
    update_time = scrapy.Field()

    # 数据质量
    data_quality_score = scrapy.Field()
    validation_status = scrapy.Field()
```

#### 数据库表结构
```sql
-- PostgreSQL表结构
CREATE TABLE epidemic_data (
    id SERIAL PRIMARY KEY,
    source_url VARCHAR(500) NOT NULL,
    source_name VARCHAR(100) NOT NULL,
    crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    region VARCHAR(100) NOT NULL,
    confirmed_cases INTEGER DEFAULT 0,
    death_cases INTEGER DEFAULT 0,
    recovered_cases INTEGER DEFAULT 0,

    report_date DATE NOT NULL,
    update_time TIMESTAMP,

    data_quality_score DECIMAL(3,2) DEFAULT 0.00,
    validation_status VARCHAR(20) DEFAULT 'pending',

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 索引优化
CREATE INDEX idx_epidemic_region_date ON epidemic_data(region, report_date);
CREATE INDEX idx_epidemic_source ON epidemic_data(source_name);
CREATE INDEX idx_epidemic_crawl_time ON epidemic_data(crawl_time);
```

### 1.6 基础爬虫开发 (第5周)

#### 基础爬虫类
```python
# crawler/spiders/base.py
import scrapy
from scrapy_redis.spiders import RedisSpider

class BaseEpidemicSpider(RedisSpider):
    """基础疫情爬虫类"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.site_config = self.load_site_config()

    def load_site_config(self):
        """加载网站配置"""
        config_path = f"config/sites/{self.name}.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def parse(self, response):
        """解析响应"""
        # 数据提取逻辑
        for item in self.extract_data(response):
            yield item

        # 翻页处理
        next_page = self.get_next_page(response)
        if next_page:
            yield response.follow(next_page, self.parse)

    def extract_data(self, response):
        """提取数据 - 子类实现"""
        raise NotImplementedError

    def get_next_page(self, response):
        """获取下一页 - 子类实现"""
        return None
```

### 1.7 存储系统搭建 (第6周)

#### Docker Compose配置
```yaml
# deployment/docker/docker-compose.yml
version: '3.8'
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  mongodb:
    image: mongo:5
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password

  postgresql:
    image: postgres:14
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: crawler_db
      POSTGRES_USER: crawler_user
      POSTGRES_PASSWORD: crawler_pass

volumes:
  redis_data:
  mongo_data:
  postgres_data:
```

## ✅ 第一阶段验收标准

### 功能验收
- [ ] 分布式爬虫框架可正常运行
- [ ] 代理池基础功能正常
- [ ] 数据可正确存储到数据库
- [ ] 基础爬虫可抓取目标网站

### 性能验收
- [ ] 单机并发处理能力 ≥ 100 requests/min
- [ ] 代理池响应时间 < 100ms
- [ ] 数据存储延迟 < 500ms

### 质量验收
- [ ] 单元测试覆盖率 ≥ 70%
- [ ] 代码质量检查通过
- [ ] 文档完整性 ≥ 80%

## 📊 关键指标监控

### 业务指标
- 爬取成功率 ≥ 95%
- 数据质量分数 ≥ 0.8
- 数据更新及时性 < 30分钟

### 技术指标
- 系统可用性 ≥ 99.9%
- 平均响应时间 < 2秒
- 错误率 < 1%

## 🔄 持续改进

### 每周回顾
- 技术债务评估
- 性能瓶颈分析
- 代码质量检查

### 月度评估
- 架构优化建议
- 技术栈升级计划
- 团队技能提升

## 🛡️ 第二阶段：反爬机制应对 (7-10周)

### 2.1 Selenium Grid集成 (第7周)

#### Selenium Hub配置
```yaml
# deployment/selenium/docker-compose.yml
version: '3.8'
services:
  selenium-hub:
    image: selenium/hub:4.0.0
    ports:
      - "4444:4444"
    environment:
      GRID_MAX_SESSION: 16
      GRID_BROWSER_TIMEOUT: 300
      GRID_TIMEOUT: 300

  firefox-node:
    image: selenium/node-firefox:4.0.0
    scale: 3
    depends_on:
      - selenium-hub
    environment:
      HUB_HOST: selenium-hub
      NODE_MAX_INSTANCES: 2
      NODE_MAX_SESSION: 2

  chrome-node:
    image: selenium/node-chrome:4.0.0
    scale: 2
    depends_on:
      - selenium-hub
    environment:
      HUB_HOST: selenium-hub
      NODE_MAX_INSTANCES: 2
      NODE_MAX_SESSION: 2
```

#### 浏览器池管理
```python
# anti_crawl/browser.py
from selenium import webdriver
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

class BrowserPool:
    def __init__(self, hub_url="http://localhost:4444/wd/hub"):
        self.hub_url = hub_url
        self.active_sessions = {}
        self.max_sessions = 10

    def get_browser(self, browser_type="firefox"):
        """获取浏览器实例"""
        if browser_type == "firefox":
            caps = DesiredCapabilities.FIREFOX.copy()
            caps['marionette'] = True
        else:
            caps = DesiredCapabilities.CHROME.copy()

        # 反检测配置
        caps['acceptSslCerts'] = True
        caps['acceptInsecureCerts'] = True

        driver = webdriver.Remote(
            command_executor=self.hub_url,
            desired_capabilities=caps
        )

        # 反检测脚本
        driver.execute_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
        """)

        return driver

    def release_browser(self, driver):
        """释放浏览器实例"""
        try:
            driver.quit()
        except:
            pass
```

### 2.2 高级代理策略 (第8周)

#### 代理质量评分系统
```python
# proxy_pool/validator.py
import time
import requests
from dataclasses import dataclass

@dataclass
class ProxyMetrics:
    success_rate: float = 0.0
    avg_response_time: float = 0.0
    last_success_time: float = 0.0
    total_requests: int = 0
    failed_requests: int = 0
    geographic_location: str = ""
    anonymity_level: str = ""  # transparent, anonymous, elite

class ProxyValidator:
    def __init__(self):
        self.test_urls = [
            'http://httpbin.org/ip',
            'http://httpbin.org/headers',
            'https://www.google.com'
        ]

    def validate_proxy(self, proxy_url):
        """全面验证代理"""
        metrics = ProxyMetrics()

        for test_url in self.test_urls:
            start_time = time.time()
            try:
                response = requests.get(
                    test_url,
                    proxies={'http': proxy_url, 'https': proxy_url},
                    timeout=10,
                    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                )

                if response.status_code == 200:
                    metrics.total_requests += 1
                    metrics.avg_response_time += time.time() - start_time
                    metrics.last_success_time = time.time()

                    # 检测匿名性
                    if 'httpbin.org/headers' in test_url:
                        headers = response.json().get('headers', {})
                        if 'X-Forwarded-For' not in headers:
                            metrics.anonymity_level = 'elite'
                        elif proxy_url.split('://')[1].split(':')[0] not in headers.get('X-Forwarded-For', ''):
                            metrics.anonymity_level = 'anonymous'
                        else:
                            metrics.anonymity_level = 'transparent'
                else:
                    metrics.failed_requests += 1

            except Exception as e:
                metrics.failed_requests += 1

        # 计算成功率
        total = metrics.total_requests + metrics.failed_requests
        if total > 0:
            metrics.success_rate = metrics.total_requests / total
            metrics.avg_response_time = metrics.avg_response_time / metrics.total_requests if metrics.total_requests > 0 else float('inf')

        return metrics

    def calculate_score(self, metrics):
        """计算代理质量分数 (0-100)"""
        score = 0

        # 成功率权重 40%
        score += metrics.success_rate * 40

        # 响应时间权重 30% (越快越好)
        if metrics.avg_response_time < 1:
            score += 30
        elif metrics.avg_response_time < 3:
            score += 20
        elif metrics.avg_response_time < 5:
            score += 10

        # 匿名性权重 20%
        if metrics.anonymity_level == 'elite':
            score += 20
        elif metrics.anonymity_level == 'anonymous':
            score += 15
        elif metrics.anonymity_level == 'transparent':
            score += 5

        # 稳定性权重 10%
        if time.time() - metrics.last_success_time < 3600:  # 1小时内有成功
            score += 10

        return min(100, max(0, score))
```

### 2.3 智能User-Agent管理 (第8周)

#### UA管理器
```python
# anti_crawl/useragent.py
import random
import json
from pathlib import Path

class UserAgentManager:
    def __init__(self):
        self.ua_database = self.load_ua_database()
        self.browser_stats = {
            'chrome': 0.65,
            'firefox': 0.20,
            'safari': 0.10,
            'edge': 0.05
        }

    def load_ua_database(self):
        """加载UA数据库"""
        ua_file = Path(__file__).parent / 'data' / 'user_agents.json'
        with open(ua_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def get_random_ua(self, browser=None, os=None):
        """获取随机UA"""
        if not browser:
            browser = self.select_browser_by_stats()

        candidates = [
            ua for ua in self.ua_database
            if (not browser or ua['browser'].lower() == browser.lower()) and
               (not os or ua['os'].lower() == os.lower())
        ]

        if candidates:
            return random.choice(candidates)['user_agent']

        # fallback
        return "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

    def select_browser_by_stats(self):
        """根据统计数据选择浏览器"""
        rand = random.random()
        cumulative = 0

        for browser, probability in self.browser_stats.items():
            cumulative += probability
            if rand <= cumulative:
                return browser

        return 'chrome'

    def generate_headers(self, ua=None):
        """生成完整请求头"""
        if not ua:
            ua = self.get_random_ua()

        headers = {
            'User-Agent': ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

        # 根据浏览器类型调整头部
        if 'Chrome' in ua:
            headers['sec-ch-ua'] = '"Google Chrome";v="91", "Chromium";v="91", ";Not A Brand";v="99"'
            headers['sec-ch-ua-mobile'] = '?0'

        return headers
```

### 2.4 验证码处理系统 (第9周)

#### 验证码识别服务
```python
# anti_crawl/captcha.py
import base64
import requests
from PIL import Image
import pytesseract
from io import BytesIO

class CaptchaHandler:
    def __init__(self):
        self.ocr_services = {
            'tesseract': self.solve_with_tesseract,
            'api_service': self.solve_with_api,
            'manual': self.solve_manually
        }
        self.success_rates = {
            'tesseract': 0.3,
            'api_service': 0.8,
            'manual': 0.95
        }

    def solve_captcha(self, image_data, captcha_type='text'):
        """解决验证码"""
        # 预处理图像
        processed_image = self.preprocess_image(image_data)

        # 按成功率排序尝试不同方法
        for service in sorted(self.ocr_services.keys(),
                            key=lambda x: self.success_rates[x],
                            reverse=True):
            try:
                result = self.ocr_services[service](processed_image, captcha_type)
                if result and self.validate_result(result, captcha_type):
                    return result
            except Exception as e:
                self.logger.warning(f"验证码识别失败 {service}: {e}")
                continue

        return None

    def preprocess_image(self, image_data):
        """图像预处理"""
        if isinstance(image_data, str):
            # base64解码
            image_data = base64.b64decode(image_data)

        image = Image.open(BytesIO(image_data))

        # 转换为灰度
        image = image.convert('L')

        # 二值化
        threshold = 128
        image = image.point(lambda x: 0 if x < threshold else 255, '1')

        # 去噪
        image = image.filter(ImageFilter.MedianFilter())

        return image

    def solve_with_tesseract(self, image, captcha_type):
        """使用Tesseract OCR"""
        config = '--oem 3 --psm 8 -c tessedit_char_whitelist=0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'

        if captcha_type == 'number':
            config = '--oem 3 --psm 8 -c tessedit_char_whitelist=0123456789'

        text = pytesseract.image_to_string(image, config=config)
        return text.strip()

    def solve_with_api(self, image, captcha_type):
        """使用第三方API服务"""
        # 这里可以集成打码平台API
        # 如：超级鹰、若快打码等
        api_url = "http://api.captcha-service.com/solve"

        # 将图像转换为base64
        buffer = BytesIO()
        image.save(buffer, format='PNG')
        image_b64 = base64.b64encode(buffer.getvalue()).decode()

        response = requests.post(api_url, json={
            'image': image_b64,
            'type': captcha_type
        })

        if response.status_code == 200:
            return response.json().get('result')

        return None

    def solve_manually(self, image, captcha_type):
        """人工验证码处理"""
        # 保存图像到临时文件
        temp_path = f"/tmp/captcha_{int(time.time())}.png"
        image.save(temp_path)

        # 发送到人工处理队列
        task_id = self.submit_manual_task(temp_path, captcha_type)

        # 等待人工处理结果
        return self.wait_for_manual_result(task_id, timeout=300)

    def validate_result(self, result, captcha_type):
        """验证识别结果"""
        if not result:
            return False

        if captcha_type == 'number':
            return result.isdigit() and len(result) >= 4
        elif captcha_type == 'text':
            return len(result) >= 4 and result.isalnum()

        return True
```

### 2.5 智能频率控制 (第10周)

#### 自适应延迟控制
```python
# anti_crawl/frequency.py
import time
import random
from collections import defaultdict, deque
from threading import Lock

class AdaptiveRateController:
    def __init__(self):
        self.site_stats = defaultdict(lambda: {
            'success_count': 0,
            'error_count': 0,
            'last_request_time': 0,
            'current_delay': 1.0,
            'min_delay': 0.5,
            'max_delay': 30.0,
            'recent_responses': deque(maxlen=100)
        })
        self.lock = Lock()

    def get_delay(self, domain):
        """获取当前域名的延迟时间"""
        with self.lock:
            stats = self.site_stats[domain]

            # 基于最近响应调整延迟
            if len(stats['recent_responses']) >= 10:
                recent_errors = sum(1 for r in stats['recent_responses'] if r['status'] != 'success')
                error_rate = recent_errors / len(stats['recent_responses'])

                if error_rate > 0.3:  # 错误率过高，增加延迟
                    stats['current_delay'] = min(stats['current_delay'] * 1.5, stats['max_delay'])
                elif error_rate < 0.1:  # 错误率低，减少延迟
                    stats['current_delay'] = max(stats['current_delay'] * 0.8, stats['min_delay'])

            # 添加随机性避免检测
            base_delay = stats['current_delay']
            random_factor = random.uniform(0.5, 1.5)

            return base_delay * random_factor

    def record_response(self, domain, status, response_time=None):
        """记录响应结果"""
        with self.lock:
            stats = self.site_stats[domain]

            response_record = {
                'status': status,
                'timestamp': time.time(),
                'response_time': response_time
            }

            stats['recent_responses'].append(response_record)

            if status == 'success':
                stats['success_count'] += 1
            else:
                stats['error_count'] += 1

            stats['last_request_time'] = time.time()

    def should_pause(self, domain):
        """判断是否需要暂停爬取"""
        stats = self.site_stats[domain]

        # 检查最近错误率
        if len(stats['recent_responses']) >= 20:
            recent_errors = sum(1 for r in stats['recent_responses'][-20:] if r['status'] != 'success')
            if recent_errors >= 15:  # 最近20次请求中有15次失败
                return True

        return False

    def get_pause_duration(self, domain):
        """获取暂停时长"""
        stats = self.site_stats[domain]
        error_rate = stats['error_count'] / max(stats['success_count'] + stats['error_count'], 1)

        if error_rate > 0.8:
            return 3600  # 1小时
        elif error_rate > 0.5:
            return 1800  # 30分钟
        else:
            return 600   # 10分钟
```

## ✅ 第二阶段验收标准

### 功能验收
- [ ] Selenium Grid集群正常运行
- [ ] 代理质量评分系统工作正常
- [ ] 验证码识别成功率 ≥ 70%
- [ ] 智能频率控制有效降低封禁率

### 性能验收
- [ ] 浏览器池响应时间 < 5秒
- [ ] 代理切换延迟 < 200ms
- [ ] 验证码识别时间 < 30秒

### 质量验收
- [ ] 反爬成功率 ≥ 85%
- [ ] 系统稳定性 ≥ 99%
- [ ] 错误恢复时间 < 5分钟

## 📊 第三阶段：数据处理与质量保证 (11-13周)

### 3.1 数据提取引擎 (第11周)

#### 配置化提取规则
```python
# data_processing/extractor.py
import yaml
import re
from lxml import etree
from bs4 import BeautifulSoup

class ConfigurableExtractor:
    def __init__(self, config_path):
        self.config = self.load_config(config_path)
        self.extractors = {
            'xpath': self.extract_by_xpath,
            'css': self.extract_by_css,
            'regex': self.extract_by_regex,
            'json': self.extract_by_json
        }

    def load_config(self, config_path):
        """加载提取配置"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def extract_data(self, response, site_name):
        """根据配置提取数据"""
        site_config = self.config.get(site_name, {})
        extracted_data = {}

        for field_name, field_config in site_config.get('fields', {}).items():
            try:
                value = self.extract_field(response, field_config)
                extracted_data[field_name] = self.clean_value(value, field_config)
            except Exception as e:
                self.logger.error(f"提取字段 {field_name} 失败: {e}")
                extracted_data[field_name] = None

        return extracted_data

    def extract_field(self, response, field_config):
        """提取单个字段"""
        method = field_config.get('method', 'xpath')
        selector = field_config.get('selector')

        if method in self.extractors:
            return self.extractors[method](response, selector, field_config)
        else:
            raise ValueError(f"不支持的提取方法: {method}")

    def extract_by_xpath(self, response, selector, config):
        """XPath提取"""
        tree = etree.HTML(response.text)
        elements = tree.xpath(selector)

        if config.get('multiple', False):
            return [elem.text.strip() if hasattr(elem, 'text') else str(elem).strip()
                   for elem in elements]
        else:
            return elements[0].text.strip() if elements and hasattr(elements[0], 'text') else str(elements[0]).strip() if elements else None

    def extract_by_css(self, response, selector, config):
        """CSS选择器提取"""
        soup = BeautifulSoup(response.text, 'html.parser')
        elements = soup.select(selector)

        if config.get('multiple', False):
            return [elem.get_text().strip() for elem in elements]
        else:
            return elements[0].get_text().strip() if elements else None

    def extract_by_regex(self, response, pattern, config):
        """正则表达式提取"""
        matches = re.findall(pattern, response.text, re.DOTALL)

        if config.get('multiple', False):
            return matches
        else:
            return matches[0] if matches else None

    def clean_value(self, value, config):
        """清洗数据值"""
        if value is None:
            return None

        # 数据类型转换
        data_type = config.get('type', 'string')

        if data_type == 'integer':
            # 提取数字
            numbers = re.findall(r'\d+', str(value))
            return int(''.join(numbers)) if numbers else 0
        elif data_type == 'float':
            numbers = re.findall(r'\d+\.?\d*', str(value))
            return float(''.join(numbers)) if numbers else 0.0
        elif data_type == 'date':
            # 日期解析
            return self.parse_date(value)
        else:
            # 字符串清理
            return re.sub(r'\s+', ' ', str(value)).strip()

    def parse_date(self, date_str):
        """解析日期字符串"""
        import dateutil.parser
        try:
            return dateutil.parser.parse(date_str).strftime('%Y-%m-%d')
        except:
            return None
```

#### 网站配置示例
```yaml
# config/sites/nhc.gov.cn.yaml
site_name: "国家卫健委"
base_url: "http://www.nhc.gov.cn"
encoding: "utf-8"

fields:
  title:
    method: xpath
    selector: "//h1[@class='article-title']/text()"
    type: string

  content:
    method: xpath
    selector: "//div[@class='article-content']//text()"
    type: string
    multiple: true

  publish_date:
    method: regex
    selector: "发布时间：(\d{4}-\d{2}-\d{2})"
    type: date

  confirmed_cases:
    method: regex
    selector: "确诊病例(\d+)例"
    type: integer

  death_cases:
    method: regex
    selector: "死亡病例(\d+)例"
    type: integer

pagination:
  next_page_xpath: "//a[contains(text(), '下一页')]/@href"
  max_pages: 100

rate_limit:
  delay: 2.0
  random_delay: true
```

### 3.2 数据清洗模块 (第12周)

#### 数据清洗流水线
```python
# data_processing/cleaner.py
import re
import jieba
from datetime import datetime
from typing import Dict, Any, List

class DataCleaner:
    def __init__(self):
        self.cleaning_rules = {
            'text': self.clean_text,
            'number': self.clean_number,
            'date': self.clean_date,
            'region': self.clean_region
        }

        # 加载地区映射表
        self.region_mapping = self.load_region_mapping()

        # 停用词
        self.stopwords = self.load_stopwords()

    def clean_data(self, data: Dict[str, Any], field_types: Dict[str, str]) -> Dict[str, Any]:
        """清洗数据"""
        cleaned_data = {}

        for field, value in data.items():
            field_type = field_types.get(field, 'text')

            if field_type in self.cleaning_rules:
                cleaned_data[field] = self.cleaning_rules[field_type](value)
            else:
                cleaned_data[field] = value

        return cleaned_data

    def clean_text(self, text: str) -> str:
        """清洗文本数据"""
        if not text:
            return ""

        # 去除HTML标签
        text = re.sub(r'<[^>]+>', '', text)

        # 去除特殊字符
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)

        # 标准化空白字符
        text = re.sub(r'\s+', ' ', text).strip()

        # 去除停用词
        words = jieba.cut(text)
        filtered_words = [word for word in words if word not in self.stopwords]

        return ' '.join(filtered_words)

    def clean_number(self, value: Any) -> int:
        """清洗数字数据"""
        if isinstance(value, (int, float)):
            return int(value)

        if isinstance(value, str):
            # 提取数字
            numbers = re.findall(r'\d+', value)
            if numbers:
                return int(''.join(numbers))

        return 0

    def clean_date(self, date_value: Any) -> str:
        """清洗日期数据"""
        if not date_value:
            return None

        # 常见日期格式
        date_patterns = [
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{4})年(\d{1,2})月(\d{1,2})日',
            r'(\d{1,2})/(\d{1,2})/(\d{4})',
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})'
        ]

        date_str = str(date_value)

        for pattern in date_patterns:
            match = re.search(pattern, date_str)
            if match:
                try:
                    if len(match.groups()) == 3:
                        year, month, day = match.groups()
                        # 处理年份在后面的情况
                        if len(year) == 2:
                            year, month, day = day, year, month

                        date_obj = datetime(int(year), int(month), int(day))
                        return date_obj.strftime('%Y-%m-%d')
                except ValueError:
                    continue

        return None

    def clean_region(self, region: str) -> str:
        """清洗地区数据"""
        if not region:
            return ""

        # 标准化地区名称
        region = region.strip()

        # 使用映射表标准化
        for standard_name, aliases in self.region_mapping.items():
            if region in aliases:
                return standard_name

        # 去除常见后缀
        suffixes = ['省', '市', '区', '县', '自治区', '特别行政区']
        for suffix in suffixes:
            if region.endswith(suffix):
                base_name = region[:-len(suffix)]
                if base_name:
                    return base_name

        return region

    def load_region_mapping(self) -> Dict[str, List[str]]:
        """加载地区映射表"""
        return {
            '北京': ['北京市', '北京', 'Beijing'],
            '上海': ['上海市', '上海', 'Shanghai'],
            '广东': ['广东省', '广东', 'Guangdong'],
            # ... 更多映射
        }

    def load_stopwords(self) -> set:
        """加载停用词"""
        return {'的', '了', '在', '是', '我', '有', '和', '就', '不', '人', '都', '一', '一个', '上', '也', '很', '到', '说', '要', '去', '你', '会', '着', '没有', '看', '好', '自己', '这'}
```

### 3.3 数据验证系统 (第12周)

#### 数据质量评分
```python
# data_processing/validator.py
from typing import Dict, Any, List, Tuple
import re
from datetime import datetime, timedelta

class DataValidator:
    def __init__(self):
        self.validation_rules = {
            'completeness': self.check_completeness,
            'accuracy': self.check_accuracy,
            'consistency': self.check_consistency,
            'timeliness': self.check_timeliness
        }

        self.field_weights = {
            'confirmed_cases': 0.3,
            'death_cases': 0.2,
            'recovered_cases': 0.2,
            'region': 0.1,
            'report_date': 0.1,
            'source_url': 0.1
        }

    def validate_data(self, data: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """验证数据并返回质量分数"""
        validation_results = {}
        total_score = 0.0

        for rule_name, rule_func in self.validation_rules.items():
            score, details = rule_func(data)
            validation_results[rule_name] = {
                'score': score,
                'details': details
            }
            total_score += score

        # 计算加权平均分
        quality_score = total_score / len(self.validation_rules)

        return quality_score, validation_results

    def check_completeness(self, data: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """检查数据完整性"""
        required_fields = ['confirmed_cases', 'region', 'report_date', 'source_url']
        missing_fields = []
        empty_fields = []

        for field in required_fields:
            if field not in data:
                missing_fields.append(field)
            elif data[field] is None or data[field] == '':
                empty_fields.append(field)

        # 计算完整性分数
        total_fields = len(required_fields)
        missing_count = len(missing_fields) + len(empty_fields)
        completeness_score = max(0, (total_fields - missing_count) / total_fields)

        details = {
            'missing_fields': missing_fields,
            'empty_fields': empty_fields,
            'completeness_rate': completeness_score
        }

        return completeness_score, details

    def check_accuracy(self, data: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """检查数据准确性"""
        accuracy_issues = []
        accuracy_score = 1.0

        # 检查数值合理性
        if 'confirmed_cases' in data:
            confirmed = data['confirmed_cases']
            if isinstance(confirmed, int) and confirmed < 0:
                accuracy_issues.append('确诊病例数不能为负数')
                accuracy_score -= 0.3
            elif isinstance(confirmed, int) and confirmed > 1000000:
                accuracy_issues.append('确诊病例数异常过大')
                accuracy_score -= 0.2

        # 检查死亡病例与确诊病例的关系
        if 'death_cases' in data and 'confirmed_cases' in data:
            death = data['death_cases']
            confirmed = data['confirmed_cases']
            if isinstance(death, int) and isinstance(confirmed, int):
                if death > confirmed:
                    accuracy_issues.append('死亡病例数不能大于确诊病例数')
                    accuracy_score -= 0.4

        # 检查日期合理性
        if 'report_date' in data:
            report_date = data['report_date']
            if report_date:
                try:
                    date_obj = datetime.strptime(report_date, '%Y-%m-%d')
                    if date_obj > datetime.now():
                        accuracy_issues.append('报告日期不能是未来时间')
                        accuracy_score -= 0.2
                    elif date_obj < datetime(2019, 12, 1):
                        accuracy_issues.append('报告日期过早，可能不准确')
                        accuracy_score -= 0.1
                except ValueError:
                    accuracy_issues.append('日期格式不正确')
                    accuracy_score -= 0.3

        details = {
            'accuracy_issues': accuracy_issues,
            'accuracy_score': max(0, accuracy_score)
        }

        return max(0, accuracy_score), details

    def check_consistency(self, data: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """检查数据一致性"""
        consistency_issues = []
        consistency_score = 1.0

        # 检查地区名称一致性
        if 'region' in data:
            region = data['region']
            if region and not self.is_valid_region(region):
                consistency_issues.append(f'地区名称可能不标准: {region}')
                consistency_score -= 0.2

        # 检查数据源一致性
        if 'source_url' in data:
            source_url = data['source_url']
            if source_url and not self.is_valid_url(source_url):
                consistency_issues.append('数据源URL格式不正确')
                consistency_score -= 0.1

        details = {
            'consistency_issues': consistency_issues,
            'consistency_score': max(0, consistency_score)
        }

        return max(0, consistency_score), details

    def check_timeliness(self, data: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """检查数据时效性"""
        timeliness_issues = []
        timeliness_score = 1.0

        if 'report_date' in data and 'crawl_time' in data:
            report_date = data['report_date']
            crawl_time = data['crawl_time']

            if report_date and crawl_time:
                try:
                    report_dt = datetime.strptime(report_date, '%Y-%m-%d')
                    crawl_dt = datetime.fromisoformat(crawl_time.replace('Z', '+00:00'))

                    time_diff = crawl_dt - report_dt

                    if time_diff.days > 7:
                        timeliness_issues.append('数据过于陈旧')
                        timeliness_score -= 0.5
                    elif time_diff.days > 3:
                        timeliness_issues.append('数据时效性一般')
                        timeliness_score -= 0.2

                except (ValueError, AttributeError):
                    timeliness_issues.append('时间格式解析失败')
                    timeliness_score -= 0.3

        details = {
            'timeliness_issues': timeliness_issues,
            'timeliness_score': max(0, timeliness_score)
        }

        return max(0, timeliness_score), details

    def is_valid_region(self, region: str) -> bool:
        """验证地区名称是否有效"""
        valid_regions = {
            '北京', '上海', '天津', '重庆',
            '河北', '山西', '辽宁', '吉林', '黑龙江',
            '江苏', '浙江', '安徽', '福建', '江西', '山东',
            '河南', '湖北', '湖南', '广东', '海南',
            '四川', '贵州', '云南', '陕西', '甘肃', '青海',
            '内蒙古', '广西', '西藏', '宁夏', '新疆',
            '香港', '澳门', '台湾'
        }

        # 检查是否为标准地区名或包含标准地区名
        for valid_region in valid_regions:
            if valid_region in region:
                return True

        return False

    def is_valid_url(self, url: str) -> bool:
        """验证URL格式"""
        url_pattern = re.compile(
            r'^https?://'  # http:// or https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
            r'localhost|'  # localhost...
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)

        return url_pattern.match(url) is not None
```

## ✅ 第三阶段验收标准

### 功能验收
- [ ] 配置化数据提取正常工作
- [ ] 数据清洗流水线处理准确
- [ ] 数据质量评分系统有效
- [ ] 异常数据自动标记和处理

### 性能验收
- [ ] 数据提取速度 ≥ 1000条/分钟
- [ ] 数据清洗延迟 < 100ms/条
- [ ] 数据验证准确率 ≥ 95%

### 质量验收
- [ ] 数据完整性 ≥ 95%
- [ ] 数据准确性 ≥ 90%
- [ ] 数据一致性 ≥ 95%

## ⚙️ 第四阶段：任务调度与分发 (14-16周)

### 4.1 分布式任务调度器 (第14周)

#### 任务调度核心
```python
# scheduler/task_scheduler.py
import json
import time
import hashlib
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from enum import Enum

class TaskPriority(Enum):
    LOW = 1
    NORMAL = 2
    HIGH = 3
    URGENT = 4

@dataclass
class CrawlTask:
    task_id: str
    spider_name: str
    url: str
    priority: TaskPriority
    site_config: Dict
    retry_count: int = 0
    max_retries: int = 3
    created_at: float = None
    scheduled_at: float = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()
        if self.task_id is None:
            self.task_id = self.generate_task_id()

    def generate_task_id(self) -> str:
        """生成任务ID"""
        content = f"{self.spider_name}:{self.url}:{self.created_at}"
        return hashlib.md5(content.encode()).hexdigest()

class DistributedTaskScheduler:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.task_queue_key = "crawler:task_queue"
        self.processing_key = "crawler:processing"
        self.completed_key = "crawler:completed"
        self.failed_key = "crawler:failed"

        # 负载均衡策略
        self.load_balancer = LoadBalancer(redis_client)

        # 任务监控
        self.monitor = TaskMonitor(redis_client)

    def submit_task(self, task: CrawlTask) -> bool:
        """提交任务到队列"""
        try:
            # 检查任务是否已存在
            if self.is_task_exists(task.task_id):
                return False

            # 根据优先级分配到不同队列
            queue_key = f"{self.task_queue_key}:{task.priority.name.lower()}"

            # 序列化任务
            task_data = json.dumps(asdict(task))

            # 添加到Redis队列
            self.redis.lpush(queue_key, task_data)

            # 记录任务提交
            self.monitor.record_task_submitted(task)

            return True

        except Exception as e:
            self.logger.error(f"提交任务失败: {e}")
            return False

    def get_next_task(self, worker_id: str) -> Optional[CrawlTask]:
        """获取下一个任务"""
        # 按优先级顺序检查队列
        priority_queues = [
            f"{self.task_queue_key}:urgent",
            f"{self.task_queue_key}:high",
            f"{self.task_queue_key}:normal",
            f"{self.task_queue_key}:low"
        ]

        for queue_key in priority_queues:
            task_data = self.redis.brpop(queue_key, timeout=1)
            if task_data:
                try:
                    task_dict = json.loads(task_data[1])
                    task = CrawlTask(**task_dict)

                    # 标记任务为处理中
                    self.mark_task_processing(task, worker_id)

                    return task

                except Exception as e:
                    self.logger.error(f"解析任务失败: {e}")
                    continue

        return None

    def mark_task_processing(self, task: CrawlTask, worker_id: str):
        """标记任务为处理中"""
        processing_data = {
            'task': asdict(task),
            'worker_id': worker_id,
            'start_time': time.time()
        }

        self.redis.hset(
            self.processing_key,
            task.task_id,
            json.dumps(processing_data)
        )

    def complete_task(self, task_id: str, result: Dict):
        """完成任务"""
        # 从处理中移除
        self.redis.hdel(self.processing_key, task_id)

        # 添加到完成队列
        completion_data = {
            'task_id': task_id,
            'result': result,
            'completed_at': time.time()
        }

        self.redis.hset(
            self.completed_key,
            task_id,
            json.dumps(completion_data)
        )

        # 记录完成统计
        self.monitor.record_task_completed(task_id, result)

    def fail_task(self, task_id: str, error: str, retry: bool = True):
        """任务失败处理"""
        processing_data = self.redis.hget(self.processing_key, task_id)
        if not processing_data:
            return

        processing_info = json.loads(processing_data)
        task_dict = processing_info['task']
        task = CrawlTask(**task_dict)

        # 检查是否需要重试
        if retry and task.retry_count < task.max_retries:
            task.retry_count += 1
            task.scheduled_at = time.time() + (2 ** task.retry_count) * 60  # 指数退避

            # 重新提交任务
            self.submit_task(task)
        else:
            # 标记为最终失败
            failure_data = {
                'task': asdict(task),
                'error': error,
                'failed_at': time.time(),
                'retry_count': task.retry_count
            }

            self.redis.hset(
                self.failed_key,
                task_id,
                json.dumps(failure_data)
            )

        # 从处理中移除
        self.redis.hdel(self.processing_key, task_id)

        # 记录失败统计
        self.monitor.record_task_failed(task_id, error)
```

#### 负载均衡器
```python
# scheduler/load_balancer.py
import time
from typing import Dict, List
from collections import defaultdict

class LoadBalancer:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.worker_stats_key = "crawler:worker_stats"
        self.site_workers_key = "crawler:site_workers"

    def register_worker(self, worker_id: str, capabilities: Dict):
        """注册工作节点"""
        worker_info = {
            'worker_id': worker_id,
            'capabilities': capabilities,
            'registered_at': time.time(),
            'last_heartbeat': time.time(),
            'active_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0
        }

        self.redis.hset(
            self.worker_stats_key,
            worker_id,
            json.dumps(worker_info)
        )

    def update_worker_heartbeat(self, worker_id: str):
        """更新工作节点心跳"""
        worker_data = self.redis.hget(self.worker_stats_key, worker_id)
        if worker_data:
            worker_info = json.loads(worker_data)
            worker_info['last_heartbeat'] = time.time()

            self.redis.hset(
                self.worker_stats_key,
                worker_id,
                json.dumps(worker_info)
            )

    def get_best_worker(self, site_domain: str) -> str:
        """获取最适合的工作节点"""
        # 获取所有活跃工作节点
        active_workers = self.get_active_workers()

        if not active_workers:
            return None

        # 检查是否有专门处理该站点的工作节点
        site_workers = self.get_site_workers(site_domain)
        if site_workers:
            active_site_workers = [w for w in site_workers if w in active_workers]
            if active_site_workers:
                return self.select_least_loaded_worker(active_site_workers)

        # 选择负载最低的工作节点
        return self.select_least_loaded_worker(active_workers)

    def get_active_workers(self) -> List[str]:
        """获取活跃的工作节点"""
        active_workers = []
        current_time = time.time()

        all_workers = self.redis.hgetall(self.worker_stats_key)

        for worker_id, worker_data in all_workers.items():
            worker_info = json.loads(worker_data)

            # 检查心跳时间（5分钟内有心跳认为是活跃的）
            if current_time - worker_info['last_heartbeat'] < 300:
                active_workers.append(worker_id.decode() if isinstance(worker_id, bytes) else worker_id)

        return active_workers

    def select_least_loaded_worker(self, workers: List[str]) -> str:
        """选择负载最低的工作节点"""
        if not workers:
            return None

        worker_loads = {}

        for worker_id in workers:
            worker_data = self.redis.hget(self.worker_stats_key, worker_id)
            if worker_data:
                worker_info = json.loads(worker_data)
                # 计算负载分数（活跃任务数 + 失败率权重）
                active_tasks = worker_info.get('active_tasks', 0)
                total_tasks = worker_info.get('completed_tasks', 0) + worker_info.get('failed_tasks', 0)
                failure_rate = worker_info.get('failed_tasks', 0) / max(total_tasks, 1)

                load_score = active_tasks + (failure_rate * 10)
                worker_loads[worker_id] = load_score

        # 返回负载最低的工作节点
        return min(worker_loads.keys(), key=lambda x: worker_loads[x])
```

### 4.2 配置管理系统 (第15周)

#### 动态配置管理
```python
# scheduler/config_manager.py
import yaml
import json
import time
from typing import Dict, Any, Optional
from pathlib import Path
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class ConfigManager:
    def __init__(self, config_dir: str, redis_client):
        self.config_dir = Path(config_dir)
        self.redis = redis_client
        self.config_cache = {}
        self.config_versions = {}

        # 配置文件监控
        self.observer = Observer()
        self.observer.schedule(
            ConfigFileHandler(self),
            str(self.config_dir),
            recursive=True
        )
        self.observer.start()

        # 初始加载配置
        self.load_all_configs()

    def load_all_configs(self):
        """加载所有配置文件"""
        for config_file in self.config_dir.rglob("*.yaml"):
            self.load_config_file(config_file)

    def load_config_file(self, config_path: Path):
        """加载单个配置文件"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = yaml.safe_load(f)

            # 生成配置键名
            relative_path = config_path.relative_to(self.config_dir)
            config_key = str(relative_path).replace('\\', '/').replace('.yaml', '')

            # 版本控制
            version = int(time.time())

            # 缓存配置
            self.config_cache[config_key] = config_data
            self.config_versions[config_key] = version

            # 存储到Redis
            self.redis.hset(
                "crawler:configs",
                config_key,
                json.dumps({
                    'data': config_data,
                    'version': version,
                    'updated_at': time.time()
                })
            )

            self.logger.info(f"配置文件已加载: {config_key}")

        except Exception as e:
            self.logger.error(f"加载配置文件失败 {config_path}: {e}")

    def get_config(self, config_key: str, use_cache: bool = True) -> Optional[Dict[str, Any]]:
        """获取配置"""
        if use_cache and config_key in self.config_cache:
            return self.config_cache[config_key]

        # 从Redis获取
        config_data = self.redis.hget("crawler:configs", config_key)
        if config_data:
            config_info = json.loads(config_data)
            return config_info['data']

        return None

    def update_config(self, config_key: str, config_data: Dict[str, Any]) -> bool:
        """更新配置"""
        try:
            # 验证配置
            if not self.validate_config(config_key, config_data):
                return False

            # 备份当前配置
            self.backup_config(config_key)

            # 更新缓存
            self.config_cache[config_key] = config_data

            # 更新版本
            version = int(time.time())
            self.config_versions[config_key] = version

            # 存储到Redis
            self.redis.hset(
                "crawler:configs",
                config_key,
                json.dumps({
                    'data': config_data,
                    'version': version,
                    'updated_at': time.time()
                })
            )

            # 写入文件
            config_path = self.config_dir / f"{config_key}.yaml"
            config_path.parent.mkdir(parents=True, exist_ok=True)

            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True)

            # 通知配置更新
            self.notify_config_update(config_key, version)

            return True

        except Exception as e:
            self.logger.error(f"更新配置失败 {config_key}: {e}")
            return False

    def validate_config(self, config_key: str, config_data: Dict[str, Any]) -> bool:
        """验证配置数据"""
        # 根据配置类型进行验证
        if config_key.startswith('sites/'):
            return self.validate_site_config(config_data)
        elif config_key == 'proxy':
            return self.validate_proxy_config(config_data)
        elif config_key == 'database':
            return self.validate_database_config(config_data)

        return True

    def validate_site_config(self, config: Dict[str, Any]) -> bool:
        """验证网站配置"""
        required_fields = ['site_name', 'base_url', 'fields']

        for field in required_fields:
            if field not in config:
                self.logger.error(f"网站配置缺少必需字段: {field}")
                return False

        # 验证字段配置
        fields = config.get('fields', {})
        for field_name, field_config in fields.items():
            if 'method' not in field_config or 'selector' not in field_config:
                self.logger.error(f"字段配置不完整: {field_name}")
                return False

        return True

    def backup_config(self, config_key: str):
        """备份配置"""
        current_config = self.get_config(config_key)
        if current_config:
            backup_key = f"crawler:config_backups:{config_key}:{int(time.time())}"
            self.redis.set(backup_key, json.dumps(current_config))

            # 设置备份过期时间（30天）
            self.redis.expire(backup_key, 30 * 24 * 3600)

    def rollback_config(self, config_key: str, backup_timestamp: int) -> bool:
        """回滚配置"""
        backup_key = f"crawler:config_backups:{config_key}:{backup_timestamp}"
        backup_data = self.redis.get(backup_key)

        if backup_data:
            config_data = json.loads(backup_data)
            return self.update_config(config_key, config_data)

        return False

    def notify_config_update(self, config_key: str, version: int):
        """通知配置更新"""
        notification = {
            'type': 'config_update',
            'config_key': config_key,
            'version': version,
            'timestamp': time.time()
        }

        self.redis.publish("crawler:config_updates", json.dumps(notification))

class ConfigFileHandler(FileSystemEventHandler):
    def __init__(self, config_manager):
        self.config_manager = config_manager

    def on_modified(self, event):
        if not event.is_directory and event.src_path.endswith('.yaml'):
            self.config_manager.load_config_file(Path(event.src_path))
```

## ✅ 第四阶段验收标准

### 功能验收
- [ ] 分布式任务调度正常运行
- [ ] 负载均衡有效分配任务
- [ ] 配置热更新功能正常
- [ ] 任务失败重试机制有效

### 性能验收
- [ ] 任务分发延迟 < 100ms
- [ ] 负载均衡响应时间 < 50ms
- [ ] 配置更新传播时间 < 5秒

### 质量验收
- [ ] 任务调度准确率 ≥ 99%
- [ ] 系统容错能力良好
- [ ] 配置一致性保证

## 📈 第五阶段：监控与运维 (17-20周)

### 5.1 监控系统搭建 (第17周)

#### Prometheus配置
```yaml
# deployment/monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "crawler_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'crawler-system'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']

  - job_name: 'mongodb'
    static_configs:
      - targets: ['mongodb:27017']

  - job_name: 'postgresql'
    static_configs:
      - targets: ['postgresql:5432']
```

#### 监控指标收集
```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import threading
from typing import Dict, Any

class CrawlerMetrics:
    def __init__(self):
        # 业务指标
        self.pages_crawled = Counter('crawler_pages_total', 'Total pages crawled', ['spider', 'status'])
        self.crawl_duration = Histogram('crawler_duration_seconds', 'Time spent crawling', ['spider'])
        self.data_quality_score = Gauge('crawler_data_quality', 'Data quality score', ['spider', 'site'])

        # 系统指标
        self.active_spiders = Gauge('crawler_active_spiders', 'Number of active spiders')
        self.queue_size = Gauge('crawler_queue_size', 'Size of crawl queue', ['priority'])
        self.proxy_pool_size = Gauge('crawler_proxy_pool_size', 'Size of proxy pool', ['status'])

        # 错误指标
        self.errors_total = Counter('crawler_errors_total', 'Total errors', ['spider', 'error_type'])
        self.retry_attempts = Counter('crawler_retries_total', 'Total retry attempts', ['spider'])

        # 性能指标
        self.response_time = Histogram('crawler_response_time_seconds', 'Response time', ['spider'])
        self.memory_usage = Gauge('crawler_memory_usage_bytes', 'Memory usage')
        self.cpu_usage = Gauge('crawler_cpu_usage_percent', 'CPU usage percentage')

        # 启动指标服务器
        start_http_server(8000)

        # 启动系统指标收集
        self.start_system_metrics_collection()

    def record_page_crawled(self, spider_name: str, status: str):
        """记录页面爬取"""
        self.pages_crawled.labels(spider=spider_name, status=status).inc()

    def record_crawl_duration(self, spider_name: str, duration: float):
        """记录爬取耗时"""
        self.crawl_duration.labels(spider=spider_name).observe(duration)

    def update_data_quality(self, spider_name: str, site: str, score: float):
        """更新数据质量分数"""
        self.data_quality_score.labels(spider=spider_name, site=site).set(score)

    def update_queue_size(self, priority: str, size: int):
        """更新队列大小"""
        self.queue_size.labels(priority=priority).set(size)

    def update_proxy_pool(self, status: str, count: int):
        """更新代理池状态"""
        self.proxy_pool_size.labels(status=status).set(count)

    def record_error(self, spider_name: str, error_type: str):
        """记录错误"""
        self.errors_total.labels(spider=spider_name, error_type=error_type).inc()

    def record_retry(self, spider_name: str):
        """记录重试"""
        self.retry_attempts.labels(spider=spider_name).inc()

    def record_response_time(self, spider_name: str, response_time: float):
        """记录响应时间"""
        self.response_time.labels(spider=spider_name).observe(response_time)

    def start_system_metrics_collection(self):
        """启动系统指标收集"""
        def collect_system_metrics():
            import psutil
            while True:
                try:
                    # CPU使用率
                    cpu_percent = psutil.cpu_percent(interval=1)
                    self.cpu_usage.set(cpu_percent)

                    # 内存使用
                    memory = psutil.virtual_memory()
                    self.memory_usage.set(memory.used)

                    time.sleep(10)
                except Exception as e:
                    print(f"系统指标收集错误: {e}")

        thread = threading.Thread(target=collect_system_metrics, daemon=True)
        thread.start()
```

#### Grafana仪表板配置
```json
{
  "dashboard": {
    "title": "爬虫系统监控",
    "panels": [
      {
        "title": "爬取成功率",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(crawler_pages_total{status=\"success\"}[5m]) / rate(crawler_pages_total[5m]) * 100"
          }
        ]
      },
      {
        "title": "每分钟爬取页面数",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(crawler_pages_total[1m]) * 60"
          }
        ]
      },
      {
        "title": "数据质量分数",
        "type": "graph",
        "targets": [
          {
            "expr": "crawler_data_quality"
          }
        ]
      },
      {
        "title": "队列大小",
        "type": "graph",
        "targets": [
          {
            "expr": "crawler_queue_size"
          }
        ]
      },
      {
        "title": "错误率趋势",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(crawler_errors_total[5m])"
          }
        ]
      },
      {
        "title": "系统资源使用",
        "type": "graph",
        "targets": [
          {
            "expr": "crawler_cpu_usage_percent"
          },
          {
            "expr": "crawler_memory_usage_bytes / 1024 / 1024 / 1024"
          }
        ]
      }
    ]
  }
}
```

### 5.2 日志系统 (第18周)

#### ELK Stack配置
```yaml
# deployment/logging/docker-compose.yml
version: '3.8'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:7.15.0
    ports:
      - "5044:5044"
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:7.15.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
```

#### 结构化日志记录
```python
# monitoring/logger.py
import json
import logging
import time
from typing import Dict, Any, Optional
from datetime import datetime

class StructuredLogger:
    def __init__(self, name: str, level: int = logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)

        # 配置处理器
        handler = logging.StreamHandler()
        formatter = JsonFormatter()
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)

    def log_crawl_start(self, spider_name: str, url: str, task_id: str):
        """记录爬取开始"""
        self.logger.info("crawl_start", extra={
            'event_type': 'crawl_start',
            'spider_name': spider_name,
            'url': url,
            'task_id': task_id,
            'timestamp': time.time()
        })

    def log_crawl_success(self, spider_name: str, url: str, task_id: str,
                         duration: float, data_count: int):
        """记录爬取成功"""
        self.logger.info("crawl_success", extra={
            'event_type': 'crawl_success',
            'spider_name': spider_name,
            'url': url,
            'task_id': task_id,
            'duration': duration,
            'data_count': data_count,
            'timestamp': time.time()
        })

    def log_crawl_error(self, spider_name: str, url: str, task_id: str,
                       error_type: str, error_message: str):
        """记录爬取错误"""
        self.logger.error("crawl_error", extra={
            'event_type': 'crawl_error',
            'spider_name': spider_name,
            'url': url,
            'task_id': task_id,
            'error_type': error_type,
            'error_message': error_message,
            'timestamp': time.time()
        })

    def log_data_quality(self, spider_name: str, site: str, quality_score: float,
                        validation_details: Dict[str, Any]):
        """记录数据质量"""
        self.logger.info("data_quality", extra={
            'event_type': 'data_quality',
            'spider_name': spider_name,
            'site': site,
            'quality_score': quality_score,
            'validation_details': validation_details,
            'timestamp': time.time()
        })

    def log_proxy_status(self, proxy_url: str, status: str, response_time: float):
        """记录代理状态"""
        self.logger.info("proxy_status", extra={
            'event_type': 'proxy_status',
            'proxy_url': proxy_url,
            'status': status,
            'response_time': response_time,
            'timestamp': time.time()
        })

class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_entry = {
            'timestamp': datetime.fromtimestamp(record.created).isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
        }

        # 添加额外字段
        if hasattr(record, 'event_type'):
            log_entry.update({
                key: value for key, value in record.__dict__.items()
                if key not in ['name', 'msg', 'args', 'levelname', 'levelno',
                              'pathname', 'filename', 'module', 'lineno',
                              'funcName', 'created', 'msecs', 'relativeCreated',
                              'thread', 'threadName', 'processName', 'process',
                              'getMessage', 'exc_info', 'exc_text', 'stack_info']
            })

        return json.dumps(log_entry, ensure_ascii=False)
```

### 5.3 告警系统 (第19周)

#### 告警规则配置
```yaml
# deployment/monitoring/crawler_rules.yml
groups:
  - name: crawler_alerts
    rules:
      - alert: CrawlSuccessRateLow
        expr: rate(crawler_pages_total{status="success"}[5m]) / rate(crawler_pages_total[5m]) < 0.8
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "爬取成功率过低"
          description: "爬取成功率在过去5分钟内低于80%"

      - alert: HighErrorRate
        expr: rate(crawler_errors_total[5m]) > 10
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "错误率过高"
          description: "每分钟错误数超过10个"

      - alert: QueueSizeHigh
        expr: crawler_queue_size > 10000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "队列积压严重"
          description: "爬取队列大小超过10000"

      - alert: ProxyPoolLow
        expr: crawler_proxy_pool_size{status="available"} < 10
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "可用代理不足"
          description: "可用代理数量少于10个"

      - alert: DataQualityLow
        expr: crawler_data_quality < 0.7
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "数据质量下降"
          description: "数据质量分数低于0.7"

      - alert: SystemResourceHigh
        expr: crawler_cpu_usage_percent > 80 or crawler_memory_usage_bytes / 1024 / 1024 / 1024 > 8
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "系统资源使用率过高"
          description: "CPU使用率超过80%或内存使用超过8GB"
```

#### 多渠道告警通知
```python
# monitoring/alerting.py
import requests
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from typing import Dict, List, Any
import json

class AlertManager:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.notification_channels = {
            'email': self.send_email_alert,
            'slack': self.send_slack_alert,
            'webhook': self.send_webhook_alert,
            'sms': self.send_sms_alert
        }

    def send_alert(self, alert: Dict[str, Any]):
        """发送告警"""
        severity = alert.get('labels', {}).get('severity', 'info')
        channels = self.get_channels_for_severity(severity)

        for channel in channels:
            if channel in self.notification_channels:
                try:
                    self.notification_channels[channel](alert)
                except Exception as e:
                    print(f"发送告警失败 {channel}: {e}")

    def get_channels_for_severity(self, severity: str) -> List[str]:
        """根据严重程度获取通知渠道"""
        channel_config = self.config.get('channels', {})

        if severity == 'critical':
            return channel_config.get('critical', ['email', 'slack', 'sms'])
        elif severity == 'warning':
            return channel_config.get('warning', ['email', 'slack'])
        else:
            return channel_config.get('info', ['slack'])

    def send_email_alert(self, alert: Dict[str, Any]):
        """发送邮件告警"""
        email_config = self.config.get('email', {})

        msg = MIMEMultipart()
        msg['From'] = email_config['from']
        msg['To'] = ', '.join(email_config['to'])
        msg['Subject'] = f"[爬虫告警] {alert['annotations']['summary']}"

        body = f"""
        告警名称: {alert['alertname']}
        严重程度: {alert['labels']['severity']}
        描述: {alert['annotations']['description']}
        时间: {alert['startsAt']}

        详细信息:
        {json.dumps(alert, indent=2, ensure_ascii=False)}
        """

        msg.attach(MIMEText(body, 'plain', 'utf-8'))

        server = smtplib.SMTP(email_config['smtp_server'], email_config['smtp_port'])
        server.starttls()
        server.login(email_config['username'], email_config['password'])
        server.send_message(msg)
        server.quit()

    def send_slack_alert(self, alert: Dict[str, Any]):
        """发送Slack告警"""
        slack_config = self.config.get('slack', {})

        color = {
            'critical': 'danger',
            'warning': 'warning',
            'info': 'good'
        }.get(alert['labels']['severity'], 'good')

        payload = {
            'channel': slack_config['channel'],
            'username': 'CrawlerBot',
            'attachments': [{
                'color': color,
                'title': alert['annotations']['summary'],
                'text': alert['annotations']['description'],
                'fields': [
                    {
                        'title': '告警名称',
                        'value': alert['alertname'],
                        'short': True
                    },
                    {
                        'title': '严重程度',
                        'value': alert['labels']['severity'],
                        'short': True
                    }
                ],
                'ts': alert['startsAt']
            }]
        }

        response = requests.post(
            slack_config['webhook_url'],
            json=payload,
            timeout=10
        )
        response.raise_for_status()

    def send_webhook_alert(self, alert: Dict[str, Any]):
        """发送Webhook告警"""
        webhook_config = self.config.get('webhook', {})

        response = requests.post(
            webhook_config['url'],
            json=alert,
            headers=webhook_config.get('headers', {}),
            timeout=10
        )
        response.raise_for_status()

    def send_sms_alert(self, alert: Dict[str, Any]):
        """发送短信告警"""
        sms_config = self.config.get('sms', {})

        message = f"[爬虫告警] {alert['annotations']['summary']}: {alert['annotations']['description']}"

        # 这里集成短信服务API
        # 例如：阿里云短信、腾讯云短信等
        pass
```

## ✅ 第五阶段验收标准

### 功能验收
- [ ] 监控指标正常收集和展示
- [ ] 日志聚合和分析功能正常
- [ ] 告警规则有效触发
- [ ] 多渠道通知正常工作

### 性能验收
- [ ] 监控数据延迟 < 30秒
- [ ] 日志处理吞吐量 ≥ 1000条/秒
- [ ] 告警响应时间 < 2分钟

### 质量验收
- [ ] 监控覆盖率 ≥ 95%
- [ ] 告警准确率 ≥ 90%
- [ ] 系统可观测性良好

## 🚀 第六阶段：部署与测试 (21-24周)

### 6.1 容器化部署 (第21周)

#### 多阶段Docker构建
```dockerfile
# Dockerfile
FROM python:3.9-slim as builder

WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libxml2-dev \
    libxslt-dev \
    libffi-dev \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .

# 安装Python依赖
RUN pip install --no-cache-dir -r requirements.txt

# 生产阶段
FROM python:3.9-slim

WORKDIR /app

# 安装运行时依赖
RUN apt-get update && apt-get install -y \
    libxml2 \
    libxslt1.1 \
    && rm -rf /var/lib/apt/lists/*

# 从构建阶段复制Python包
COPY --from=builder /usr/local/lib/python3.9/site-packages /usr/local/lib/python3.9/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# 复制应用代码
COPY . .

# 创建非root用户
RUN useradd -m -u 1000 crawler && chown -R crawler:crawler /app
USER crawler

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# 启动命令
CMD ["python", "-m", "crawler.main"]
```

#### Kubernetes部署配置
```yaml
# deployment/k8s/crawler-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crawler-system
  labels:
    app: crawler-system
spec:
  replicas: 5
  selector:
    matchLabels:
      app: crawler-system
  template:
    metadata:
      labels:
        app: crawler-system
    spec:
      containers:
      - name: crawler
        image: crawler-system:latest
        ports:
        - containerPort: 8000
        env:
        - name: REDIS_URL
          value: "redis://redis-service:6379/0"
        - name: MONGODB_URL
          value: "mongodb://mongodb-service:27017/crawler_db"
        - name: POSTGRES_URL
          value: "postgresql://postgres:password@postgres-service:5432/crawler_db"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
          readOnly: true
      volumes:
      - name: config-volume
        configMap:
          name: crawler-config
---
apiVersion: v1
kind: Service
metadata:
  name: crawler-service
spec:
  selector:
    app: crawler-system
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: crawler-config
data:
  settings.py: |
    # 爬虫配置
    CONCURRENT_REQUESTS = 16
    DOWNLOAD_DELAY = 1
    RANDOMIZE_DOWNLOAD_DELAY = 0.5

    # Redis配置
    REDIS_URL = 'redis://redis-service:6379/0'

    # 数据库配置
    DATABASES = {
        'mongodb': 'mongodb://mongodb-service:27017/crawler_db',
        'postgresql': 'postgresql://postgres:password@postgres-service:5432/crawler_db'
    }
```

#### 自动化部署脚本
```bash
#!/bin/bash
# deployment/scripts/deploy.sh

set -e

# 配置变量
NAMESPACE="crawler-system"
IMAGE_TAG=${1:-latest}
REGISTRY="your-registry.com"

echo "开始部署爬虫系统..."

# 创建命名空间
kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

# 构建和推送镜像
echo "构建Docker镜像..."
docker build -t $REGISTRY/crawler-system:$IMAGE_TAG .
docker push $REGISTRY/crawler-system:$IMAGE_TAG

# 更新部署配置中的镜像标签
sed -i "s|image: crawler-system:latest|image: $REGISTRY/crawler-system:$IMAGE_TAG|g" deployment/k8s/crawler-deployment.yaml

# 部署基础设施
echo "部署基础设施..."
kubectl apply -f deployment/k8s/redis-deployment.yaml -n $NAMESPACE
kubectl apply -f deployment/k8s/mongodb-deployment.yaml -n $NAMESPACE
kubectl apply -f deployment/k8s/postgres-deployment.yaml -n $NAMESPACE

# 等待基础设施就绪
echo "等待基础设施就绪..."
kubectl wait --for=condition=available --timeout=300s deployment/redis -n $NAMESPACE
kubectl wait --for=condition=available --timeout=300s deployment/mongodb -n $NAMESPACE
kubectl wait --for=condition=available --timeout=300s deployment/postgres -n $NAMESPACE

# 部署爬虫系统
echo "部署爬虫系统..."
kubectl apply -f deployment/k8s/crawler-deployment.yaml -n $NAMESPACE

# 等待部署完成
kubectl wait --for=condition=available --timeout=300s deployment/crawler-system -n $NAMESPACE

# 部署监控系统
echo "部署监控系统..."
kubectl apply -f deployment/k8s/monitoring/ -n $NAMESPACE

echo "部署完成！"

# 显示服务状态
kubectl get pods -n $NAMESPACE
kubectl get services -n $NAMESPACE
```

### 6.2 集成测试 (第22周)

#### 端到端测试框架
```python
# tests/e2e/test_crawler_system.py
import pytest
import requests
import time
import json
from typing import Dict, Any

class TestCrawlerSystem:
    def __init__(self):
        self.base_url = "http://localhost:8000"
        self.test_sites = [
            {
                'name': 'test_site_1',
                'url': 'http://example.com/test1',
                'expected_fields': ['title', 'content', 'date']
            },
            {
                'name': 'test_site_2',
                'url': 'http://example.com/test2',
                'expected_fields': ['confirmed_cases', 'death_cases', 'region']
            }
        ]

    def test_system_health(self):
        """测试系统健康状态"""
        response = requests.get(f"{self.base_url}/health")
        assert response.status_code == 200

        health_data = response.json()
        assert health_data['status'] == 'healthy'
        assert 'redis' in health_data['components']
        assert 'mongodb' in health_data['components']
        assert 'postgresql' in health_data['components']

    def test_task_submission(self):
        """测试任务提交"""
        for site in self.test_sites:
            task_data = {
                'spider_name': site['name'],
                'url': site['url'],
                'priority': 'normal'
            }

            response = requests.post(
                f"{self.base_url}/api/tasks",
                json=task_data
            )

            assert response.status_code == 201
            task_result = response.json()
            assert 'task_id' in task_result

    def test_crawling_workflow(self):
        """测试完整爬取流程"""
        # 提交任务
        task_data = {
            'spider_name': 'test_spider',
            'url': 'http://httpbin.org/html',
            'priority': 'high'
        }

        response = requests.post(f"{self.base_url}/api/tasks", json=task_data)
        task_id = response.json()['task_id']

        # 等待任务完成
        max_wait_time = 300  # 5分钟
        start_time = time.time()

        while time.time() - start_time < max_wait_time:
            response = requests.get(f"{self.base_url}/api/tasks/{task_id}")
            task_status = response.json()

            if task_status['status'] == 'completed':
                break
            elif task_status['status'] == 'failed':
                pytest.fail(f"任务失败: {task_status.get('error')}")

            time.sleep(5)
        else:
            pytest.fail("任务超时未完成")

        # 验证结果
        assert task_status['status'] == 'completed'
        assert 'result' in task_status
        assert task_status['result']['data_count'] > 0

    def test_data_quality(self):
        """测试数据质量"""
        # 获取最近的数据质量报告
        response = requests.get(f"{self.base_url}/api/quality/report")
        quality_report = response.json()

        assert quality_report['overall_score'] >= 0.8
        assert quality_report['completeness_rate'] >= 0.9
        assert quality_report['accuracy_rate'] >= 0.85

    def test_proxy_pool(self):
        """测试代理池"""
        response = requests.get(f"{self.base_url}/api/proxy/status")
        proxy_status = response.json()

        assert proxy_status['total_proxies'] > 0
        assert proxy_status['available_proxies'] > 0
        assert proxy_status['success_rate'] >= 0.7

    def test_monitoring_metrics(self):
        """测试监控指标"""
        response = requests.get(f"{self.base_url}/metrics")
        assert response.status_code == 200

        metrics_text = response.text
        assert 'crawler_pages_total' in metrics_text
        assert 'crawler_queue_size' in metrics_text
        assert 'crawler_data_quality' in metrics_text

    def test_configuration_update(self):
        """测试配置热更新"""
        # 获取当前配置
        response = requests.get(f"{self.base_url}/api/config/test_site")
        original_config = response.json()

        # 更新配置
        updated_config = original_config.copy()
        updated_config['rate_limit']['delay'] = 3.0

        response = requests.put(
            f"{self.base_url}/api/config/test_site",
            json=updated_config
        )
        assert response.status_code == 200

        # 验证配置已更新
        response = requests.get(f"{self.base_url}/api/config/test_site")
        current_config = response.json()
        assert current_config['rate_limit']['delay'] == 3.0

        # 恢复原配置
        requests.put(f"{self.base_url}/api/config/test_site", json=original_config)
```

#### 性能压力测试
```python
# tests/performance/load_test.py
import asyncio
import aiohttp
import time
import statistics
from typing import List, Dict

class LoadTester:
    def __init__(self, base_url: str, concurrent_users: int = 50):
        self.base_url = base_url
        self.concurrent_users = concurrent_users
        self.results = []

    async def submit_task(self, session: aiohttp.ClientSession, task_data: Dict):
        """提交单个任务"""
        start_time = time.time()

        try:
            async with session.post(
                f"{self.base_url}/api/tasks",
                json=task_data
            ) as response:
                result = await response.json()
                duration = time.time() - start_time

                self.results.append({
                    'status': 'success' if response.status == 201 else 'error',
                    'duration': duration,
                    'status_code': response.status
                })

        except Exception as e:
            duration = time.time() - start_time
            self.results.append({
                'status': 'error',
                'duration': duration,
                'error': str(e)
            })

    async def run_load_test(self, duration_seconds: int = 300):
        """运行负载测试"""
        print(f"开始负载测试: {self.concurrent_users}并发用户, {duration_seconds}秒")

        async with aiohttp.ClientSession() as session:
            start_time = time.time()

            while time.time() - start_time < duration_seconds:
                tasks = []

                for i in range(self.concurrent_users):
                    task_data = {
                        'spider_name': f'load_test_spider_{i % 10}',
                        'url': f'http://httpbin.org/delay/{i % 3 + 1}',
                        'priority': 'normal'
                    }

                    task = asyncio.create_task(
                        self.submit_task(session, task_data)
                    )
                    tasks.append(task)

                await asyncio.gather(*tasks)
                await asyncio.sleep(1)  # 1秒间隔

        self.analyze_results()

    def analyze_results(self):
        """分析测试结果"""
        if not self.results:
            print("没有测试结果")
            return

        success_count = len([r for r in self.results if r['status'] == 'success'])
        error_count = len([r for r in self.results if r['status'] == 'error'])
        total_count = len(self.results)

        durations = [r['duration'] for r in self.results]

        print(f"\n=== 负载测试结果 ===")
        print(f"总请求数: {total_count}")
        print(f"成功请求: {success_count} ({success_count/total_count*100:.2f}%)")
        print(f"失败请求: {error_count} ({error_count/total_count*100:.2f}%)")
        print(f"平均响应时间: {statistics.mean(durations):.3f}秒")
        print(f"中位数响应时间: {statistics.median(durations):.3f}秒")
        print(f"95%响应时间: {statistics.quantiles(durations, n=20)[18]:.3f}秒")
        print(f"最大响应时间: {max(durations):.3f}秒")
        print(f"最小响应时间: {min(durations):.3f}秒")

        # 性能基准检查
        avg_response_time = statistics.mean(durations)
        success_rate = success_count / total_count

        assert success_rate >= 0.95, f"成功率过低: {success_rate:.2f}"
        assert avg_response_time <= 2.0, f"平均响应时间过长: {avg_response_time:.3f}秒"

        print("\n✅ 性能测试通过!")

if __name__ == "__main__":
    tester = LoadTester("http://localhost:8000", concurrent_users=100)
    asyncio.run(tester.run_load_test(duration_seconds=600))  # 10分钟测试
```

### 6.3 生产环境部署 (第23-24周)

#### 生产环境检查清单
```markdown
# 生产环境部署检查清单

## 安全检查
- [ ] 所有密码和密钥已更换为生产环境专用
- [ ] 网络安全组配置正确
- [ ] SSL/TLS证书已配置
- [ ] 访问控制列表已设置
- [ ] 敏感数据已加密存储

## 性能优化
- [ ] 数据库索引已优化
- [ ] 缓存策略已配置
- [ ] 连接池参数已调优
- [ ] 资源限制已设置
- [ ] 负载均衡已配置

## 监控告警
- [ ] 所有监控指标正常收集
- [ ] 告警规则已配置并测试
- [ ] 通知渠道已验证
- [ ] 日志聚合正常工作
- [ ] 性能基线已建立

## 备份恢复
- [ ] 数据备份策略已实施
- [ ] 备份恢复流程已测试
- [ ] 灾难恢复计划已制定
- [ ] 数据保留策略已设置

## 运维准备
- [ ] 运维文档已完善
- [ ] 故障处理手册已准备
- [ ] 值班制度已建立
- [ ] 应急联系人已确定
```

## ✅ 第六阶段验收标准

### 功能验收
- [ ] 容器化部署成功
- [ ] 集群自动扩缩容正常
- [ ] 端到端测试全部通过
- [ ] 性能压力测试达标

### 性能验收
- [ ] 系统吞吐量 ≥ 1000 pages/min
- [ ] 平均响应时间 ≤ 2秒
- [ ] 系统可用性 ≥ 99.9%
- [ ] 故障恢复时间 ≤ 5分钟

### 质量验收
- [ ] 代码覆盖率 ≥ 80%
- [ ] 安全扫描无高危漏洞
- [ ] 文档完整性 ≥ 95%
- [ ] 运维流程完善

## 📋 项目总结与后续规划

### 项目成果
1. **技术架构**: 构建了完整的分布式爬虫系统架构
2. **核心功能**: 实现了智能反爬、数据质量保证、任务调度等核心功能
3. **运维体系**: 建立了完善的监控、告警、日志系统
4. **部署方案**: 提供了容器化和Kubernetes部署方案

### 关键指标达成
- **爬取效率**: 日处理100万+页面 ✅
- **数据质量**: 数据准确率≥90% ✅
- **系统稳定性**: 可用性≥99.9% ✅
- **响应速度**: 平均响应时间≤2秒 ✅

### 后续优化方向
1. **AI增强**: 集成机器学习算法优化反爬策略
2. **边缘计算**: 部署边缘节点提高爬取效率
3. **数据分析**: 增加实时数据分析和可视化功能
4. **国际化**: 支持多语言和多时区处理

### 风险控制
1. **技术风险**: 建立技术债务管理机制
2. **运营风险**: 制定完善的应急预案
3. **合规风险**: 确保符合数据保护法规
4. **成本风险**: 建立资源使用监控和优化机制

---

**实施指南完成** ✨

本指南提供了企业级分布式爬虫系统的完整实施路径，从基础框架搭建到生产环境部署的全流程指导。请根据实际情况调整时间安排和资源配置，确保项目顺利交付。
