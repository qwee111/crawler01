# -*- coding: utf-8 -*-
"""
çˆ¬å–è§„åˆ™å¼•æ“

æ”¯æŒé…ç½®åŒ–çš„ç½‘ç«™çˆ¬å–è§„åˆ™ï¼Œé¿å…ä¸ºæ¯ä¸ªç½‘ç«™ç¼–å†™ç‰¹å®šä»£ç 
"""

import logging
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
from urllib.parse import urljoin, urlparse

import yaml

logger = logging.getLogger(__name__)


class RuleEngine:
    """çˆ¬å–è§„åˆ™å¼•æ“"""

    def __init__(self, config_dir: str = "config/sites"):
        self.config_dir = Path(config_dir)
        self.rules = {}
        self.logger = logger  # æ·»åŠ loggerå±æ€§
        self.load_all_rules()

    def load_all_rules(self):
        """åŠ è½½æ‰€æœ‰ç½‘ç«™è§„åˆ™"""
        if not self.config_dir.exists():
            logger.warning(f"é…ç½®ç›®å½•ä¸å­˜åœ¨: {self.config_dir}")
            return

        for config_file in self.config_dir.glob("*.yaml"):
            try:
                with open(config_file, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)
                    site_name = config_file.stem
                    self.rules[site_name] = config
                    logger.info(f"åŠ è½½ç½‘ç«™è§„åˆ™: {site_name}")
            except Exception as e:
                logger.error(f"åŠ è½½è§„åˆ™æ–‡ä»¶å¤±è´¥ {config_file}: {e}")

    def get_rule(self, site_name: str) -> Optional[Dict]:
        """è·å–æŒ‡å®šç½‘ç«™çš„è§„åˆ™"""
        return self.rules.get(site_name)

    def match_site(self, url: str) -> Optional[str]:
        """æ ¹æ®URLåŒ¹é…ç½‘ç«™è§„åˆ™"""
        domain = urlparse(url).netloc

        for site_name, rule in self.rules.items():
            allowed_domains = rule.get("allowed_domains", [])
            for allowed_domain in allowed_domains:
                if domain.endswith(allowed_domain):
                    return site_name
        return None

    def extract_data(self, response, rule):
        """ä»å“åº”ä¸­æå–æ•°æ®"""
        try:
            # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
            if not response:
                self.logger.error("âŒ å“åº”å¯¹è±¡ä¸ºç©º")
                return {}

            # æ£€æŸ¥å“åº”çŠ¶æ€
            if hasattr(response, "status") and response.status >= 400:
                self.logger.error(f"âŒ HTTPé”™è¯¯çŠ¶æ€: {response.status}")
                return {"url": response.url, "error": f"HTTP {response.status}"}

            # å®‰å…¨æ£€æŸ¥å“åº”ç±»å‹
            content_type = self._get_content_type(response)
            self.logger.info(f"ğŸ“‹ å“åº”ç±»å‹: {content_type}")

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æœ¬å“åº”
            if not self._is_text_response_safe(response):
                self.logger.warning(f"âš ï¸ éæ–‡æœ¬å“åº”: {content_type}")
                return {
                    "url": response.url,
                    "status": "non_text_content",
                    "content_type": content_type,
                    "error": "Response content is not text",
                }

            # è·å–å­—æ®µæå–è§„åˆ™
            fields_config = rule.get("fields", {})
            if not fields_config:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°å­—æ®µæå–è§„åˆ™")
                return {"url": response.url, "error": "No field extraction rules"}

            extracted_data = {"url": response.url}

            # æå–æ¯ä¸ªå­—æ®µ
            for field_name, field_config in fields_config.items():
                try:
                    value = self._extract_field(response, field_config)
                    if value is not None:
                        extracted_data[field_name] = value
                        self.logger.debug(f"âœ… æå–å­—æ®µ {field_name}: {str(value)[:100]}...")
                    else:
                        self.logger.debug(f"âš ï¸ å­—æ®µ {field_name} æœªæå–åˆ°å€¼")
                except Exception as e:
                    self.logger.error(f"âŒ æå–å­—æ®µ {field_name} å¤±è´¥: {e}")
                    extracted_data[f"{field_name}_error"] = str(e)

            self.logger.info(f"ğŸ“Š æˆåŠŸæå– {len(extracted_data)} ä¸ªå­—æ®µ")
            return extracted_data

        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®æå–å¼‚å¸¸: {e}")
            import traceback

            self.logger.error(f"âŒ å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
            return {
                "url": getattr(response, "url", "unknown"),
                "error": str(e),
                "status": "extraction_error",
            }

    def _get_content_type(self, response):
        """å®‰å…¨è·å–Content-Type"""
        try:
            if hasattr(response, "headers"):
                content_type = response.headers.get("Content-Type", b"")
                if isinstance(content_type, bytes):
                    content_type = content_type.decode("utf-8", errors="ignore")
                return content_type
            return "unknown"
        except Exception:
            return "unknown"

    def _is_text_response_safe(self, response):
        """å®‰å…¨æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºæ–‡æœ¬å†…å®¹"""
        try:
            # æ£€æŸ¥Content-Typeå¤´
            content_type = self._get_content_type(response).lower()

            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æœ¬ç±»å‹
            text_types = [
                "text/html",
                "text/plain",
                "text/xml",
                "application/xml",
                "application/xhtml+xml",
                "application/json",
            ]

            for text_type in text_types:
                if text_type in content_type:
                    return True

            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„Content-Typeï¼Œå°è¯•æ£€æŸ¥å“åº”ä½“
            if not content_type or content_type == "unknown":
                if hasattr(response, "body") and response.body:
                    try:
                        # å°è¯•è§£ç å‰100å­—èŠ‚
                        sample = response.body[:100]
                        if isinstance(sample, bytes):
                            sample = sample.decode("utf-8", errors="ignore")
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«HTMLæ ‡ç­¾
                        return "<" in sample and ">" in sample
                    except Exception:
                        # äºŒè¿›åˆ¶/ç¼–ç è§£ç é”™è¯¯ç­‰ï¼šè§†ä¸ºéæ–‡æœ¬ï¼Œäº¤ç”±åç»­åˆ¤æ–­
                        return False

            # å¦‚æœContent-Typeä¸æ˜ç¡®ï¼Œå‡è®¾ä¸ºæ–‡æœ¬
            if not content_type or content_type == "unknown":
                self.logger.warning("âš ï¸ æ— æ³•ç¡®å®šå“åº”ç±»å‹ï¼Œå‡è®¾ä¸ºæ–‡æœ¬")
                return True

            # éæ–‡æœ¬ç±»å‹
            return False

        except Exception as e:
            self.logger.error(f"âŒ æ£€æŸ¥å“åº”ç±»å‹å¤±è´¥: {e}")
            # å‡ºé”™æ—¶å‡è®¾ä¸ºæ–‡æœ¬ï¼Œè®©åç»­å¤„ç†å†³å®š
            return True

    def _extract_field(self, response, field_rule: Union[str, Dict]) -> Any:
        """æå–å•ä¸ªå­—æ®µ"""
        if isinstance(field_rule, str):
            # ç®€å•CSSé€‰æ‹©å™¨
            return self._extract_by_css(response, field_rule)

        elif isinstance(field_rule, dict):
            method = field_rule.get("method", "css")
            selector = field_rule.get("selector", "")
            attr = field_rule.get("attr", "text")
            multiple = field_rule.get("multiple", False)
            regex = field_rule.get("regex")
            default = field_rule.get("default")

            if method == "css":
                result = self._extract_by_css(response, selector, attr, multiple)
            elif method == "xpath":
                result = self._extract_by_xpath(response, selector, attr, multiple)
            elif method == "regex":
                result = self._extract_by_regex(response, selector)
            else:
                result = default

            # åº”ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤„ç†
            if regex and result:
                if isinstance(result, list):
                    result = [self._apply_regex(item, regex) for item in result]
                else:
                    result = self._apply_regex(result, regex)

            return result if result is not None else default

        return None

    def _extract_by_css(
        self, response, selector: str, attr: str = "text", multiple: bool = False
    ):
        """ä½¿ç”¨CSSé€‰æ‹©å™¨æå–"""
        elements = response.css(selector)

        if not elements:
            return [] if multiple else None

        if attr == "text":
            values = [elem.get().strip() for elem in elements if elem.get()]
        else:
            values = [elem.attrib.get(attr, "").strip() for elem in elements]

        values = [v for v in values if v]  # è¿‡æ»¤ç©ºå€¼

        if multiple:
            return values
        else:
            return values[0] if values else None

    def _extract_by_xpath(
        self, response, selector: str, attr: str = "text", multiple: bool = False
    ):
        """ä½¿ç”¨XPathé€‰æ‹©å™¨æå–"""
        if attr == "text":
            xpath = f"{selector}//text()"
        else:
            xpath = f"{selector}/@{attr}"

        values = response.xpath(xpath).getall()
        values = [v.strip() for v in values if v.strip()]

        if multiple:
            return values
        else:
            return values[0] if values else None

    def _extract_by_regex(self, response, pattern: str):
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–"""
        matches = re.findall(pattern, response.text)
        return matches[0] if matches else None

    def _apply_regex(self, text: str, pattern: str) -> str:
        """åº”ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤„ç†æ–‡æœ¬"""
        if not text:
            return text

        match = re.search(pattern, text)
        return match.group(1) if match and match.groups() else text

    def get_links(self, response, rule: Dict) -> List[str]:
        """æ ¹æ®è§„åˆ™æå–é“¾æ¥"""
        links = []
        link_rules = rule.get("links", {})

        for link_type, link_rule in link_rules.items():
            try:
                if isinstance(link_rule, str):
                    # ç®€å•CSSé€‰æ‹©å™¨
                    found_links = response.css(f"{link_rule}::attr(href)").getall()
                else:
                    # å¤æ‚è§„åˆ™
                    selector = link_rule.get("selector", "")
                    attr = link_rule.get("attr", "href")
                    found_links = response.css(f"{selector}::attr({attr})").getall()

                # è½¬æ¢ä¸ºç»å¯¹URL
                for link in found_links:
                    if link:
                        absolute_url = urljoin(response.url, link)
                        links.append(
                            {
                                "url": absolute_url,
                                "type": link_type,
                                "source": response.url,
                            }
                        )

            except Exception as e:
                logger.warning(f"æå–é“¾æ¥ç±»å‹ {link_type} å¤±è´¥: {e}")

        return links

    def should_follow(self, url: str, rule: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·Ÿè¿›é“¾æ¥"""
        follow_rules = rule.get("follow", {})

        # æ£€æŸ¥å…è®¸çš„URLæ¨¡å¼
        allow_patterns = follow_rules.get("allow", [])
        if allow_patterns:
            for pattern in allow_patterns:
                if re.search(pattern, url):
                    break
            else:
                return False

        # æ£€æŸ¥ç¦æ­¢çš„URLæ¨¡å¼
        deny_patterns = follow_rules.get("deny", [])
        for pattern in deny_patterns:
            if re.search(pattern, url):
                return False

        return True

    def get_request_settings(self, rule: Dict) -> Dict:
        """è·å–è¯·æ±‚è®¾ç½®"""
        return rule.get("request_settings", {})


class AdaptiveSpider:
    """è‡ªé€‚åº”çˆ¬è™«æ··å…¥ç±»"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.rule_engine = RuleEngine()

    def parse_with_rules(self, response):
        """ä½¿ç”¨è§„åˆ™å¼•æ“è§£æå“åº”"""
        # åŒ¹é…ç½‘ç«™è§„åˆ™
        site_name = self.rule_engine.match_site(response.url)
        if not site_name:
            self.logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„ç½‘ç«™è§„åˆ™: {response.url}")
            return

        rule = self.rule_engine.get_rule(site_name)
        self.logger.info(f"ä½¿ç”¨è§„åˆ™ {site_name} è§£æ: {response.url}")

        # æå–æ•°æ®
        data = self.rule_engine.extract_data(response, rule)
        yield data

        # æå–å¹¶è·Ÿè¿›é“¾æ¥
        links = self.rule_engine.get_links(response, rule)
        for link_info in links:
            url = link_info["url"]
            if self.rule_engine.should_follow(url, rule):
                yield response.follow(
                    url,
                    callback=self.parse_with_rules,
                    meta={"link_type": link_info["type"]},
                )


# ä½¿ç”¨ç¤ºä¾‹çš„é…ç½®ç”Ÿæˆå™¨
def generate_site_config(site_name: str, config: Dict):
    """ç”Ÿæˆç½‘ç«™é…ç½®æ–‡ä»¶"""
    config_dir = Path("config/sites")
    config_dir.mkdir(parents=True, exist_ok=True)

    config_file = config_dir / f"{site_name}.yaml"
    with open(config_file, "w", encoding="utf-8") as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

    logger.info(f"ç”Ÿæˆé…ç½®æ–‡ä»¶: {config_file}")
