"""
é‡æ„åçš„è‡ªé€‚åº”çˆ¬è™«

ä½¿ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œä»£ç ç®€æ´æ¸…æ™°
"""

import asyncio
import hashlib
import json
import logging
import time
from typing import Dict, Optional
from urllib.parse import urljoin

import scrapy
from scrapy_redis import connection
from scrapy_redis.spiders import RedisSpider

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from crawler.core import ConfigManager, ExtractionEngine, PageAnalyzer, SiteDetector

logger = logging.getLogger(__name__)


class AdaptiveSpiderV2(RedisSpider):
    """é‡æ„åçš„è‡ªé€‚åº”çˆ¬è™« (RedisSpider ç‰ˆæœ¬ï¼Œæ”¯æŒ Redis åŠ¨æ€ç§å­)"""

    name = "adaptive_v2"
    # é»˜è®¤çš„å…¨å±€é˜Ÿåˆ—é”®ï¼›è‹¥æŒ‡å®š -a site=xxxï¼Œå°†åœ¨ __init__ ä¸­åˆ‡æ¢ä¸ºæŒ‰ç«™ç‚¹åˆ†æ¡¶çš„é”®
    redis_key = "adaptive_v2:start_urls"
    # å…³é—­é¡µé¢ç±»å‹è‡ªåŠ¨è¯†åˆ«ï¼Œä¼˜å…ˆä½¿ç”¨ Request.meta['page_type']ï¼ˆé»˜è®¤å¼€å¯ï¼Œå¯æŒ‰éœ€æ”¹ä¸º Falseï¼‰
    disable_page_detection = True

    def __init__(
        self,
        target_site: str = None,
        site: str = None,
        redis_key: str = None,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)

        # Redis é˜Ÿåˆ—é”®åŠ¨æ€è®¾ç½®ï¼šä¼˜å…ˆæŒ‰ç«™ç‚¹åˆ†æ¡¶ï¼›å¦åˆ™ä½¿ç”¨ä¼ å…¥çš„ redis_key
        if site or target_site:
            self.redis_key = f"adaptive_v2:{site or target_site}:start_urls"
        elif redis_key:
            self.redis_key = redis_key

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.config_manager = ConfigManager()
        self.site_detector = SiteDetector(self.config_manager)
        self.page_analyzer = PageAnalyzer(self.config_manager)
        self.extraction_engine = ExtractionEngine(self.config_manager)

        # å…¼å®¹å‘½ä»¤è¡Œå‚æ•°ï¼šæ”¯æŒ -a target_site=xxx å’Œ -a site=xxx
        passed_site = kwargs.pop("site", None)
        passed_target_site = kwargs.pop("target_site", None)
        self.target_site = target_site or site or passed_site or passed_target_site
        self.site_config = None

        if self.target_site:
            self._load_site_config()
        else:
            logger.warning("âš ï¸ æœªæä¾›ç›®æ ‡ç½‘ç«™å‚æ•°ã€‚è¯·ä½¿ç”¨ -a target_site=<site> æˆ– -a site=<site>")
            logger.info(f"ğŸ’¡ å¯ç”¨çš„ç½‘ç«™é…ç½®: {self.config_manager.list_sites()}")

        logger.info(f"ğŸš€ è‡ªé€‚åº”çˆ¬è™«V2å¯åŠ¨: ç›®æ ‡ç½‘ç«™={self.target_site}")

    def _load_site_config(self):
        """åŠ è½½ç½‘ç«™é…ç½®"""
        self.site_config = self.config_manager.get_config_by_site(self.target_site)

        if not self.site_config:
            logger.error(f"âŒ æœªæ‰¾åˆ°ç½‘ç«™é…ç½®: {self.target_site}")
            logger.info(f"ğŸ’¡ å¯ç”¨çš„ç½‘ç«™é…ç½®: {self.config_manager.list_sites()}")
            return

        logger.info(f"âœ… æ‰¾åˆ°ç½‘ç«™é…ç½®: {self.target_site}")
        logger.info(f"ğŸ“Š é…ç½®éƒ¨åˆ†: {list(self.site_config.keys())}")

        # è®¾ç½®èµ·å§‹URL
        start_urls_config = self.site_config.get("start_urls", [])
        if start_urls_config:
            self.start_urls = [
                item["url"] for item in start_urls_config if "url" in item
            ]
            logger.info("ğŸ“‹ åŠ è½½èµ·å§‹URL: %s ä¸ª", len(self.start_urls))
            for i, url in enumerate(self.start_urls, 1):
                logger.info(f"   {i}. {url}")
        else:
            logger.warning("âš ï¸ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰start_urlséƒ¨åˆ†")
            self.start_urls = []

        # è®¾ç½®è¯·æ±‚é…ç½®
        request_config = self.site_config.get("request", {})
        if request_config:
            self._apply_request_config(request_config)

        logger.info(f"âœ… ç½‘ç«™é…ç½®åŠ è½½å®Œæˆ: {self.target_site}")

    def _apply_request_config(self, request_config: Dict):
        """åº”ç”¨è¯·æ±‚é…ç½®"""
        # è®¾ç½®è¯·æ±‚å¤´
        headers = request_config.get("headers", {})
        if headers:
            self.custom_settings = self.custom_settings or {}
            self.custom_settings["DEFAULT_REQUEST_HEADERS"] = headers

        # è®¾ç½®å»¶è¿Ÿ
        delays = request_config.get("delays", {})
        if delays:
            download_delay = delays.get("download_delay", 2.0)
            self.custom_settings = self.custom_settings or {}
            self.custom_settings["DOWNLOAD_DELAY"] = download_delay
            if delays.get("randomize_delay", True):
                self.custom_settings["RANDOMIZE_DOWNLOAD_DELAY"] = True

    def _detect_direct_file(self, response) -> Optional[str]:
        """
        æ£€æµ‹æ˜¯å¦ä¸ºå¯ç›´æ¥ä¸‹è½½çš„æ–‡ä»¶ï¼ˆéä»…é™PDFï¼‰ã€‚
        è¿”å›æ–‡ä»¶æ‰©å±•åï¼ˆä¸å«ç‚¹ï¼Œå¦‚ 'pdf','docx','xls','zip'ï¼‰ï¼Œå¦åˆ™è¿”å› Noneã€‚
        åˆ¤å®šä¾æ®ï¼š
        - URL æ‰©å±•åå‘½ä¸­ç™½åå•
        - æˆ– Content-Type å‘½ä¸­å¸¸è§æ–‡ä»¶ç±»å‹
        """
        try:
            url = (response.url or "").lower()
            ctype_bytes = response.headers.get(b"Content-Type") or b""
            ctype = ctype_bytes.decode("utf-8", errors="ignore").lower()
        except Exception:
            url, ctype = response.url.lower(), ""

        # 1) åŸºäºURLæ‰©å±•å
        try:
            import os as _os
            from urllib.parse import urlparse

            path = urlparse(url).path
            _, ext = _os.path.splitext(path)
            ext = (ext or "").lower().lstrip(".")
        except Exception:
            ext = ""

        known_exts = {
            "pdf",
            "doc",
            "docx",
            "xls",
            "xlsx",
            "ppt",
            "pptx",
            "csv",
            "txt",
            "zip",
            "rar",
            "7z",
            "gz",
            "tar",
            "xml",
            "json",
        }
        if ext in known_exts:
            return ext

        # 2) åŸºäº Content-Type
        ctype_map = {
            "application/pdf": "pdf",
            "application/msword": "doc",
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document": "docx",
            "application/vnd.ms-excel": "xls",
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": "xlsx",
            "application/vnd.ms-powerpoint": "ppt",
            "application/vnd.openxmlformats-officedocument.presentationml.presentation": "pptx",
            "text/csv": "csv",
            "text/plain": "txt",
            "application/zip": "zip",
            "application/x-rar-compressed": "rar",
            "application/x-7z-compressed": "7z",
            "application/gzip": "gz",
            "application/x-tar": "tar",
            "application/xml": "xml",
            "text/xml": "xml",
            "application/json": "json",
        }
        for ct, mapped_ext in ctype_map.items():
            if ct in ctype:
                return mapped_ext

        # æŸäº›æœåŠ¡ä½¿ç”¨é€šç”¨çš„ octet-stream ä½œä¸ºé™„ä»¶
        if "application/octet-stream" in ctype:
            # å°è¯•å†ä»URLçŒœæµ‹
            return ext or None

        return None

    def make_request_from_data(self, data: bytes):
        """ä» Redis çš„ç§å­æ•°æ®åˆ›å»º Requestï¼Œå…¼å®¹ JSON æˆ– çº¯å­—ç¬¦ä¸² URL"""
        text = data.decode("utf-8").strip()
        try:
            payload = json.loads(text)
            url = payload.get("url") or payload.get("u")
            if not url:
                raise ValueError("seed json missing url")
            meta = payload.get("meta", {}) or {}
            headers = payload.get("headers")
            cb_name = payload.get("callback")
            cb_fn = getattr(self, cb_name, None) if cb_name else self.parse
            # é€ä¼  site
            if self.target_site and "site" not in meta:
                meta["site"] = self.target_site
            elif "site" in payload:
                meta.setdefault("site", payload["site"])

            # å¦‚æœç«™ç‚¹é…ç½®ä¸­å¯ç”¨äº† Seleniumï¼Œåˆ™åœ¨ meta ä¸­æ·»åŠ  use_selenium
            if self.site_config and self.site_config.get("selenium", {}).get(
                "enabled", False
            ):
                meta["use_selenium"] = True

            req = scrapy.Request(
                url, callback=cb_fn, headers=headers, meta=meta, dont_filter=False
            )
            return req
        except Exception:
            # å…¼å®¹çº¯å­—ç¬¦ä¸² URL
            meta = {}
            if self.target_site:
                meta["site"] = self.target_site
                # å¦‚æœç«™ç‚¹é…ç½®ä¸­å¯ç”¨äº† Seleniumï¼Œåˆ™åœ¨ meta ä¸­æ·»åŠ  use_selenium
                if self.site_config and self.site_config.get("selenium", {}).get(
                    "enabled", False
                ):
                    meta["use_selenium"] = True
            req = scrapy.Request(
                text, callback=self.parse, dont_filter=False, meta=meta
            )
            return req

    async def start(self):
        """ç”Ÿæˆèµ·å§‹è¯·æ±‚ï¼ˆScrapy 2.13+ï¼‰ï¼š
        1) å…ˆå‘æœ¬åœ°é…ç½®/ç«™ç‚¹é…ç½®çš„èµ·å§‹URLï¼ˆåˆ—è¡¨é¡µï¼Œå¼ºåˆ¶åˆ·æ–°ï¼‰
        2) å¯åŠ¨åŸºäºRedisçš„åˆ—è¡¨å‘¨æœŸåˆ·æ–°ï¼ˆZSETï¼‰
        3) ç›‘å¬Redisé˜Ÿåˆ—æ¶ˆè´¹åŠ¨æ€ç§å­ï¼ˆå…¼å®¹RedisSpiderï¼‰
        """

        # 1) åˆå§‹åŒ–Redisè¿æ¥ï¼ˆç”¨äºåˆ·æ–°/å¢é‡è¯†åˆ«ï¼‰
        try:
            self.server = connection.get_redis_from_settings(self.crawler.settings)
        except Exception as e:
            self.server = None
            logger.warning(f"âš ï¸ æ— æ³•è¿æ¥Redisï¼Œå°†ä»¥é™çº§æ¨¡å¼è¿è¡Œ: {e}")

        # è·å–åˆå§‹URLåˆ—è¡¨
        start_urls = list(getattr(self, "start_urls", []) or [])
        if not start_urls and self.site_config and "start_urls" in self.site_config:
            start_urls = [
                u.get("url")
                for u in self.site_config.get("start_urls", [])
                if u.get("url")
            ]

        # 2) å°†åˆå§‹URLä½œä¸º Request å¯¹è±¡ yield å‡ºå»ï¼Œè®© Scrapy è°ƒåº¦å™¨å¤„ç†å…¥é˜Ÿ
        if start_urls:
            for url in start_urls:
                logger.info(f"ğŸ“‹ åˆå§‹URLå·²ä½œä¸ºè¯·æ±‚ yield: {url}")
                meta = {
                    "page_type": "list_page",
                    "site_name": self.target_site,
                    "site": self.target_site,
                }
                if self.site_config and self.site_config.get("selenium", {}).get(
                    "enabled", False
                ):
                    meta["use_selenium"] = True
                yield scrapy.Request(
                    url=url,
                    callback=self.parse,
                    dont_filter=True,  # åˆå§‹ç§å­é€šå¸¸ä¸åº”è¢«å»é‡å™¨è¿‡æ»¤
                    meta=meta,
                    errback=self.handle_error,
                )
        else:
            logger.info("ğŸ“‹ æ²¡æœ‰é…ç½®åˆå§‹URLã€‚")

        # 3) ç›‘å¬ Redis é˜Ÿåˆ—ï¼ˆç»§æ‰¿è‡ª RedisSpider / Spider çš„ start å®ç°ï¼‰
        # RedisSpider ä¼šä» self.redis_key å¯¹åº”çš„é˜Ÿåˆ—ä¸­æ‹‰å–è¯·æ±‚
        try:
            async for req in super().start():
                yield req
        except Exception as e:
            logger.warning(f"âš ï¸ Redis é˜Ÿåˆ—ä¸å¯ç”¨æˆ–æœªé…ç½®: {e}")

    def parse(self, response):
        """è§£æé¡µé¢çš„ä¸»å…¥å£ï¼ˆæ”¯æŒåˆ—è¡¨é¡µå¢é‡ä¸è¯¦æƒ…é¡µå†…å®¹æŒ‡çº¹ï¼‰"""
        try:
            logger.info(f"âœ… å¼€å§‹è§£æé¡µé¢: {response.url}")

            # ç¡®å®šç½‘ç«™åï¼šä¼˜å…ˆä½¿ç”¨metaä¸­çš„site_nameï¼Œå…¶æ¬¡æ˜¯çˆ¬è™«å®ä¾‹çš„target_siteï¼Œæœ€åæ‰è‡ªåŠ¨æ£€æµ‹
            site_name = response.meta.get("site_name")
            if not site_name:
                site_name = self.target_site
            if not site_name:
                site_name = self._detect_site(response)

            if not site_name:
                logger.warning(f"âš ï¸ æ— æ³•ç¡®å®šç½‘ç«™åï¼Œè·³è¿‡è§£æ: {response.url}")
                return

            # é¡µé¢ç±»å‹ï¼šå½“ç¦ç”¨è‡ªåŠ¨æ£€æµ‹æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ meta æä¾›çš„ page_type
            if getattr(self, "disable_page_detection", False):
                page_type = response.meta.get("page_type") or "unknown_page"
                page_analysis = {"page_type": page_type, "site_name": site_name}
                logger.info(f"ğŸ” é¡µé¢ç±»å‹(ç¦ç”¨è‡ªåŠ¨æ£€æµ‹): {page_type}")
            else:
                # åˆ†æé¡µé¢
                page_analysis = self.page_analyzer.analyze_page(response, site_name)
                page_type = page_analysis.get("page_type")
                logger.info(f"ğŸ” é¡µé¢ç±»å‹: {page_type}")

            # æå–æ•°æ®
            extracted = self.extraction_engine.extract_data(
                response, site_name, page_analysis
            )

            # è‹¥ä¸ºç›´é“¾å¯ä¸‹è½½æ–‡ä»¶ï¼ˆä¸é™äºPDFï¼‰ï¼Œç›´æ¥äº§å‡ºå¹¶è¿”å›
            try:
                direct_ext = self._detect_direct_file(response)
                if direct_ext:
                    title = (
                        response.meta.get("list_title") or response.url.split("/")[-1]
                    )
                    publish_date = response.meta.get("list_date")
                    yield {
                        "url": response.url,
                        "title": title,
                        "publish_date": publish_date,
                        "file_urls": [response.url],
                        "content_type": direct_ext,
                        "spider_name": self.name,
                        "site_name": site_name,
                    }
                    return
            except Exception:
                pass

            # åˆ—è¡¨é¡µï¼šåªåšå¢é‡è¯†åˆ«ä¸æ´¾å‘
            if page_type == "list_page":
                items = (
                    extracted.get("items", []) if isinstance(extracted, dict) else []
                )
                logger.info(f"ğŸ§® åˆ—è¡¨é¡¹æ•°é‡: {len(items)}")
                logger.info(f"ğŸ§ª åˆ—è¡¨é¡¹æ ·ä¾‹: {items[:1]}")
                if items:
                    yield from self._handle_list_incremental(response, site_name, items)
                    return

                # åˆ—è¡¨é¡µä¸ºç©ºæ—¶ï¼Œå°è¯•ä½¿ç”¨é…ç½®çš„åˆ—è¡¨APIè·å–æ•°æ®
                api_cfg = (
                    (self.site_config.get("extraction", {}) or {})
                    .get("list_page", {})
                    .get("api")
                )
                if api_cfg:
                    try:
                        from urllib.parse import urlencode, urljoin

                        base_url = api_cfg.get("url") or ""
                        api_url = urljoin(response.url, base_url)
                        params = api_cfg.get("params") or {}
                        if params:
                            api_url = f"{api_url}{'&' if '?' in api_url else '?'}{urlencode(params)}"
                        headers = api_cfg.get("headers") or {}
                        logger.info(f"ğŸ§ª é€šè¿‡APIè·å–åˆ—è¡¨: {api_url}")
                        yield scrapy.Request(
                            url=api_url,
                            callback=self.parse_list_api,
                            headers=headers,
                            meta={
                                "site_name": site_name,
                                "page_type": "list_api",
                                "api_config": api_cfg,
                                "origin_url": response.url,
                            },
                            dont_filter=True,
                        )
                        return
                    except Exception as e:
                        logger.warning(f"âš ï¸ è§¦å‘åˆ—è¡¨APIå¤±è´¥: {e}")

                # æ— APIé…ç½®åˆ™ç»“æŸ
                yield from self._handle_list_incremental(response, site_name, items)
                return

            # è¯¦æƒ…é¡µï¼šä¼˜å…ˆæ£€æµ‹ç›´é“¾å¯ä¸‹è½½æ–‡ä»¶ï¼ˆå‘½ä¸­åˆ™ç›´æ¥äº§å‡ºï¼‰
            try:
                direct_ext = self._detect_direct_file(response)
                if direct_ext:
                    title = (
                        response.meta.get("list_title") or response.url.split("/")[-1]
                    )
                    publish_date = response.meta.get("list_date")
                    yield {
                        "url": response.url,
                        "title": title,
                        "publish_date": publish_date,
                        "file_urls": [response.url],
                        "content_type": direct_ext,
                        "spider_name": self.name,
                        "site_name": site_name,
                    }
                    return
            except Exception:
                pass

            # è¯¦æƒ…é¡µï¼šè¾“å‡ºæ•°æ®é¡¹ï¼Œç”± ContentUpdatePipeline è´Ÿè´£â€œå†…å®¹æŒ‡çº¹å»é‡â€
            if isinstance(extracted, dict):
                # é™„å¸¦åŸå§‹HTMLä»¥å¢å¼ºåç»­å¤„ç†å¯é æ€§ï¼ˆä»…é™æ–‡æœ¬å“åº”ï¼‰
                try:
                    from scrapy.http import TextResponse

                    raw_html = (
                        response.text if isinstance(response, TextResponse) else None
                    )
                except Exception:
                    raw_html = None

                # è‹¥è¯¦æƒ…é¡µæœªæåˆ°æ ‡é¢˜æˆ–æ ‡é¢˜å¼‚å¸¸ï¼Œä¼˜å…ˆå›é€€åˆ—è¡¨æ ‡é¢˜
                try:
                    if (
                        not extracted.get("title")
                        or str(extracted.get("title")).strip() == ""
                    ):
                        lt = response.meta.get("list_title")
                        if lt:
                            extracted["title"] = lt
                except Exception:
                    pass

                # é€ä¼ åˆ—è¡¨é¡µæ—¥æœŸä½œä¸ºå›é€€å‘å¸ƒæ—¶é—´
                if not extracted.get("publish_date") and response.meta.get("list_date"):
                    extracted["publish_date"] = response.meta.get("list_date")

                extracted.update(
                    {
                        "spider_name": self.name,
                        "spider_version": "2.0",
                        "site_name": site_name,
                        "page_analysis": page_analysis,
                        "response_meta": {
                            "status_code": response.status,
                            "content_type": response.headers.get(
                                "Content-Type", b""
                            ).decode("utf-8", errors="ignore"),
                            "content_length": len(response.body),
                            "url": response.url,
                        },
                    }
                )
                if raw_html and (not extracted.get("content")):
                    extracted["raw_html"] = raw_html
                yield extracted

            logger.info(f"âœ… é¡µé¢è§£æå®Œæˆ: {response.url}")

        except Exception as e:
            logger.error(f"âŒ é¡µé¢è§£æå¤±è´¥: {response.url}, é”™è¯¯: {e}")
            yield {
                "url": response.url,
                "error": str(e),
                "spider_name": self.name,
                "status": "parse_failed",
            }

    def _detect_site(self, response) -> Optional[str]:
        """æ£€æµ‹ç½‘ç«™"""
        # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„ç½‘ç«™å
        if self.target_site:
            return self.target_site

        # è‡ªåŠ¨æ£€æµ‹
        return self.site_detector.detect_site(response.url)

    def _schedule_next_refresh(self, list_url: str, interval: int):
        """ç™»è®°åˆ—è¡¨é¡µçš„ä¸‹æ¬¡åˆ·æ–°æ—¶é—´ï¼ˆä½¿ç”¨ Redis ZSET å®ç°ï¼‰"""
        if not self.server:
            return
        try:
            site = self.target_site or "default"
            refresh_key = f"refresh_queue:{site}"
            mid = hashlib.sha1(list_url.encode("utf-8")).hexdigest()
            self.server.set(f"list_url:{mid}", list_url)
            self.server.zadd(refresh_key, {mid: time.time() + interval})
        except Exception:
            logger.warning("âš ï¸ åˆ—è¡¨åˆ·æ–°ç™»è®°å¤±è´¥ï¼ˆå¼‚å¸¸å·²å¿½ç•¥ï¼‰")

    def _pop_due_refresh(self, refresh_key: str):
        """å¼¹å‡ºåˆ°æœŸçš„åˆ·æ–°æˆå‘˜ã€‚
        ä¼˜å…ˆç”¨ ZPOPMINï¼›ä¸æ”¯æŒæ—¶å›é€€ï¼šZRANGE æœ€å° + ZREM åŸå­æ€§ä¿è¯é è¿”å›å€¼ã€‚
        è¿”å› (member:str, score:float) æˆ– None
        """
        if not self.server:
            return None
        now = time.time()
        try:
            # ä¼˜å…ˆä½¿ç”¨ZPOPMIN
            res = self.server.zpopmin(refresh_key)
            if not res:
                return None
            member, score = res[0]
            if isinstance(member, bytes):
                member = member.decode("utf-8", errors="ignore")
            try:
                score = float(score)
            except Exception:
                score = now
            if score > now:
                # æœªåˆ°æœŸï¼Œæ”¾å›
                try:
                    self.server.zadd(refresh_key, {member: score})
                except Exception:
                    pass
                return None
            return member, score
        except Exception:
            # å›é€€æ–¹æ¡ˆï¼šZRANGE + ZREMï¼ˆä»¥ZREMè¿”å›1ä¿è¯åªæœ‰ä¸€æ–¹æˆåŠŸï¼‰
            try:
                res = self.server.zrange(refresh_key, 0, 0, withscores=True)
                if not res:
                    return None
                member, score = res[0]
                if isinstance(member, bytes):
                    member = member.decode("utf-8", errors="ignore")
                if float(score) > now:
                    return None
                # ä»…å½“ZREMæˆåŠŸï¼ˆè¿”å›1ï¼‰æ—¶è§†ä¸ºæŠ¢åˆ°
                if self.server.zrem(refresh_key, member) == 1:
                    return member, float(score)
                return None
            except Exception:
                return None

    def _handle_list_incremental(self, response, site_name: str, items: list):
        """å¢é‡è¯†åˆ«åˆ—è¡¨ä¸­çš„æ–‡ç« é“¾æ¥å¹¶å‘èµ·è¯·æ±‚"""
        interval = int(
            (self.site_config.get("update_detection", {}) or {}).get(
                "list_refresh_interval",
                self.settings.getint("LIST_REFRESH_INTERVAL", 900),
            )
        )
        self._schedule_next_refresh(response.url, interval)

        to_follow = []
        for i, it in enumerate(items):  # æ·»åŠ ç´¢å¼• i
            if not isinstance(it, dict):
                continue
            url = it.get("url")
            if not url:
                continue
            absolute_url = urljoin(response.url, url)
            to_follow.append(
                {
                    "url": absolute_url,
                    "list_title": it.get("title"),
                    "list_date": it.get("date") or it.get("publish_date"),
                    "item_index": i,  # æ·»åŠ ç´¢å¼•
                }
            )

        if not to_follow:
            return

        # è·å– click_selector é…ç½®
        click_selector_config = (
            self.site_config.get("extraction", {})
            .get("list_page", {})
            .get("list_items", {})
            .get("fields", {})
            .get("click_selector", {})
        )
        click_selector_value = click_selector_config.get("selector")
        use_selenium_for_site = self.site_config and self.site_config.get(
            "selenium", {}
        ).get("enabled", False)

        # Redis å¢é‡ï¼šåªæŠ“æ–°é“¾æ¥
        seen_key = f"seen_articles:{site_name or 'default'}"
        for entry in to_follow:
            link = entry["url"]
            meta = {"site_name": site_name, "page_type": "detail_page"}
            if entry.get("list_title"):
                meta["list_title"] = entry["list_title"]
            if entry.get("list_date"):
                meta["list_date"] = entry["list_date"]

            # å¦‚æœé…ç½®äº† click_selector å¹¶ä¸”ç«™ç‚¹å¯ç”¨äº† Selenium
            if click_selector_value and use_selenium_for_site:
                meta["use_selenium"] = True
                meta["selenium_click_selector"] = click_selector_value
                meta["selenium_item_index"] = entry["item_index"]
                meta["detail_page_url"] = link  # å­˜å‚¨çœŸå®çš„è¯¦æƒ…é¡µURLï¼Œç”¨äºåç»­å»é‡å’Œæ•°æ®å…³è”
                request_url = response.url  # è¯·æ±‚åˆ—è¡¨é¡µ
            elif (
                use_selenium_for_site
            ):  # å¦‚æœåªå¯ç”¨äº†Seleniumï¼Œä½†æ²¡æœ‰click_selectorï¼Œåˆ™æŒ‰åŸSeleniumé€»è¾‘å¤„ç†
                meta["use_selenium"] = True
                request_url = link
            else:  # ä¸ä½¿ç”¨Selenium
                request_url = link

            if not self.server:
                # é™çº§ï¼šä¸ä½¿ç”¨å¢é‡è¿‡æ»¤
                yield scrapy.Request(
                    url=request_url,  # ä½¿ç”¨æ–°çš„ request_url
                    callback=self.parse,
                    meta=meta,
                    errback=self.handle_error,
                )
                continue
            try:
                uhash = hashlib.sha1(link.encode("utf-8")).hexdigest()
                # ç¡®ä¿ seen_key ä½¿ç”¨ site_name è¿›è¡Œéš”ç¦»
                if not self.server.sismember(seen_key, uhash):
                    self.server.sadd(seen_key, uhash)
                    yield scrapy.Request(
                        url=request_url,  # ä½¿ç”¨æ–°çš„ request_url
                        callback=self.parse,
                        meta=meta,
                        errback=self.handle_error,
                    )
            except Exception as e:
                logger.warning(f"âš ï¸ Redis å¢é‡è¯†åˆ«å¤±è´¥ï¼Œé™çº§ç›´æŠ“: {e}")
                yield scrapy.Request(
                    url=request_url,  # ä½¿ç”¨æ–°çš„ request_url
                    callback=self.parse,
                    meta=meta,
                    errback=self.handle_error,
                )

        return self.site_detector.detect_site(response.url)

    def parse_list_api(self, response):
        """è§£æåˆ—è¡¨APIçš„å“åº”ï¼Œå°†å…¶è½¬æ¢æˆ items ç»“æ„ã€‚
        æ”¯æŒä¸‰ç§å½¢å¼ï¼š
        1) JSON + åˆ—è¡¨æ•°ç»„ï¼ˆjson_path æŒ‡åˆ°æ•°ç»„ï¼Œfield_mappings æŒ‡å®šå­—æ®µåï¼‰
        2) JSON + HTMLå­—ç¬¦ä¸²ï¼ˆjson_html_field æŒ‡åˆ° HTML å­—æ®µï¼Œhtml_item_selector æå– liï¼‰
        3) çº¯ HTML ç‰‡æ®µï¼ˆhtml_item_selector æå– liï¼‰
        """
        import json

        from parsel import Selector

        site_name = response.meta.get("site_name")
        api_cfg = response.meta.get("api_config") or {}
        resp_type = (api_cfg.get("response_type") or "json").lower()
        items = []
        self.logger.info(f"resp_type: {resp_type} ")

        def parse_li_elements(elements):
            out = []
            for i, el in enumerate(elements):
                title = (
                    el.css("a::attr(title)").get()
                    or (el.css("a::text").get() or "").strip()
                )
                url = el.css("a::attr(href)").get()
                li_text = " ".join(
                    [t.strip() for t in el.css("::text").getall() if t and t.strip()]
                )
                import re

                m = re.search(r"\d{4}-\d{2}-\d{2}", li_text)
                date = m.group(0) if m else None
                if not url:
                    continue
                out.append({"title": title, "url": url, "date": date, "index": i + 1})
            return out

        try:
            if resp_type == "json":
                self.logger.info("json")

                data = json.loads(response.text)
                path = (api_cfg.get("json_path") or "").strip()
                node = data
                if path:
                    for part in path.split("."):
                        if not part:
                            continue
                        if isinstance(node, dict):
                            node = node.get(part)
                        else:
                            node = None
                        if node is None:
                            break
                if isinstance(node, list):
                    fmap = api_cfg.get("field_mappings") or {}
                    url_template = api_cfg.get("url_template")
                    for i, it in enumerate(node):
                        try:
                            title = it.get(fmap.get("title", "title"))
                            url = it.get(fmap.get("url", "url"))
                            date = it.get(fmap.get("date", "date"))

                            # å¤„ç†URLæ¨¡æ¿
                            if url_template and url:
                                temp = url
                                url = url_template.format(url=temp)

                            self.logger.info(
                                f"json title: {title}, url: {url}, date: {date}"
                            )

                            if not url:
                                continue
                            items.append(
                                {
                                    "title": title,
                                    "url": url,
                                    "date": date,
                                    "index": i + 1,
                                }
                            )
                        except Exception:
                            continue
                else:
                    html_field = (api_cfg.get("json_html_field") or "").strip()
                    if html_field:
                        node = data
                        for part in html_field.split("."):
                            if not part:
                                continue
                            if isinstance(node, dict):
                                node = node.get(part)
                            else:
                                node = None
                            if node is None:
                                break
                        if isinstance(node, str) and node.strip():
                            sel = Selector(text=node)
                            li_sel = (
                                api_cfg.get("html_item_selector")
                                or "div.page-content ul li"
                            ).strip()
                            elements = sel.css(li_sel)
                            items = parse_li_elements(elements)
            else:
                sel = Selector(text=response.text)
                li_sel = (
                    api_cfg.get("html_item_selector") or "div.page-content ul li"
                ).strip()
                elements = sel.css(li_sel)
                items = parse_li_elements(elements)
        except Exception as e:
            self.logger.warning(f"âš ï¸ è§£æåˆ—è¡¨APIå¤±è´¥: {e}")

        self.logger.info(f"ğŸ§ª åˆ—è¡¨APIæå–åˆ° {len(items)} é¡¹")
        if items:
            # ç›´æ¥å¤ç”¨ç»Ÿä¸€çš„å¢é‡å¤„ç†é€»è¾‘
            yield from self._handle_list_incremental(response, site_name, items)

    def _follow_links(self, response, site_name: str, extracted_data: Dict):
        """è·Ÿè¿›é“¾æ¥"""
        try:
            # è·å–æå–çš„é“¾æ¥
            links = []

            # ä»æå–çš„æ•°æ®ä¸­è·å–é“¾æ¥
            if "items" in extracted_data:
                for item in extracted_data["items"]:
                    if "url" in item and item["url"]:
                        links.append(item["url"])

            # é™åˆ¶é“¾æ¥æ•°é‡
            max_links = 50  # å¯é…ç½®
            links = links[:max_links]

            logger.info(f"ğŸ”— å‡†å¤‡è·Ÿè¿› {len(links)} ä¸ªé“¾æ¥")

            for link in links:
                absolute_url = urljoin(response.url, link)
                yield scrapy.Request(
                    url=absolute_url,
                    callback=self.parse,
                    meta={"site_name": site_name, "page_type": "detail_page"},
                    errback=self.handle_error,
                )

        except Exception as e:
            logger.error(f"âŒ é“¾æ¥è·Ÿè¿›å¤±è´¥: {e}")

    def handle_error(self, failure):
        """å¤„ç†è¯·æ±‚é”™è¯¯"""
        logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {failure.request.url}")
        logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {failure.value}")

        yield {
            "url": failure.request.url,
            "error": str(failure.value),
            "spider_name": self.name,
            "status": "request_failed",
        }

    def closed(self, reason):
        """çˆ¬è™«å…³é—­æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("ğŸ è‡ªé€‚åº”çˆ¬è™«V2å…³é—­")
        logger.info(f"ğŸ“Š å…³é—­åŸå› : {reason}")

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        stats = self.crawler.stats.get_stats()
        logger.info("ğŸ“ˆ çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            if "count" in key.lower():
                logger.info("    %s: %s", key, value)
