# -*- coding: utf-8 -*-
"""
åŸºç¡€çˆ¬è™«

æœ€ç®€å•çš„çˆ¬è™«ç¤ºä¾‹ï¼Œç”¨äºå­¦ä¹ å’Œæµ‹è¯•
"""

import scrapy
from scrapy.http import Request


class BaseSpider(scrapy.Spider):
    """åŸºç¡€çˆ¬è™«ç±»"""

    name = "base"
    allowed_domains = ["httpbin.org", "example.com"]

    # æµ‹è¯•ç”¨çš„èµ·å§‹URL
    start_urls = [
        "http://httpbin.org/ip",  # è·å–IPä¿¡æ¯
        "http://httpbin.org/headers",  # è·å–è¯·æ±‚å¤´ä¿¡æ¯
        "http://httpbin.org/user-agent",  # è·å–User-Agentä¿¡æ¯
    ]

    custom_settings = {
        "DOWNLOAD_DELAY": 1,
        "CONCURRENT_REQUESTS": 1,
        "ROBOTSTXT_OBEY": False,
        "USER_AGENT": "BaseSpider/1.0 (+http://www.example.com/bot)",
        "LOG_LEVEL": "INFO",
    }

    def start_requests(self):
        """ç”Ÿæˆåˆå§‹è¯·æ±‚"""
        self.logger.info("ğŸš€ å¼€å§‹åŸºç¡€çˆ¬è™«...")

        for i, url in enumerate(self.start_urls):
            self.logger.info(f"ğŸ“‹ å‡†å¤‡è¯·æ±‚ç¬¬{i+1}ä¸ªURL: {url}")
            yield Request(
                url=url,
                callback=self.parse,
                meta={"index": i + 1, "url_type": self.get_url_type(url)},
            )

    def get_url_type(self, url):
        """æ ¹æ®URLåˆ¤æ–­ç±»å‹"""
        if "ip" in url:
            return "ip_info"
        elif "headers" in url:
            return "headers_info"
        elif "user-agent" in url:
            return "user_agent_info"
        else:
            return "unknown"

    def parse(self, response):
        """è§£æå“åº”"""
        index = response.meta.get("index", 0)
        url_type = response.meta.get("url_type", "unknown")

        self.logger.info(f"âœ… ç¬¬{index}ä¸ªè¯·æ±‚æˆåŠŸ")
        self.logger.info(f"ğŸ“Š URL: {response.url}")
        self.logger.info(f"ğŸ“Š çŠ¶æ€ç : {response.status}")
        self.logger.info(f"ğŸ“Š ç±»å‹: {url_type}")
        self.logger.info(f"ğŸ“Š å“åº”å¤§å°: {len(response.body)} bytes")

        # å°è¯•è§£æJSONå“åº”
        try:
            import json

            data = json.loads(response.text)
            self.logger.info(f"ğŸ“„ JSONæ•°æ®: {data}")
        except:
            self.logger.info(f"ğŸ“„ æ–‡æœ¬å†…å®¹: {response.text[:200]}...")

        # è¿”å›ç»“æ„åŒ–æ•°æ®
        yield {
            "index": index,
            "url": response.url,
            "status": response.status,
            "url_type": url_type,
            "content_length": len(response.body),
            "content": response.text[:500] + "..."
            if len(response.text) > 500
            else response.text,
            "timestamp": self.get_timestamp(),
            "spider_name": self.name,
        }

        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè¯·æ±‚ï¼Œç”Ÿæˆé¢å¤–çš„æµ‹è¯•è¯·æ±‚
        if index == 1:
            self.logger.info("ğŸ”„ ç”Ÿæˆé¢å¤–çš„æµ‹è¯•è¯·æ±‚...")
            extra_urls = [
                "http://httpbin.org/json",
                "http://httpbin.org/html",
            ]

            for extra_url in extra_urls:
                yield Request(
                    url=extra_url,
                    callback=self.parse_extra,
                    meta={"source": "extra_request"},
                )

    def parse_extra(self, response):
        """è§£æé¢å¤–è¯·æ±‚çš„å“åº”"""
        self.logger.info(f"ğŸ” é¢å¤–è¯·æ±‚å®Œæˆ: {response.url}")
        self.logger.info(f"ğŸ“Š çŠ¶æ€ç : {response.status}")

        yield {
            "type": "extra_request",
            "url": response.url,
            "status": response.status,
            "content_type": response.headers.get("Content-Type", b"").decode(),
            "content_preview": response.text[:200] + "..."
            if len(response.text) > 200
            else response.text,
            "timestamp": self.get_timestamp(),
            "spider_name": self.name,
        }

    def get_timestamp(self):
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        import datetime

        return datetime.datetime.now().isoformat()

    def closed(self, reason):
        """çˆ¬è™«å…³é—­æ—¶çš„å¤„ç†"""
        self.logger.info(f"ğŸ åŸºç¡€çˆ¬è™«å®Œæˆï¼Œå…³é—­åŸå› : {reason}")

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        stats = self.crawler.stats.get_stats()
        self.logger.info("ğŸ“ˆ çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            if isinstance(value, (int, float, str)):
                self.logger.info(f"   {key}: {value}")


# ä¸ºäº†å…¼å®¹æ€§ï¼Œä¹Ÿåˆ›å»ºä¸€ä¸ªåˆ«å
class BaseCrawlerSpider(BaseSpider):
    """åŸºç¡€çˆ¬è™«åˆ«å"""

    name = "base_crawler"
