# -*- coding: utf-8 -*-
"""
è‡ªé€‚åº”çˆ¬è™«

åŸºäºé…ç½®è§„åˆ™çš„é€šç”¨çˆ¬è™«ï¼Œå¯ä»¥é€‚åº”ä¸åŒç½‘ç«™ç»“æ„
"""

import scrapy
from scrapy.http import Request

from ..rule_engine import RuleEngine


class AdaptiveSpider(scrapy.Spider):
    """è‡ªé€‚åº”çˆ¬è™« - æ ¹æ®é…ç½®è§„åˆ™è‡ªåŠ¨é€‚åº”ä¸åŒç½‘ç«™"""

    name = "adaptive"

    # é»˜è®¤è®¾ç½®ï¼ˆç±»çº§åˆ«ï¼‰
    custom_settings = {
        "ROBOTSTXT_OBEY": False,
        "LOG_LEVEL": "INFO",
        "DOWNLOAD_DELAY": 1,
        "CONCURRENT_REQUESTS": 1,
    }

    def __init__(self, site=None, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # åˆå§‹åŒ–è§„åˆ™å¼•æ“
        self.rule_engine = RuleEngine()

        # æŒ‡å®šè¦çˆ¬å–çš„ç½‘ç«™
        self.target_site = site
        if not site:
            raise ValueError("å¿…é¡»æŒ‡å®šè¦çˆ¬å–çš„ç½‘ç«™ï¼Œä¾‹å¦‚: scrapy crawl adaptive -a site=nhc_new")

        # åŠ è½½ç½‘ç«™è§„åˆ™
        self.site_rule = self.rule_engine.get_rule(site)
        if not self.site_rule:
            raise ValueError(f"æœªæ‰¾åˆ°ç½‘ç«™ '{site}' çš„é…ç½®è§„åˆ™")

        self.logger.info(f"ğŸ¯ ä½¿ç”¨ç½‘ç«™è§„åˆ™: {site}")
        site_info = self.site_rule.get("site_info", {})
        description = site_info.get("description", "æ— æè¿°")
        self.logger.info(f"ğŸ“‹ ç½‘ç«™æè¿°: {description}")

        # è®¾ç½®å…è®¸çš„åŸŸå
        self.allowed_domains = self.site_rule.get("allowed_domains", [])

        # è®¾ç½®èµ·å§‹URL
        start_url_configs = self.site_rule.get("target_pages", [])
        self.start_urls = []
        for url_config in start_url_configs:
            if isinstance(url_config, dict):
                self.start_urls.append(url_config["url"])
            else:
                self.start_urls.append(url_config)

        self.logger.info(f"ğŸš€ èµ·å§‹URLæ•°é‡: {len(self.start_urls)}")

        # åŠ¨æ€æ›´æ–°è®¾ç½®
        self._update_settings_from_rule()

    def _update_settings_from_rule(self):
        """æ ¹æ®è§„åˆ™æ›´æ–°çˆ¬è™«è®¾ç½®"""
        request_settings = self.site_rule.get("request_settings", {})

        # æ›´æ–°ç±»çº§åˆ«çš„custom_settings
        if "download_delay" in request_settings:
            self.custom_settings["DOWNLOAD_DELAY"] = request_settings["download_delay"]

        if "concurrent_requests" in request_settings:
            self.custom_settings["CONCURRENT_REQUESTS"] = request_settings[
                "concurrent_requests"
            ]

        if "user_agent" in request_settings:
            self.custom_settings["USER_AGENT"] = request_settings["user_agent"]

    async def start(self):
        """ç”Ÿæˆåˆå§‹è¯·æ±‚ - æ–°çš„å¼‚æ­¥æ–¹æ³•"""
        for i, url in enumerate(self.start_urls):
            self.logger.info(f"ğŸ“‹ å‡†å¤‡è¯·æ±‚ç¬¬{i+1}ä¸ªURL: {url}")

            # è·å–è¯·æ±‚å¤´è®¾ç½®
            headers = self.site_rule.get("request_settings", {}).get("headers", {})
            self.logger.info(f"ğŸ“‹ è¯·æ±‚å¤´è®¾ç½®: {headers}")
            yield Request(
                url=url,
                callback=self.parse,
                headers=headers,
                meta={
                    "site": self.target_site,
                    "url_index": i,
                    "page_type": "start_page",
                },
                errback=self.parse_error,  # æ·»åŠ é”™è¯¯å¤„ç†
            )

    def start_requests(self):
        """ä¿æŒå‘åå…¼å®¹çš„start_requestsæ–¹æ³•"""
        # åŒæ­¥ç‰ˆæœ¬ï¼Œç”¨äºå…¼å®¹æ—§ç‰ˆæœ¬Scrapy
        for i, url in enumerate(self.start_urls):
            self.logger.info(f"ğŸ“‹ å‡†å¤‡è¯·æ±‚ç¬¬{i+1}ä¸ªURL: {url}")

            # è·å–è¯·æ±‚å¤´è®¾ç½®
            headers = self.site_rule.get("request_settings", {}).get("headers", {})

            yield Request(
                url=url,
                callback=self.parse,
                headers=headers,
                meta={
                    "site": self.target_site,
                    "url_index": i,
                    "page_type": "start_page",
                },
                errback=self.parse_error,  # æ·»åŠ é”™è¯¯å¤„ç†
            )

    def parse(self, response):
        """è§£æå“åº”"""
        self.logger.info(f"âœ… ğŸ‰ PARSEæ–¹æ³•è¢«è°ƒç”¨! è§£æé¡µé¢: {response.url}")
        self.logger.info(f"ğŸ“Š çŠ¶æ€ç : {response.status}")
        self.logger.info(f"ğŸ“ å“åº”å¤§å°: {len(response.body)} å­—èŠ‚")
        self.logger.info(f"ğŸ¯ ç›®æ ‡ç½‘ç«™: {self.target_site}")

        # æ£€æŸ¥Content-Type
        content_type = response.headers.get("Content-Type", b"").decode(
            "utf-8", errors="ignore"
        )
        self.logger.info(f"ğŸ“‹ Content-Type: {content_type}")

        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status >= 400:
            self.logger.error(f"âŒ HTTPé”™è¯¯çŠ¶æ€ç : {response.status}")
            yield {
                "url": response.url,
                "status": response.status,
                "error": f"HTTP {response.status}",
                "spider_name": self.name,
                "site": self.target_site,
                "content_type": content_type,
            }
            return

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æœ¬å†…å®¹
        if not self._is_text_response(response):
            self.logger.warning(f"âš ï¸ éæ–‡æœ¬å“åº”ï¼Œè·³è¿‡å¤„ç†: {response.url}")
            self.logger.warning(f"âš ï¸ Content-Type: {content_type}")
            yield {
                "url": response.url,
                "status": "non_text_content",
                "error": f"Non-text content: {content_type}",
                "spider_name": self.name,
                "site": self.target_site,
                "content_type": content_type,
            }
            return

        # è°ƒè¯•ï¼šæ˜¾ç¤ºé¡µé¢å†…å®¹ç‰‡æ®µ
        try:
            content_preview = response.text[:300] if response.text else "æ— å†…å®¹"
            self.logger.info(f"ğŸ“„ é¡µé¢å†…å®¹é¢„è§ˆ: {content_preview}...")
        except Exception as e:
            self.logger.error(f"âŒ æ— æ³•è·å–æ–‡æœ¬å†…å®¹: {e}")
            yield {
                "url": response.url,
                "status": "text_decode_error",
                "error": f"Text decode error: {e}",
                "spider_name": self.name,
                "site": self.target_site,
            }
            return

        # æ™ºèƒ½åˆ¤æ–­é¡µé¢ç±»å‹
        page_type = self._detect_page_type(response)
        self.logger.info(f"ğŸ” æ£€æµ‹åˆ°é¡µé¢ç±»å‹: {page_type}")

        # æ ¹æ®é…ç½®æ–‡ä»¶çš„å­—æ®µè§„åˆ™è¿›è¡Œæ•°æ®æå–
        try:
            self.logger.info("ğŸ”§ å¼€å§‹æ ¹æ®é…ç½®æ–‡ä»¶æå–æ•°æ®...")

            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„fieldsè§„åˆ™è¿›è¡Œæå–
            data = self._extract_data_by_config(response, page_type)

            # æ·»åŠ å…ƒæ•°æ®
            data["spider_name"] = self.name
            data["site"] = self.target_site
            data["page_type"] = page_type
            data["content_type"] = content_type

            self.logger.info("ğŸ“„ æå–æ•°æ®å­—æ®µ: %s", list(data.keys()))

            # è°ƒè¯•ï¼šæ˜¾ç¤ºæå–çš„å…³é”®æ•°æ®
            if data.get("title"):
                self.logger.info(f"ğŸ“ æå–åˆ°æ ‡é¢˜: {data['title']}")
            else:
                self.logger.warning("âš ï¸ æœªæå–åˆ°æ ‡é¢˜")

            if data.get("content"):
                content_length = len(str(data["content"]))
                self.logger.info("ğŸ“ æå–åˆ°å†…å®¹: %s å­—ç¬¦", content_length)
            else:
                self.logger.warning("âš ï¸ æœªæå–åˆ°å†…å®¹")

            self.logger.info("ğŸ“¤ å‡†å¤‡yieldæ•°æ®é¡¹")
            yield data

        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®æå–å¤±è´¥: {e}")
            import traceback

            self.logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            yield {
                "url": response.url,
                "error": str(e),
                "status": "extraction_failed",
                "spider_name": self.name,
                "site": self.target_site,
                "content_type": content_type,
            }

        # æå–å¹¶è·Ÿè¿›é“¾æ¥
        try:
            links = self.rule_engine.get_links(response, self.site_rule)
            self.logger.info(f"ğŸ”— æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")

            for link_info in links:
                url = link_info["url"]
                link_type = link_info["type"]

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·Ÿè¿›
                if self.rule_engine.should_follow(url, self.site_rule):
                    self.logger.debug(f"ğŸ”„ è·Ÿè¿›é“¾æ¥ ({link_type}): {url}")

                    yield Request(
                        url=url,
                        callback=self.parse,
                        meta={
                            "site": self.target_site,
                            "link_type": link_type,
                            "page_type": "followed_page",
                            "source_url": response.url,
                        },
                        errback=self.parse_error,
                    )
                else:
                    self.logger.debug(f"â­ï¸ è·³è¿‡é“¾æ¥: {url}")

        except Exception as e:
            self.logger.error(f"âŒ é“¾æ¥æå–å¤±è´¥: {e}")

    def _is_text_response(self, response):
        """æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºæ–‡æœ¬å†…å®¹"""
        content_type = (
            response.headers.get("Content-Type", b"")
            .decode("utf-8", errors="ignore")
            .lower()
        )

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æœ¬ç±»å‹
        text_types = [
            "text/html",
            "text/plain",
            "text/xml",
            "application/xml",
            "application/xhtml+xml",
            "application/json",
        ]

        for text_type in text_types:
            if text_type in content_type:
                return True

        # å¦‚æœæ²¡æœ‰Content-Typeå¤´ï¼Œå°è¯•æ£€æŸ¥å†…å®¹
        if not content_type:
            try:
                # å°è¯•è§£ç å‰100å­—èŠ‚
                sample = response.body[:100].decode("utf-8", errors="ignore")
                # æ£€æŸ¥æ˜¯å¦åŒ…å«HTMLæ ‡ç­¾
                return "<" in sample and ">" in sample
            except Exception:
                return False

        return False

    def _detect_page_type(self, response):
        """æ™ºèƒ½æ£€æµ‹é¡µé¢ç±»å‹"""
        try:
            # è·å–é¡µé¢æ–‡æœ¬å†…å®¹
            url = response.url.lower()

            # 1. æ ¹æ®URLè·¯å¾„åˆ¤æ–­
            if any(
                keyword in url for keyword in ["list", "index", "category", "åˆ—è¡¨", "ç›®å½•"]
            ):
                # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯åˆ—è¡¨é¡µ
                if self._is_list_page_content(response):
                    return "list_page"

            if any(
                keyword in url
                for keyword in [
                    "detail",
                    "article",
                    "news",
                    "content",
                    "è¯¦æƒ…",
                    "æ–°é—»",
                    "æ–‡ç« ",
                ]
            ):
                # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯è¯¦æƒ…é¡µ
                if self._is_detail_page_content(response):
                    return "detail_page"

            # 2. æ ¹æ®é¡µé¢å†…å®¹ç‰¹å¾åˆ¤æ–­
            if self._is_list_page_content(response):
                return "list_page"
            elif self._is_detail_page_content(response):
                return "detail_page"

            # 3. é»˜è®¤åˆ¤æ–­
            return "unknown_page"

        except Exception as e:
            self.logger.warning(f"âš ï¸ é¡µé¢ç±»å‹æ£€æµ‹å¤±è´¥: {e}")
            return "unknown_page"

    def _is_list_page_content(self, response):
        """æ£€æµ‹æ˜¯å¦ä¸ºåˆ—è¡¨é¡µå†…å®¹"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªé“¾æ¥é¡¹
            links = response.css("a::attr(href)").getall()
            if len(links) < 5:  # åˆ—è¡¨é¡µé€šå¸¸æœ‰å¤šä¸ªé“¾æ¥
                return False

            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ—è¡¨ç›¸å…³çš„HTMLç»“æ„
            list_indicators = [
                "ul li a",  # æ— åºåˆ—è¡¨ä¸­çš„é“¾æ¥
                "ol li a",  # æœ‰åºåˆ—è¡¨ä¸­çš„é“¾æ¥
                ".list",  # åŒ…å«listç±»åçš„å…ƒç´ 
                ".news-list",
                ".content_list",
                '[class*="list"]',
            ]

            for indicator in list_indicators:
                elements = response.css(indicator)
                if len(elements) >= 3:  # è‡³å°‘3ä¸ªåˆ—è¡¨é¡¹
                    self.logger.debug(f"ğŸ” å‘ç°åˆ—è¡¨ç»“æ„: {indicator} ({len(elements)}ä¸ª)")
                    return True

            # æ£€æŸ¥æ˜¯å¦æœ‰æ—¥æœŸæ¨¡å¼ï¼ˆåˆ—è¡¨é¡µå¸¸æœ‰å‘å¸ƒæ—¥æœŸï¼‰
            date_patterns = response.css("*::text()").re(
                r"\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?"
            )
            if len(date_patterns) >= 3:  # å¤šä¸ªæ—¥æœŸè¡¨ç¤ºå¯èƒ½æ˜¯åˆ—è¡¨é¡µ
                self.logger.debug(f"ğŸ” å‘ç°å¤šä¸ªæ—¥æœŸæ¨¡å¼: {len(date_patterns)}ä¸ª")
                return True

            return False

        except Exception as e:
            self.logger.warning(f"âš ï¸ åˆ—è¡¨é¡µæ£€æµ‹å¤±è´¥: {e}")
            return False

    def _is_detail_page_content(self, response):
        """æ£€æµ‹æ˜¯å¦ä¸ºè¯¦æƒ…é¡µå†…å®¹"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¯¦æƒ…é¡µçš„å…¸å‹ç»“æ„
            detail_indicators = [
                "article",  # HTML5 articleæ ‡ç­¾
                ".article",
                ".content",
                ".detail",
                ".news-content",
                '[class*="content"]',
                '[class*="article"]',
                '[class*="detail"]',
            ]

            for indicator in detail_indicators:
                elements = response.css(indicator)
                if elements:
                    # æ£€æŸ¥å†…å®¹é•¿åº¦
                    content_text = " ".join(elements.css("*::text()").getall())
                    if len(content_text) > 200:  # è¯¦æƒ…é¡µé€šå¸¸æœ‰è¾ƒé•¿å†…å®¹
                        self.logger.debug(
                            f"ğŸ” å‘ç°è¯¦æƒ…é¡µç»“æ„: {indicator} (å†…å®¹é•¿åº¦: {len(content_text)})"
                        )
                        return True

            # æ£€æŸ¥é¡µé¢æ–‡æœ¬æ€»é•¿åº¦
            all_text = " ".join(response.css("*::text()").getall())
            if len(all_text) > 1000:  # è¯¦æƒ…é¡µé€šå¸¸å†…å®¹è¾ƒå¤š
                # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡é¢˜
                title_selectors = [
                    "h1",
                    "h2",
                    ".title",
                    ".headline",
                    '[class*="title"]',
                ]
                for selector in title_selectors:
                    if response.css(selector):
                        self.logger.debug(f"ğŸ” å‘ç°è¯¦æƒ…é¡µç‰¹å¾: é•¿å†…å®¹({len(all_text)}) + æ ‡é¢˜ç»“æ„")
                        return True

            return False

        except Exception as e:
            self.logger.warning(f"âš ï¸ è¯¦æƒ…é¡µæ£€æµ‹å¤±è´¥: {e}")
            return False

    def _extract_data_by_config(self, response, page_type):
        """æ ¹æ®é…ç½®æ–‡ä»¶çš„fieldsè§„åˆ™æå–æ•°æ®"""
        data = {"url": response.url}

        # è·å–é…ç½®æ–‡ä»¶ä¸­çš„å­—æ®µè§„åˆ™
        fields_config = self.site_rule.get("fields", {})
        if not fields_config:
            self.logger.warning("âš ï¸ é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°fieldsè§„åˆ™ï¼Œä½¿ç”¨é»˜è®¤æå–æ–¹æ³•")
            return self._extract_default_data(response, page_type)

        self.logger.info(f"ğŸ“‹ ä½¿ç”¨é…ç½®æ–‡ä»¶æå– {len(fields_config)} ä¸ªå­—æ®µ")

        # éå†æ¯ä¸ªå­—æ®µé…ç½®è¿›è¡Œæå–
        for field_name, field_config in fields_config.items():
            try:
                value = self._extract_field_by_config(
                    response, field_name, field_config, page_type
                )
                if value is not None:
                    data[field_name] = value
                    self.logger.debug(f"âœ… å­—æ®µ {field_name}: {str(value)[:100]}...")
                else:
                    self.logger.debug(f"âš ï¸ å­—æ®µ {field_name}: æœªæå–åˆ°å€¼")

            except Exception as e:
                self.logger.error(f"âŒ æå–å­—æ®µ {field_name} å¤±è´¥: {e}")
                data[f"{field_name}_error"] = str(e)

        self.logger.info(
            f"ğŸ“Š é…ç½®æ–‡ä»¶æå–å®Œæˆ: {len([k for k in data.keys() if not k.endswith('_error')])} ä¸ªå­—æ®µæˆåŠŸ"
        )
        return data

    def _extract_field_by_config(self, response, field_name, field_config, page_type):
        """æ ¹æ®å­—æ®µé…ç½®æå–å•ä¸ªå­—æ®µ"""
        method = field_config.get("method", "xpath")
        selector = field_config.get("selector", "")
        field_type = field_config.get("type", "string")
        multiple = field_config.get("multiple", False)
        required = field_config.get("required", False)

        if not selector:
            if required:
                self.logger.warning(f"âš ï¸ å¿…éœ€å­—æ®µ {field_name} ç¼ºå°‘é€‰æ‹©å™¨")
            return None

        try:
            # æ ¹æ®æ–¹æ³•ç±»å‹è¿›è¡Œæå–
            if method == "xpath":
                if multiple:
                    values = response.xpath(selector).getall()
                else:
                    values = response.xpath(selector).get()
            elif method == "css":
                if multiple:
                    values = response.css(selector).getall()
                else:
                    values = response.css(selector).get()
            elif method == "regex":
                import re

                text_content = response.text
                if multiple:
                    values = re.findall(selector, text_content)
                else:
                    match = re.search(selector, text_content)
                    values = match.group(1) if match else None
            else:
                self.logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æå–æ–¹æ³•: {method}")
                return None

            # æ•°æ®æ¸…æ´—å’Œç±»å‹è½¬æ¢
            if values:
                cleaned_values = self._clean_extracted_values(
                    values, field_type, multiple
                )

                # ç‰¹æ®Šå¤„ç†ï¼šæ ¹æ®é¡µé¢ç±»å‹è°ƒæ•´æå–ç­–ç•¥
                if page_type == "list_page" and field_name in [
                    "news_links",
                    "news_titles",
                    "publish_dates",
                ]:
                    # åˆ—è¡¨é¡µçš„ç‰¹æ®Šå¤„ç†
                    cleaned_values = self._process_list_page_field(
                        response, field_name, cleaned_values
                    )
                elif page_type == "detail_page" and field_name in ["content", "title"]:
                    # è¯¦æƒ…é¡µçš„ç‰¹æ®Šå¤„ç†
                    cleaned_values = self._process_detail_page_field(
                        response, field_name, cleaned_values
                    )

                return cleaned_values
            else:
                if required:
                    self.logger.warning(f"âš ï¸ å¿…éœ€å­—æ®µ {field_name} æœªæå–åˆ°å€¼")
                return None

        except Exception as e:
            self.logger.error(f"âŒ å­—æ®µ {field_name} æå–å¼‚å¸¸: {e}")
            return None

    def _clean_extracted_values(self, values, field_type, multiple):
        """æ¸…æ´—æå–çš„å€¼"""
        if multiple and isinstance(values, list):
            # æ¸…æ´—åˆ—è¡¨ä¸­çš„æ¯ä¸ªå€¼
            cleaned = []
            for value in values:
                if isinstance(value, str):
                    cleaned_value = value.strip()
                    if cleaned_value:  # åªä¿ç•™éç©ºå€¼
                        cleaned.append(cleaned_value)
                else:
                    cleaned.append(value)
            return cleaned
        else:
            # æ¸…æ´—å•ä¸ªå€¼
            if isinstance(values, str):
                cleaned = values.strip()

                # æ ¹æ®å­—æ®µç±»å‹è¿›è¡Œè½¬æ¢
                if field_type == "integer":
                    try:
                        return int(cleaned) if cleaned else None
                    except ValueError:
                        return None
                elif field_type == "date":
                    # ç®€å•çš„æ—¥æœŸæ¸…æ´—
                    import re

                    date_match = re.search(
                        r"\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?", cleaned
                    )
                    return date_match.group(0) if date_match else cleaned

                return cleaned if cleaned else None
            else:
                return values

    def _process_list_page_field(self, response, field_name, values):
        """å¤„ç†åˆ—è¡¨é¡µç‰¹å®šå­—æ®µ"""
        if field_name == "news_links" and values:
            # è½¬æ¢ä¸ºç»å¯¹URL
            absolute_urls = []
            for url in values:
                if url:
                    absolute_url = response.urljoin(url)
                    absolute_urls.append(absolute_url)
            return absolute_urls

        return values

    def _process_detail_page_field(self, response, field_name, values):
        """å¤„ç†è¯¦æƒ…é¡µç‰¹å®šå­—æ®µ"""
        if field_name == "content" and isinstance(values, list):
            # å°†å¤šä¸ªæ®µè½åˆå¹¶ä¸ºå®Œæ•´å†…å®¹
            content_text = " ".join([text.strip() for text in values if text.strip()])
            return content_text if content_text else None

        return values

    def _extract_default_data(self, response, page_type):
        """å½“é…ç½®æ–‡ä»¶ç¼ºå¤±æ—¶çš„é»˜è®¤æå–æ–¹æ³•"""
        if page_type == "list_page":
            return self._extract_list_page_data(response)
        elif page_type == "detail_page":
            return self._extract_detail_page_data(response)
        else:
            # ä½¿ç”¨è§„åˆ™å¼•æ“
            return self.rule_engine.extract_data(response, self.site_rule)

    def _extract_list_page_data(self, response):
        """æå–åˆ—è¡¨é¡µæ•°æ®"""
        data = {"url": response.url}

        try:
            # æå–é¡µé¢æ ‡é¢˜
            title_selectors = ["title", "h1", "h2", ".title", ".page-title"]
            for selector in title_selectors:
                title_elements = response.css(f"{selector}::text()").getall()
                if title_elements:
                    data["title"] = title_elements[0].strip()
                    break

            # æå–æ–°é—»åˆ—è¡¨
            news_items = []

            # å°è¯•å¤šç§åˆ—è¡¨é€‰æ‹©å™¨
            list_selectors = [
                "ul.content_list li",  # åŒ—äº¬ç–¾æ§ä¸­å¿ƒçš„ç»“æ„
                "ul li a",
                ".news-list li",
                ".list-item",
                '[class*="list"] li',
            ]

            for selector in list_selectors:
                items = response.css(selector)
                if len(items) >= 3:  # è‡³å°‘3ä¸ªé¡¹ç›®æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆåˆ—è¡¨
                    self.logger.info(f"ğŸ“‹ ä½¿ç”¨é€‰æ‹©å™¨æå–åˆ—è¡¨: {selector} ({len(items)}é¡¹)")

                    for i, item in enumerate(items[:20]):  # æœ€å¤šæå–20é¡¹
                        try:
                            # æå–é“¾æ¥
                            link = item.css("a::attr(href)").get()
                            if link:
                                link = response.urljoin(link)

                            # æå–æ ‡é¢˜
                            title = item.css("a::text()").get()
                            if title:
                                title = title.strip()

                            # æå–æ—¥æœŸ
                            date = item.css("span::text()").get()
                            if date:
                                date = date.strip()

                            if link and title:
                                news_items.append(
                                    {
                                        "title": title,
                                        "url": link,
                                        "date": date,
                                        "index": i + 1,
                                    }
                                )
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ æå–ç¬¬{i+1}é¡¹å¤±è´¥: {e}")
                            continue

                    if news_items:
                        break

            data["news_items"] = news_items
            data["news_count"] = len(news_items)

            # æå–é¡µé¢å†…å®¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            content_selectors = [".content", ".main-content", "#content"]
            for selector in content_selectors:
                content_elements = response.css(f"{selector} *::text()").getall()
                if content_elements:
                    data["content"] = " ".join(
                        [text.strip() for text in content_elements if text.strip()]
                    )
                    break

            self.logger.info(
                f"ğŸ“‹ åˆ—è¡¨é¡µæå–å®Œæˆ: æ ‡é¢˜={data.get('title', 'æ— ')}, æ–°é—»æ•°={len(news_items)}"
            )
            return data

        except Exception as e:
            self.logger.error(f"âŒ åˆ—è¡¨é¡µæ•°æ®æå–å¤±è´¥: {e}")
            return {"url": response.url, "error": f"List page extraction failed: {e}"}

    def _extract_detail_page_data(self, response):
        """æå–è¯¦æƒ…é¡µæ•°æ®"""
        data = {"url": response.url}

        try:
            # æå–æ ‡é¢˜
            title_selectors = [
                "h1",
                "h2",
                ".title",
                ".article-title",
                ".news-title",
                "title",
            ]
            for selector in title_selectors:
                title_elements = response.css(f"{selector}::text()").getall()
                if title_elements:
                    data["title"] = title_elements[0].strip()
                    break

            # æå–å†…å®¹
            content_selectors = [
                ".article-content",
                ".content",
                ".news-content",
                ".detail-content",
                "article",
                "#content",
                ".main-content",
            ]

            for selector in content_selectors:
                content_elements = response.css(f"{selector} p::text()").getall()
                if not content_elements:
                    content_elements = response.css(f"{selector} *::text()").getall()

                if content_elements:
                    content_text = " ".join(
                        [text.strip() for text in content_elements if text.strip()]
                    )
                    if len(content_text) > 50:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                        data["content"] = content_text
                        break

            # æå–å‘å¸ƒæ—¥æœŸ
            date_selectors = [".date", ".time", ".publish-time", ".create-time"]
            for selector in date_selectors:
                date_elements = response.css(f"{selector}::text()").getall()
                if date_elements:
                    data["publish_date"] = date_elements[0].strip()
                    break

            # æå–ä½œè€…
            author_selectors = [".author", ".writer", ".source"]
            for selector in author_selectors:
                author_elements = response.css(f"{selector}::text()").getall()
                if author_elements:
                    data["author"] = author_elements[0].strip()
                    break

            self.logger.info(
                f"ğŸ“„ è¯¦æƒ…é¡µæå–å®Œæˆ: æ ‡é¢˜={data.get('title', 'æ— ')}, å†…å®¹é•¿åº¦={len(data.get('content', ''))}"
            )
            return data

        except Exception as e:
            self.logger.error(f"âŒ è¯¦æƒ…é¡µæ•°æ®æå–å¤±è´¥: {e}")
            return {"url": response.url, "error": f"Detail page extraction failed: {e}"}

    def parse_error(self, failure):
        """å¤„ç†è¯·æ±‚é”™è¯¯"""
        self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {failure.request.url}")
        self.logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(failure.value).__name__}")
        self.logger.error(f"âŒ é”™è¯¯ä¿¡æ¯: {failure.value}")

        # æ£€æŸ¥æ˜¯å¦ä¸ºç‰¹å®šçš„Scrapyé”™è¯¯
        error_type = type(failure.value).__name__
        error_message = str(failure.value)

        # ç‰¹æ®Šå¤„ç†"Response content isn't text"é”™è¯¯
        if "Response content isn't text" in error_message:
            self.logger.warning("âš ï¸ æ£€æµ‹åˆ°éæ–‡æœ¬å“åº”é”™è¯¯ï¼Œå¯èƒ½æ˜¯ç½‘ç«™è¿”å›äº†äºŒè¿›åˆ¶å†…å®¹")
            yield {
                "url": failure.request.url,
                "error": "Non-text response detected",
                "error_type": "content_type_error",
                "status": "non_text_content",
                "spider_name": self.name,
                "site": self.target_site,
                "suggestion": "æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç½‘ç«™æ˜¯å¦è¿”å›äº†å›¾ç‰‡/PDFç­‰äºŒè¿›åˆ¶æ–‡ä»¶",
            }
        else:
            yield {
                "url": failure.request.url,
                "error": error_message,
                "error_type": error_type,
                "status": "request_failed",
                "spider_name": self.name,
                "site": self.target_site,
            }

    def closed(self, reason):
        """çˆ¬è™«å…³é—­æ—¶çš„å¤„ç†"""
        self.logger.info("ğŸ è‡ªé€‚åº”çˆ¬è™«å®Œæˆ")
        self.logger.info(f"ğŸ“Š å…³é—­åŸå› : {reason}")
        self.logger.info(f"ğŸ¯ ç›®æ ‡ç½‘ç«™: {self.target_site}")

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        stats = self.crawler.stats.get_stats()
        self.logger.info("ğŸ“ˆ çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯:")

        key_stats = [
            "item_scraped_count",
            "response_received_count",
            "request_count",
            "downloader/response_status_count/200",
            "downloader/response_status_count/404",
            "elapsed_time_seconds",
        ]

        for key in key_stats:
            if key in stats:
                self.logger.info(f"   {key}: {stats[key]}")


class MultiSiteSpider(scrapy.Spider):
    """å¤šç«™ç‚¹çˆ¬è™« - å¯ä»¥åŒæ—¶çˆ¬å–å¤šä¸ªç½‘ç«™"""

    name = "multisite"

    def __init__(self, sites=None, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # åˆå§‹åŒ–è§„åˆ™å¼•æ“
        self.rule_engine = RuleEngine()

        # è§£æè¦çˆ¬å–çš„ç½‘ç«™åˆ—è¡¨
        if not sites:
            # é»˜è®¤çˆ¬å–æ‰€æœ‰é…ç½®çš„ç½‘ç«™
            self.target_sites = list(self.rule_engine.rules.keys())
        else:
            self.target_sites = sites.split(",")

        self.logger.info(f"ğŸ¯ ç›®æ ‡ç½‘ç«™: {self.target_sites}")

        # æ”¶é›†æ‰€æœ‰èµ·å§‹URL
        self.start_urls = []
        self.allowed_domains = []

        for site in self.target_sites:
            rule = self.rule_engine.get_rule(site)
            if rule:
                # æ·»åŠ åŸŸå
                domains = rule.get("allowed_domains", [])
                self.allowed_domains.extend(domains)

                # æ·»åŠ èµ·å§‹URL
                start_url_configs = rule.get("start_urls", [])
                for url_config in start_url_configs:
                    if isinstance(url_config, dict):
                        self.start_urls.append(url_config["url"])
                    else:
                        self.start_urls.append(url_config)

        self.logger.info(f"ğŸš€ æ€»èµ·å§‹URLæ•°é‡: {len(self.start_urls)}")
        self.logger.info(f"ğŸŒ å…è®¸åŸŸå: {self.allowed_domains}")

    def parse(self, response):
        """è§£æå“åº” - è‡ªåŠ¨è¯†åˆ«ç½‘ç«™ç±»å‹"""
        # æ ¹æ®URLåŒ¹é…ç½‘ç«™è§„åˆ™
        site_name = self.rule_engine.match_site(response.url)

        if not site_name:
            self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„ç½‘ç«™è§„åˆ™: {response.url}")
            return

        self.logger.info(f"ğŸ¯ è¯†åˆ«ç½‘ç«™: {site_name} - {response.url}")

        # ä½¿ç”¨å¯¹åº”çš„è§„åˆ™å¤„ç†
        rule = self.rule_engine.get_rule(site_name)

        # æå–æ•°æ®
        data = self.rule_engine.extract_data(response, rule)
        data["spider_name"] = self.name
        data["site"] = site_name
        data["auto_detected"] = True

        yield data

        # æå–å¹¶è·Ÿè¿›é“¾æ¥
        links = self.rule_engine.get_links(response, rule)
        for link_info in links:
            url = link_info["url"]
            if self.rule_engine.should_follow(url, rule):
                yield response.follow(url, callback=self.parse)
