# -*- coding: utf-8 -*-
"""
æ•°æ®ç®¡é“æ¨¡å—

åŒ…å«å„ç§æ•°æ®å¤„ç†ç®¡é“ï¼š
- æ•°æ®éªŒè¯ç®¡é“
- æ•°æ®æ¸…æ´—ç®¡é“
- å»é‡ç®¡é“
- å­˜å‚¨ç®¡é“
"""

import hashlib
import json
import logging
import re

from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem

logger = logging.getLogger(__name__)


class ValidationPipeline:
    """æ•°æ®éªŒè¯ç®¡é“"""

    def process_item(self, item, spider):
        """éªŒè¯æ•°æ®é¡¹"""
        adapter = ItemAdapter(item)

        # æ ¹æ®Itemç±»å‹ç¡®å®šå¿…éœ€å­—æ®µ
        item_type = type(item).__name__

        if item_type == "NewsItem":
            required_fields = ["url", "title", "crawl_time"]
            url_field = "url"
        elif item_type == "EpidemicDataItem":
            required_fields = ["source_url", "title", "crawl_time"]
            url_field = "source_url"
        elif item_type == "PolicyItem":
            required_fields = ["url", "title", "crawl_time"]
            url_field = "url"
        elif item_type == "StatisticsItem":
            required_fields = ["source_url", "crawl_time"]
            url_field = "source_url"
        else:
            # é»˜è®¤éªŒè¯
            required_fields = ["crawl_time"]
            url_field = None

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if not adapter.get(field):
                raise DropItem(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field} (Itemç±»å‹: {item_type})")

        # éªŒè¯URLæ ¼å¼
        if url_field:
            url_value = adapter.get(url_field)
            if url_value and not url_value.startswith(("http://", "https://")):
                raise DropItem(f"æ— æ•ˆçš„URLæ ¼å¼: {url_value} (å­—æ®µ: {url_field})")

        logger.debug(
            f"æ•°æ®éªŒè¯é€šè¿‡: {item_type} - {adapter.get(url_field) if url_field else 'No URL'}"
        )
        return item


class CleaningPipeline:
    """æ•°æ®æ¸…æ´—ç®¡é“"""

    def process_item(self, item, spider):
        """æ¸…æ´—æ•°æ®é¡¹"""
        adapter = ItemAdapter(item)

        # æ¸…æ´—æ–‡æœ¬å­—æ®µ
        text_fields = ["title", "content", "region"]
        for field in text_fields:
            value = adapter.get(field)
            if value:
                # å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
                cleaned_value = " ".join(value.split())
                adapter[field] = cleaned_value

        # æ¸…æ´—æ•°å­—å­—æ®µ
        number_fields = ["confirmed_cases", "death_cases", "recovered_cases"]
        for field in number_fields:
            value = adapter.get(field)
            if value is not None:
                try:
                    # ç¡®ä¿æ˜¯æ•´æ•°
                    adapter[field] = int(value)
                except (ValueError, TypeError):
                    adapter[field] = 0

        logger.debug(f"æ•°æ®æ¸…æ´—å®Œæˆ: {adapter.get('source_url')}")
        return item


class DuplicatesPipeline:
    """å»é‡ç®¡é“"""

    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        """æ£€æŸ¥é‡å¤æ•°æ®"""
        adapter = ItemAdapter(item)

        # ç”Ÿæˆæ•°æ®æŒ‡çº¹
        fingerprint = self.generate_fingerprint(adapter)

        if fingerprint in self.ids_seen:
            raise DropItem(f"é‡å¤æ•°æ®: {adapter.get('source_url')}")
        else:
            self.ids_seen.add(fingerprint)
            logger.debug(f"æ–°æ•°æ®: {adapter.get('source_url')}")
            return item

    def generate_fingerprint(self, adapter):
        """ç”Ÿæˆæ•°æ®æŒ‡çº¹"""
        # ä½¿ç”¨URLå’Œå…³é”®å­—æ®µç”ŸæˆæŒ‡çº¹
        key_fields = ["source_url", "region", "report_date"]
        fingerprint_data = []

        for field in key_fields:
            value = adapter.get(field, "")
            fingerprint_data.append(str(value))

        fingerprint_string = "|".join(fingerprint_data)
        return hashlib.md5(fingerprint_string.encode()).hexdigest()

class ContentUpdatePipeline:
    """åŸºäºå†…å®¹æŒ‡çº¹çš„æ›´æ–°æ£€æµ‹ä¸å»é‡ï¼ˆåˆ†å¸ƒå¼ï¼ŒRedisåŸå­CASï¼‰ã€‚

    - å¯¹ item['content'] åšè§„èŒƒåŒ–åè®¡ç®— SHA256 æŒ‡çº¹
    - Redis HASH: content_fp:<site>ï¼Œfield=sha1(url)ï¼Œvalue=sha256(content)
    - Lua åŸå­è„šæœ¬è¿”å›ç ï¼š
        1 => createdï¼ˆé¦–æ¬¡å‡ºç°ï¼‰
        2 => modifiedï¼ˆå†…å®¹å˜åŒ–ï¼‰
        0 => unchangedï¼ˆæ— å˜åŒ–ï¼Œä¸¢å¼ƒï¼‰
    """

    def __init__(self, redis_url: str = None):
        self.redis_url = redis_url
        self.redis = None
        self.lua = None

    @classmethod
    def from_crawler(cls, crawler):
        redis_url = crawler.settings.get("REDIS_URL")
        pipe = cls(redis_url)
        pipe._connect()
        return pipe

    def _connect(self):
        try:
            import redis

            self.redis = redis.from_url(self.redis_url)
            self.redis.ping()
            # æ³¨å†ŒLuaè„šæœ¬
            script = """
            local key = KEYS[1]
            local field = ARGV[1]
            local newv = ARGV[2]
            local oldv = redis.call('HGET', key, field)
            if (not oldv) then
                redis.call('HSET', key, field, newv)
                return 1
            end
            if (oldv ~= newv) then
                redis.call('HSET', key, field, newv)
                return 2
            end
            return 0
            """
            self.lua = self.redis.register_script(script)
        except Exception as e:
            logger.warning(f"âš ï¸ ContentUpdatePipeline æ— æ³•è¿æ¥Redisï¼Œå°†é™çº§è¿è¡Œ: {e}")
            self.redis = None

    def _normalize(self, text: str) -> str:
        if not text:
            return ""
        # ç®€å•è§„èŒƒåŒ–ï¼šå»HTMLæ ‡ç­¾æ®‹ç•™ï¼ˆè‹¥å·²æ¸…æ´—åˆ™åŸºæœ¬æ— å½±å“ï¼‰ã€å‹ç¼©ç©ºç™½
        try:
            stripped = "".join(text.split())  # å¼ºå‹ç¼©ç©ºç™½
            return stripped
        except Exception:
            return text

    def process_item(self, item, spider):
        if not spider.settings.getbool("CONTENT_DEDUP_ENABLED", True):
            return item
        url = item.get("source_url") or item.get("url")
        content = item.get("content")
        if not url or content is None:
            return item

        # è®¡ç®—æŒ‡çº¹
        ufield = hashlib.sha1(url.encode("utf-8")).hexdigest()
        cfp = hashlib.sha256(self._normalize(content).encode("utf-8")).hexdigest()

        # Redis ä¸å¯ç”¨ -> ç›´æ¥é€šè¿‡ï¼ˆä¿ç•™ itemï¼‰
        if not self.redis or not self.lua:
            item["dedup_status"] = "unknown"
            item["content_fingerprint"] = cfp
            return item

        site = getattr(spider, "target_site", None) or "default"
        key = f"content_fp:{site}"
        try:
            rc = self.lua(keys=[key], args=[ufield, cfp])
        except Exception as e:
            logger.warning(f"âš ï¸ Redis Lua æ‰§è¡Œå¤±è´¥ï¼Œé™çº§é€šè¿‡: {e}")
            item["dedup_status"] = "unknown"
            item["content_fingerprint"] = cfp
            return item

        status = {0: "unchanged", 1: "created", 2: "modified"}.get(int(rc), "unknown")
        item["dedup_status"] = status
        item["content_fingerprint"] = cfp

        if status == "unchanged":
            raise DropItem("å†…å®¹æœªå˜åŒ–ï¼Œä¸¢å¼ƒä»¥èŠ‚çœå­˜å‚¨")
        return item



class MongoPipeline:
    """MongoDBå­˜å‚¨ç®¡é“"""

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
        self.client = None
        self.db = None

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get("MONGODB_URI"),
            mongo_db=crawler.settings.get("MONGODB_DATABASE", "crawler_db"),
        )

    def open_spider(self, spider):
        """çˆ¬è™«å¼€å§‹æ—¶è¿æ¥MongoDB"""
        try:
            import pymongo

            self.client = pymongo.MongoClient(self.mongo_uri)
            self.db = self.client[self.mongo_db]
            logger.info("MongoDBè¿æ¥æˆåŠŸ")
        except ImportError:
            logger.error("pymongoæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨MongoDBç®¡é“")
        except Exception as e:
            logger.error(f"MongoDBè¿æ¥å¤±è´¥: {e}")

    def close_spider(self, spider):
        """çˆ¬è™«ç»“æŸæ—¶å…³é—­MongoDBè¿æ¥"""
        if self.client:
            self.client.close()
            logger.info("MongoDBè¿æ¥å·²å…³é—­")

    def process_item(self, item, spider):
        """å­˜å‚¨æ•°æ®åˆ°MongoDB"""
        if self.db is None:
            logger.warning("âŒ MongoDBæ•°æ®åº“è¿æ¥ä¸ºç©º")
            return item

        logger.info(f"ğŸ’¾ MongoPipeline å¤„ç†æ•°æ®é¡¹")

        try:
            adapter = ItemAdapter(item)
            collection_name = f"{spider.name}_data"
            collection = self.db[collection_name]

            logger.info(f"ğŸ“Š å‡†å¤‡å­˜å‚¨åˆ°é›†åˆ: {collection_name}")
            logger.info(f"ğŸ“„ æ•°æ®é¡¹å­—æ®µæ•°: {len(adapter.asdict())}")

            # æ’å…¥æ•°æ®
            result = collection.insert_one(adapter.asdict())
            logger.info(f"âœ… æ•°æ®å·²å­˜å‚¨åˆ°MongoDB: {result.inserted_id}")

        except Exception as e:
            logger.error(f"âŒ MongoDBå­˜å‚¨å¤±è´¥: {e}")
            import traceback

            logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise DropItem(f"MongoDBå­˜å‚¨å¤±è´¥: {e}")

        return item


class PostgresPipeline:
    """PostgreSQLå­˜å‚¨ç®¡é“"""

    def __init__(self, postgres_settings):
        self.postgres_settings = postgres_settings
        self.connection = None
        self.cursor = None

    @classmethod
    def from_crawler(cls, crawler):
        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ‰€æœ‰PostgreSQLç›¸å…³çš„è®¾ç½®
        logger.info("ğŸ” PostgresPipeline è°ƒè¯•ä¿¡æ¯:")
        logger.info(f"  POSTGRES_HOST (åŸå§‹): {crawler.settings.get('POSTGRES_HOST')}")
        logger.info(f"  POSTGRES_PORT: {crawler.settings.get('POSTGRES_PORT')}")
        logger.info(f"  POSTGRES_DATABASE: {crawler.settings.get('POSTGRES_DATABASE')}")
        logger.info(f"  POSTGRES_USER: {crawler.settings.get('POSTGRES_USER')}")
        logger.info(
            f"  POSTGRES_PASSWORD: {'*' * len(str(crawler.settings.get('POSTGRES_PASSWORD', '')))}"
        )

        # è·å–ä¸»æœºåœ°å€ï¼Œå¦‚æœæ˜¯DockeræœåŠ¡ååˆ™è½¬æ¢ä¸ºlocalhost
        postgres_host = crawler.settings.get("POSTGRES_HOST")
        original_host = postgres_host
        if postgres_host == "postgresql":
            postgres_host = "localhost"
            logger.info(f"  ä¸»æœºåœ°å€è½¬æ¢: {original_host} -> {postgres_host}")
        else:
            logger.info(f"  ä½¿ç”¨åŸå§‹ä¸»æœºåœ°å€: {postgres_host}")

        postgres_settings = {
            "host": postgres_host,
            "port": crawler.settings.get("POSTGRES_PORT"),
            "database": crawler.settings.get("POSTGRES_DATABASE"),
            "user": crawler.settings.get("POSTGRES_USER"),
            "password": crawler.settings.get("POSTGRES_PASSWORD"),
            "client_encoding": "utf8",  # æ·»åŠ ç¼–ç è®¾ç½®
            "connect_timeout": 10,  # æ·»åŠ è¿æ¥è¶…æ—¶
        }

        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæœ€ç»ˆçš„è¿æ¥é…ç½®
        logger.info("ğŸ“‹ æœ€ç»ˆè¿æ¥é…ç½®:")
        for key, value in postgres_settings.items():
            if key == "password":
                logger.info(f"  {key}: {'*' * len(str(value)) if value else 'None'}")
            else:
                logger.info(f"  {key}: {value}")

        return cls(postgres_settings)

    def open_spider(self, spider):
        """çˆ¬è™«å¼€å§‹æ—¶è¿æ¥PostgreSQL"""
        try:
            import psycopg2
            import psycopg2.extensions

            logger.info("ğŸš€ å¼€å§‹PostgreSQLè¿æ¥è¿‡ç¨‹...")

            # å†æ¬¡æ˜¾ç¤ºè¿æ¥å‚æ•°ï¼ˆç¡®ä¿æ²¡æœ‰è¢«ä¿®æ”¹ï¼‰
            logger.info("ğŸ” è¿æ¥å‚æ•°éªŒè¯:")
            for key, value in self.postgres_settings.items():
                if key == "password":
                    logger.info(
                        f"  {key}: {'*' * len(str(value)) if value else 'None'}"
                    )
                else:
                    logger.info(f"  {key}: {value}")

            # è®¾ç½®è¿æ¥ç¼–ç 
            psycopg2.extensions.register_type(psycopg2.extensions.UNICODE)
            psycopg2.extensions.register_type(psycopg2.extensions.UNICODEARRAY)

            logger.info(
                f"ğŸ”Œ å°è¯•è¿æ¥PostgreSQL: {self.postgres_settings['host']}:{self.postgres_settings['port']}"
            )
            logger.info(f"ğŸ“Š æ•°æ®åº“: {self.postgres_settings['database']}")
            logger.info(f"ğŸ‘¤ ç”¨æˆ·: {self.postgres_settings['user']}")

            self.connection = psycopg2.connect(**self.postgres_settings)
            self.connection.set_client_encoding("UTF8")
            self.cursor = self.connection.cursor()

            # æµ‹è¯•è¿æ¥
            self.cursor.execute("SELECT version();")
            version = self.cursor.fetchone()
            logger.info(f"âœ… PostgreSQLè¿æ¥æˆåŠŸ!")
            logger.info(f"ğŸ“‹ ç‰ˆæœ¬ä¿¡æ¯: {version[0]}")

            # æµ‹è¯•ç¼–ç 
            self.cursor.execute("SHOW client_encoding;")
            encoding = self.cursor.fetchone()
            logger.info(f"ğŸ”¤ å®¢æˆ·ç«¯ç¼–ç : {encoding[0]}")

            # æµ‹è¯•å½“å‰ç”¨æˆ·å’Œæ•°æ®åº“
            self.cursor.execute("SELECT current_user, current_database();")
            user_db = self.cursor.fetchone()
            logger.info(f"ğŸ‘¤ å½“å‰ç”¨æˆ·: {user_db[0]}")
            logger.info(f"ğŸ“Š å½“å‰æ•°æ®åº“: {user_db[1]}")

        except ImportError:
            logger.error("âŒ psycopg2æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨PostgreSQLç®¡é“")
            logger.error("è¯·è¿è¡Œ: pip install psycopg2-binary")
        except psycopg2.OperationalError as e:
            logger.error(f"âŒ PostgreSQLè¿æ¥å¤±è´¥ (æ“ä½œé”™è¯¯): {e}")
            logger.error("å¯èƒ½çš„åŸå› :")
            logger.error("  1. æ•°æ®åº“æœåŠ¡æœªè¿è¡Œ")
            logger.error("  2. ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
            logger.error("  3. æ•°æ®åº“ä¸å­˜åœ¨")
            logger.error("  4. ç½‘ç»œè¿æ¥é—®é¢˜")
        except psycopg2.DatabaseError as e:
            logger.error(f"âŒ PostgreSQLæ•°æ®åº“é”™è¯¯: {e}")
        except UnicodeDecodeError as e:
            logger.error(f"âŒ UTF-8è§£ç é”™è¯¯: {e}")
            logger.error("è¿™é€šå¸¸è¡¨ç¤ºç”¨æˆ·åæˆ–å¯†ç åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œæˆ–PostgreSQLè¿”å›äº†éUTF-8ç¼–ç çš„é”™è¯¯æ¶ˆæ¯")
            logger.error("å»ºè®®æ£€æŸ¥ç”¨æˆ·åå’Œå¯†ç æ˜¯å¦æ­£ç¡®")
        except Exception as e:
            logger.error(f"âŒ PostgreSQLè¿æ¥å¤±è´¥ (æœªçŸ¥é”™è¯¯): {e}")
            import traceback

            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

    def close_spider(self, spider):
        """çˆ¬è™«ç»“æŸæ—¶å…³é—­PostgreSQLè¿æ¥"""
        if self.cursor:
            self.cursor.close()
        if self.connection:
            self.connection.close()
        logger.info("PostgreSQLè¿æ¥å·²å…³é—­")

    def process_item(self, item, spider):
        """å­˜å‚¨æ•°æ®åˆ°PostgreSQL"""
        if self.connection is None or self.cursor is None:
            logger.warning("PostgreSQLè¿æ¥æœªå»ºç«‹ï¼Œè·³è¿‡å­˜å‚¨")
            return item

        try:
            adapter = ItemAdapter(item)

            # æ ¹æ®Itemç±»å‹ç¡®å®šè¡¨å
            item_type = type(item).__name__
            table_mapping = {
                "NewsItem": "news_data",
                "EpidemicDataItem": "epidemic_data",
                "PolicyItem": "policy_data",
                "StatisticsItem": "statistics_data",
            }

            # å¯¹äºadaptive spiderçš„é€šç”¨æ•°æ®ï¼Œä½¿ç”¨crawler_dataè¡¨
            if item_type == "dict" or "adaptive" in adapter.get("spider_name", ""):
                table_name = "crawler_data"
            else:
                table_name = table_mapping.get(item_type, "crawler_data")

            # æ•°æ®é¢„å¤„ç†ï¼šè½¬æ¢æ•°æ®ç±»å‹
            processed_data = self._preprocess_data(adapter, item_type)

            # è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®ç±»å‹
            logger.debug(f"PostgreSQLå­˜å‚¨è°ƒè¯• - è¡¨: {table_name}")
            for key, value in processed_data.items():
                logger.debug(f"  {key}: {type(value)} = {str(value)[:100]}")

            # æ„å»ºæ’å…¥SQL
            fields = list(processed_data.keys())
            values = list(processed_data.values())

            placeholders = ", ".join(["%s"] * len(fields))
            fields_str = ", ".join(fields)

            sql = f"INSERT INTO {table_name} ({fields_str}) VALUES ({placeholders})"

            # æ‰§è¡Œæ’å…¥
            logger.debug(f"æ‰§è¡ŒSQL: {sql}")
            self.cursor.execute(sql, values)
            self.connection.commit()

            logger.info(
                f"âœ… æ•°æ®å·²ä¿å­˜åˆ°PostgreSQLè¡¨ {table_name}: {adapter.get('title', 'No Title')[:50]}..."
            )

        except Exception as e:
            self.connection.rollback()
            logger.error(f"âŒ PostgreSQLå­˜å‚¨å¤±è´¥: {e}")
            logger.error(f"âŒ æ•°æ®å†…å®¹: {dict(adapter)}")

            # è¯¦ç»†é”™è¯¯åˆ†æ
            if "can't adapt type" in str(e):
                logger.error("âŒ æ•°æ®ç±»å‹é€‚é…é”™è¯¯ï¼Œæ£€æŸ¥ä»¥ä¸‹å­—æ®µçš„æ•°æ®ç±»å‹:")
                for key, value in adapter.items():
                    if isinstance(value, (dict, list, tuple, set)):
                        logger.error(f"   é—®é¢˜å­—æ®µ: {key} = {type(value)} -> {value}")

            raise DropItem(f"PostgreSQLå­˜å‚¨å¤±è´¥: {e}")

        return item

    def _preprocess_data(self, adapter, item_type):
        """é¢„å¤„ç†æ•°æ®ï¼Œè½¬æ¢æ•°æ®ç±»å‹ä»¥åŒ¹é…PostgreSQLè¡¨ç»“æ„"""
        import datetime
        import json

        # è·å–æ‰€æœ‰æ•°æ®
        data = adapter.asdict()

        processed_data = {}

        for key, value in adapter.items():
            # å¤„ç†å­—å…¸å’Œåˆ—è¡¨ç±»å‹ - è½¬æ¢ä¸ºJSON
            if isinstance(value, (dict, list)):
                try:
                    processed_data[key] = json.dumps(value, ensure_ascii=False)
                    logger.debug(f"å­—æ®µ {key} è½¬æ¢ä¸ºJSON: {type(value)} -> str")
                except (TypeError, ValueError) as e:
                    logger.warning(f"å­—æ®µ {key} JSONåºåˆ—åŒ–å¤±è´¥: {e}")
                    processed_data[key] = str(value)

            # å¤„ç†æ—¶é—´æˆ³å­—æ®µ
            elif key in ["crawl_timestamp"] and value is not None:
                if isinstance(value, (int, float)):
                    # å°†Unixæ—¶é—´æˆ³è½¬æ¢ä¸ºdatetimeå¯¹è±¡
                    processed_data[key] = datetime.datetime.fromtimestamp(
                        value, tz=datetime.timezone.utc
                    )
                else:
                    processed_data[key] = value

            # å¤„ç†crawl_timeå­—æ®µ
            elif key in ["crawl_time"] and value is not None:
                if isinstance(value, str):
                    try:
                        # å°è¯•è§£æISOæ ¼å¼çš„æ—¶é—´å­—ç¬¦ä¸²
                        processed_data[key] = datetime.datetime.fromisoformat(
                            value.replace("Z", "+00:00")
                        )
                    except ValueError:
                        processed_data[key] = value
                else:
                    processed_data[key] = value

            # å¸¸è§æ•°å€¼è§„èŒƒåŒ–
            elif (
                key in ["status_code", "content_length", "chinese_char_count"]
                and value is not None
            ):
                try:
                    processed_data[key] = int(value)
                except Exception:
                    processed_data[key] = None

            # å¤„ç†æ—¥æœŸå­—æ®µ
            elif (
                key in ["publish_date", "report_date", "issue_date", "effective_date"]
                and value is not None
            ):
                if isinstance(value, str):
                    try:
                        # å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²
                        processed_data[key] = datetime.datetime.strptime(
                            value, "%Y-%m-%d"
                        ).date()
                    except ValueError:
                        processed_data[key] = value
                else:
                    processed_data[key] = value

            # å¤„ç†æ•°ç»„å­—æ®µï¼ˆPostgreSQLçš„TEXT[]ç±»å‹ï¼‰
            elif key in ["tags", "keywords"] and value is not None:
                if isinstance(value, list):
                    # ç¡®ä¿åˆ—è¡¨ä¸­éƒ½æ˜¯å­—ç¬¦ä¸²
                    processed_data[key] = [
                        str(item) for item in value if item is not None
                    ]
                elif isinstance(value, str):
                    processed_data[key] = [
                        tag.strip() for tag in value.split(",") if tag.strip()
                    ]
                else:
                    processed_data[key] = []

            # å¤„ç†æ•°å€¼å­—æ®µ
            elif (
                key
                in [
                    "confirmed_cases",
                    "death_cases",
                    "recovered_cases",
                    "active_cases",
                    "new_confirmed",
                    "new_deaths",
                    "new_recovered",
                    "view_count",
                    "comment_count",
                ]
                and value is not None
            ):
                try:
                    processed_data[key] = int(value) if value != "" else 0
                except (ValueError, TypeError):
                    processed_data[key] = 0

            # å¤„ç†æµ®ç‚¹æ•°å­—æ®µ
            elif key in ["data_quality_score"] and value is not None:
                try:
                    processed_data[key] = float(value) if value != "" else 0.0
                except (ValueError, TypeError):
                    processed_data[key] = 0.0

            # å¤„ç†Noneå€¼
            elif value is None:
                processed_data[key] = None

            # å…¶ä»–å­—æ®µç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
            else:
                if isinstance(value, (str, int, float, bool)):
                    processed_data[key] = value
                else:
                    # å¤æ‚å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    processed_data[key] = str(value)
                    logger.debug(f"å­—æ®µ {key} è½¬æ¢ä¸ºå­—ç¬¦ä¸²: {type(value)} -> str")

        return processed_data


class JsonWriterPipeline:
    """JSONæ–‡ä»¶å†™å…¥ç®¡é“"""

    def open_spider(self, spider):
        """çˆ¬è™«å¼€å§‹æ—¶æ‰“å¼€æ–‡ä»¶"""
        self.file = open(f"data/{spider.name}_items.jl", "w", encoding="utf-8")

    def close_spider(self, spider):
        """çˆ¬è™«ç»“æŸæ—¶å…³é—­æ–‡ä»¶"""
        self.file.close()

    def process_item(self, item, spider):
        """å†™å…¥æ•°æ®åˆ°JSONæ–‡ä»¶"""
        line = json.dumps(ItemAdapter(item).asdict(), ensure_ascii=False) + "\n"
        self.file.write(line)
        return item
