# -*- coding: utf-8 -*-
"""
æ•°æ®ç®¡é“æ¨¡å—

åŒ…å«å„ç§æ•°æ®å¤„ç†ç®¡é“ï¼š
- æ•°æ®éªŒè¯ç®¡é“
- æ•°æ®æ¸…æ´—ç®¡é“
- å»é‡ç®¡é“
- å­˜å‚¨ç®¡é“
"""

import hashlib
import json
import logging
import os # å¯¼å…¥osæ¨¡å—ç”¨äºè·å–ç¯å¢ƒå˜é‡

from openai import OpenAI # å¯¼å…¥OpenAIåº“
from zai import ZhipuAiClient # å¯¼å…¥ZhipuAiClientåº“
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem

logger = logging.getLogger(__name__)


class ValidationPipeline:
    """æ•°æ®éªŒè¯ç®¡é“"""

    def process_item(self, item, spider):
        """éªŒè¯æ•°æ®é¡¹"""
        adapter = ItemAdapter(item)

        # æ ¹æ®Itemç±»å‹ç¡®å®šå¿…éœ€å­—æ®µ
        item_type = type(item).__name__

        if item_type == "NewsItem":
            required_fields = ["url", "title", "crawl_time"]
            url_field = "url"
        elif item_type == "EpidemicDataItem":
            required_fields = ["source_url", "title", "crawl_time"]
            url_field = "source_url"
        elif item_type == "PolicyItem":
            required_fields = ["url", "title", "crawl_time"]
            url_field = "url"
        elif item_type == "StatisticsItem":
            required_fields = ["source_url", "crawl_time"]
            url_field = "source_url"
        else:
            # é»˜è®¤éªŒè¯
            required_fields = ["crawl_time"]
            url_field = None

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if not adapter.get(field):
                raise DropItem(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field} (Itemç±»å‹: {item_type})")

        # éªŒè¯URLæ ¼å¼
        if url_field:
            url_value = adapter.get(url_field)
            if url_value and not url_value.startswith(("http://", "https://")):
                raise DropItem(f"æ— æ•ˆçš„URLæ ¼å¼: {url_value} (å­—æ®µ: {url_field})")

        logger.debug(
            f"æ•°æ®éªŒè¯é€šè¿‡: {item_type} - {adapter.get(url_field) if url_field else 'No URL'}"
        )
        return item


class CleaningPipeline:
    """æ•°æ®æ¸…æ´—ç®¡é“"""

    def process_item(self, item, spider):
        """æ¸…æ´—æ•°æ®é¡¹"""
        adapter = ItemAdapter(item)

        # æ¸…æ´—æ–‡æœ¬å­—æ®µ
        text_fields = ["title", "content", "region"]
        for field in field_text:
            value = adapter.get(field)
            if value:
                # å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
                cleaned_value = " ".join(value.split())
                adapter[field] = cleaned_value

        # æ¸…æ´—æ•°å­—å­—æ®µ
        number_fields = ["confirmed_cases", "death_cases", "recovered_cases"]
        for field in number_fields:
            value = adapter.get(field)
            if value is not None:
                try:
                    # ç¡®ä¿æ˜¯æ•´æ•°
                    adapter[field] = int(value)
                except (ValueError, TypeError):
                    adapter[field] = 0

        logger.debug(f"æ•°æ®æ¸…æ´—å®Œæˆ: {adapter.get('source_url')}")
        return item


class DuplicatesPipeline:
    """å»é‡ç®¡é“"""

    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        """æ£€æŸ¥é‡å¤æ•°æ®"""
        adapter = ItemAdapter(item)

        # ç”Ÿæˆæ•°æ®æŒ‡çº¹
        fingerprint = self.generate_fingerprint(adapter)

        if fingerprint in self.ids_seen:
            raise DropItem(f"é‡å¤æ•°æ®: {adapter.get('source_url')}")
        else:
            self.ids_seen.add(fingerprint)
            logger.debug(f"æ–°æ•°æ®: {adapter.get('source_url')}")
            return item

    def generate_fingerprint(self, adapter):
        """ç”Ÿæˆæ•°æ®æŒ‡çº¹"""
        # ä½¿ç”¨URLå’Œå…³é”®å­—æ®µç”ŸæˆæŒ‡çº¹
        key_fields = ["source_url", "region", "report_date"]
        fingerprint_data = []

        for field in key_fields:
            value = adapter.get(field, "")
            fingerprint_data.append(str(value))

        fingerprint_string = "|".join(fingerprint_data)
        return hashlib.md5(fingerprint_string.encode()).hexdigest()


class AIPipeline:
    """
    AIåˆ¤æ–­ç®¡é“ï¼Œç”¨äºè¯†åˆ«æ–‡ç« æ ‡é¢˜æ˜¯å¦ä¸ä¼ æŸ“ç—…ç–«æƒ…ç›¸å…³ã€‚
    æ”¯æŒDeepSeekå’Œæ™ºè°±AIå¤§æ¨¡å‹è¿›è¡Œåˆ†ç±»ã€‚
    """

    def __init__(self, settings):
        self.model_provider = settings.get("AI_MODEL_PROVIDER", "deepseek")
        self.client = None
        self.model_name = None

        if self.model_provider == "deepseek":
            self.deepseek_api_key = settings.get("DEEPSEEK_API_KEY", "<DeepSeek API Key>")
            self.deepseek_base_url = "https://api.deepseek.com"
            self.model_name = settings.get("DEEPSEEK_MODEL_NAME", "deepseek-chat")

            if not self.deepseek_api_key or self.deepseek_api_key == "<DeepSeek API Key>":
                logger.error("DeepSeek API Key æœªé…ç½®æˆ–ä½¿ç”¨é»˜è®¤å ä½ç¬¦ï¼ŒAIPipelineå°†è·³è¿‡DeepSeekåˆå§‹åŒ–ã€‚")
                return
            try:
                self.client = OpenAI(
                    api_key=self.deepseek_api_key, base_url=self.deepseek_base_url
                )
                logger.info("DeepSeek OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
            except Exception as e:
                logger.error(f"DeepSeek OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")

        elif self.model_provider == "zhipuai":
            self.zhipuai_api_key = settings.get("ZHIPUAI_API_KEY", "<ZhipuAI API Key>")
            self.model_name = settings.get("ZHIPUAI_MODEL_NAME", "glm-4.5")

            if not self.zhipuai_api_key or self.zhipuai_api_key == "<ZhipuAI API Key>":
                logger.error("ZhipuAI API Key æœªé…ç½®æˆ–ä½¿ç”¨é»˜è®¤å ä½ç¬¦ï¼ŒAIPipelineå°†è·³è¿‡ZhipuAIåˆå§‹åŒ–ã€‚")
                return
            try:
                self.client = ZhipuAiClient(api_key=self.zhipuai_api_key)
                logger.info("ZhipuAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
            except Exception as e:
                logger.error(f"ZhipuAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.error(f"ä¸æ”¯æŒçš„AIæ¨¡å‹æä¾›å•†: {self.model_provider}ã€‚AIPipelineå°†è·³è¿‡åˆå§‹åŒ–ã€‚")

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.settings)

    def process_item(self, item, spider):
        """
        å¤„ç†æ•°æ®é¡¹ï¼Œé€šè¿‡AIåˆ¤æ–­æ ‡é¢˜ç›¸å…³æ€§ã€‚
        """
        adapter = ItemAdapter(item)
        title = adapter.get("title")

        if not title:
            logger.debug("Itemç¼ºå°‘æ ‡é¢˜ï¼Œè·³è¿‡AIåˆ¤æ–­ã€‚")
            return item

        if not self.client:
            logger.warning(f"{self.model_provider}å®¢æˆ·ç«¯æœªåˆå§‹åŒ–æˆåŠŸï¼ŒAIåˆ¤æ–­åŠŸèƒ½å°†è·³è¿‡ã€‚")
            return item

        try:
            system_content = (
                                """ 
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ç« æ ‡é¢˜åˆ†ç±»åŠ©æ‰‹ï¼Œä¸“é—¨æ ¹æ®ç²¾ç¡®æ ‡å‡†åˆ¤æ–­æ ‡é¢˜æ˜¯å¦ä¸ä¼ æŸ“ç—…åŠç–«æƒ…ç›¸å…³ä¸»é¢˜ç›¸å…³ã€‚è¯·åªå›ç­”â€œç›¸å…³â€æˆ–â€œä¸ç›¸å…³â€ã€‚

**ç›¸å…³æ ‡å‡†ï¼š**  
å¦‚æœæ ‡é¢˜æ¶‰åŠä»¥ä¸‹ä»»ä½•ä¸»é¢˜ï¼Œåˆ™å›ç­”â€œç›¸å…³â€ï¼š  
- ç–«æƒ…ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºCOVID-19ã€æµæ„Ÿã€åŸƒåšæ‹‰ç­‰ä¼ æŸ“ç—…çš„æµè¡Œã€é˜²æ§æˆ–å½±å“ï¼‰ã€‚  
- ä¼ æŸ“ç—…çš„åŠ¨å‘ï¼ˆå¦‚ä¼ æ’­è¶‹åŠ¿ã€æ–°å¢ç—…ä¾‹ã€æ­»äº¡äººæ•°ã€å˜å¼‚æƒ…å†µï¼‰ã€‚  
- å„åœ°ç–«æƒ…æµè¡Œæˆ–çˆ†å‘æƒ…å†µï¼ˆåŒ…æ‹¬æœ¬åœ°ã€åŒºåŸŸæˆ–å…¨çƒèŒƒå›´çš„çˆ†å‘äº‹ä»¶ã€èšé›†æ€§ç–«æƒ…ï¼‰ã€‚  
- å®˜æ–¹ç»Ÿè®¡è¡¨ï¼ˆå¦‚ç–¾æ§ä¸­å¿ƒã€WHOã€å«ç”Ÿéƒ¨é—¨å‘å¸ƒçš„ç–«æƒ…æ•°æ®ã€å›¾è¡¨ã€æŠ¥å‘Šï¼‰ã€‚  
- å½“å­£æµè¡Œæ€§ç—…æ¯’ï¼ˆå¦‚å­£èŠ‚æ€§æµæ„Ÿã€å‘¼å¸é“åˆèƒç—…æ¯’ã€è¯ºå¦‚ç—…æ¯’ç­‰æµè¡Œæƒ…å†µï¼‰ã€‚  
- ä¼ æŸ“ç—…ç ”ç©¶æ–°åŠ¨å‘ï¼ˆåŒ…æ‹¬ç–«è‹—ç ”å‘ã€æ²»ç–—æ–¹æ³•ã€ç—…åŸä½“å‘ç°ã€å­¦æœ¯çªç ´ï¼‰ã€‚  
- ä¼ æŸ“ç—…é‡è¦ä¼šè®®å’Œæ”¿ç­–ï¼ˆå¦‚å›½é™…å«ç”Ÿæ¡ä¾‹ã€é˜²æ§æŒ‡å—ã€å…¬å…±å«ç”Ÿæ”¿ç­–å‘å¸ƒï¼‰ã€‚  
- å…¶ä»–ç›´æ¥ç›¸å…³ä¸»é¢˜ï¼ˆå¦‚ç–«æƒ…é¢„è­¦ã€æµè¡Œç—…å­¦è°ƒæŸ¥ã€é˜²æ§æªæ–½ã€ç–«æƒ…ç»æµå½±å“ï¼‰ã€‚  

**ä¸ç›¸å…³æ ‡å‡†ï¼š**  
å¦‚æœæ ‡é¢˜æ˜æ˜¾ä¸ä»¥ä¸Šä¸»é¢˜æ— å…³ï¼Œä¾‹å¦‚æ¶‰åŠéä¼ æŸ“æ€§ç–¾ç—…ï¼ˆå¦‚ç™Œç—‡ã€ç³–å°¿ç—…ï¼‰ã€ä¸€èˆ¬å¥åº·è¯é¢˜ï¼ˆå¦‚å¥èº«ã€è¥å…»ï¼‰ã€éç–«æƒ…æ–°é—»ï¼ˆå¦‚æ”¿æ²»ã€ä½“è‚²ã€å¨±ä¹ï¼‰ã€æˆ–å…¶ä»–æ— å…³é¢†åŸŸï¼Œåˆ™å›ç­”â€œä¸ç›¸å…³â€ã€‚  

**åˆ¤æ–­åŸåˆ™ï¼š**  
- ç¡®ä¿å‡†ç¡®ç‡ï¼šé¿å…å°†æ˜æ˜¾ä¸æ²¾è¾¹çš„ä¿¡æ¯ï¼ˆå¦‚æ—¥å¸¸å¤©æ°”ã€æ™®é€šç§‘æŠ€æ–°é—»ï¼‰æˆ–è€…ä¸¾åŠçš„æ¯”èµ›ã€æ´»åŠ¨ã€åŸ¹è®­ã€å·¥ä½œæ£€æŸ¥ç­‰æ— å…³ä¿¡æ¯è¯¯åˆ¤ä¸ºç›¸å…³ã€‚  

è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸Šæ ‡å‡†è¿›è¡Œåˆ†ç±»ã€‚
                """
            )
            user_content = f"è¯·åˆ¤æ–­ä»¥ä¸‹æ ‡é¢˜æ˜¯å¦ç›¸å…³ï¼š'{title}'"

            messages = [
                {"role": "system", "content": system_content},
                {"role": "user", "content": user_content},
            ]

            if self.model_provider == "deepseek":
                response = self.client.chat.completions.create(
                    model=self.model_name, messages=messages, stream=False, temperature=0.0
                )
                ai_response_content = response.choices[0].message.content.strip()
            elif self.model_provider == "zhipuai":
                response = self.client.chat.completions.create(
                    model=self.model_name, messages=messages, stream=False, temperature=0.0
                )
                ai_response_content = response.choices[0].message.content.strip()
            else:
                logger.error(f"ä¸æ”¯æŒçš„AIæ¨¡å‹æä¾›å•†: {self.model_provider}ã€‚æ— æ³•è¿›è¡ŒAIåˆ¤æ–­ã€‚")
                adapter["ai_relevant"] = "error"
                return item

            logger.info(f"{self.model_provider} AIå¯¹æ ‡é¢˜ '{title}' çš„å“åº”: '{ai_response_content}'")

            is_relevant = (ai_response_content == "ç›¸å…³")

            if is_relevant:
                logger.info(f"AIåˆ¤æ–­æ ‡é¢˜ '{title}' ä¸ç–«æƒ…ç›¸å…³ï¼Œè¿›è¡Œä¿å­˜ã€‚")
                adapter["ai_relevant"] = True
                return item
            else:
                logger.info(f"AIåˆ¤æ–­æ ‡é¢˜ '{title}' ä¸ç–«æƒ…ä¸ç›¸å…³ï¼Œè¿›è¡Œä¸¢å¼ƒã€‚")
                adapter["ai_relevant"] = False
                raise DropItem(f"AIåˆ¤æ–­æ ‡é¢˜ä¸ç›¸å…³: {title}")
        except DropItem:
            raise
        except Exception as e:
            logger.error(f"{self.model_provider} AIåˆ¤æ–­å¤±è´¥æˆ–APIè°ƒç”¨é”™è¯¯: {e}ï¼Œæ ‡é¢˜: {title}")
            adapter["ai_relevant"] = "error"
            return item


class ContentUpdatePipeline:
    """åŸºäºå†…å®¹æŒ‡çº¹çš„æ›´æ–°æ£€æµ‹ä¸å»é‡ï¼ˆåˆ†å¸ƒå¼ï¼ŒRedisåŸå­CASï¼‰ã€‚

    - å¯¹ item['content'] åšè§„èŒƒåŒ–åè®¡ç®— SHA256 æŒ‡çº¹
    - Redis HASH: content_fp:<site>ï¼Œfield=sha1(url)ï¼Œvalue=sha256(content)
    - Redis SET: content_seen:<scope>ï¼Œmember=sha256(content)ï¼ˆè·¨URLå†…å®¹å»é‡ï¼‰
    - Lua åŸå­è„šæœ¬è¿”å›ç ï¼š
        1 => createdï¼ˆé¦–æ¬¡å‡ºç°ï¼‰
        2 => modifiedï¼ˆå†…å®¹å˜åŒ–ï¼‰
        0 => unchangedï¼ˆæ— å˜åŒ–ï¼Œä¸¢å¼ƒï¼‰
    """

    def __init__(self, redis_url: str = None):
        self.redis_url = redis_url
        self.redis = None
        self.lua = None
        # å»é‡é…ç½®ï¼ˆé»˜è®¤å€¼ï¼Œå¯è¢« settings è¦†ç›–ï¼‰
        self.global_dedup_enabled = True
        self.dedup_scope = "per_site"  # per_site | global
        self.dedup_ttl_seconds = 0

    @classmethod
    def from_crawler(cls, crawler):
        redis_url = crawler.settings.get("REDIS_URL")
        pipe = cls(redis_url)
        pipe._connect()
        # è¯»å–é…ç½®é¡¹
        try:
            pipe.global_dedup_enabled = crawler.settings.getbool("CONTENT_GLOBAL_DEDUP_ENABLED", True)
        except Exception:
            pipe.global_dedup_enabled = True
        pipe.dedup_scope = crawler.settings.get("CONTENT_GLOBAL_DEDUP_SCOPE", "per_site")
        try:
            pipe.dedup_ttl_seconds = int(crawler.settings.getint("CONTENT_DEDUP_TTL_SECONDS", 0))
        except Exception:
            pipe.dedup_ttl_seconds = 0
        return pipe

    def _connect(self):
        try:
            import redis

            self.redis = redis.from_url(self.redis_url)
            self.redis.ping()
            # æ³¨å†ŒLuaè„šæœ¬
            script = """
            local key = KEYS[1]
            local field = ARGV[1]
            local newv = ARGV[2]
            local oldv = redis.call('HGET', key, field)
            if (not oldv) then
                redis.call('HSET', key, field, newv)
                return 1
            end
            if (oldv ~= newv) then
                redis.call('HSET', key, field, newv)
                return 2
            end
            return 0
            """
            self.lua = self.redis.register_script(script)
        except Exception as e:
            logger.warning(f"âš ï¸ ContentUpdatePipeline æ— æ³•è¿æ¥Redisï¼Œå°†é™çº§è¿è¡Œ: {e}")
            self.redis = None

    def _normalize(self, text: str) -> str:
        if not text:
            return ""
        # ç®€å•è§„èŒƒåŒ–ï¼šå»HTMLæ ‡ç­¾æ®‹ç•™ï¼ˆè‹¥å·²æ¸…æ´—åˆ™åŸºæœ¬æ— å½±å“ï¼‰ã€å‹ç¼©ç©ºç™½
        try:
            stripped = "".join(text.split())  # å¼ºå‹ç¼©ç©ºç™½
            return stripped
        except Exception:
            return text

    def process_item(self, item, spider):
        if not spider.settings.getbool("CONTENT_DEDUP_ENABLED", True):
            return item
        url = item.get("source_url") or item.get("url")
        content = item.get("content")
        if not url or content is None:
            return item

        # è®¡ç®—æŒ‡çº¹
        ufield = hashlib.sha1(url.encode("utf-8")).hexdigest()
        cfp = hashlib.sha256(self._normalize(content).encode("utf-8")).hexdigest()

        # Redis ä¸å¯ç”¨ -> ç›´æ¥é€šè¿‡ï¼ˆä¿ç•™ itemï¼‰
        if not self.redis or not self.lua:
            item["dedup_status"] = "unknown"
            item["content_fingerprint"] = cfp
            return item

        site = (item.get("site") or item.get("site_name") or getattr(spider, "target_site", None) or spider.name or "default")
        key = f"content_fp:{site}"
        try:
            rc = self.lua(keys=[key], args=[ufield, cfp])
        except Exception as e:
            logger.warning(f"âš ï¸ Redis Lua æ‰§è¡Œå¤±è´¥ï¼Œé™çº§é€šè¿‡: {e}")
            item["dedup_status"] = "unknown"
            item["content_fingerprint"] = cfp
            return item

        status = {0: "unchanged", 1: "created", 2: "modified"}.get(int(rc), "unknown")
        item["dedup_status"] = status
        item["content_fingerprint"] = cfp

        if status == "unchanged":
            raise DropItem("å†…å®¹æœªå˜åŒ–ï¼Œä¸¢å¼ƒä»¥èŠ‚çœå­˜å‚¨")

        # è·¨ URL å†…å®¹å»é‡ï¼ˆåŸºäºå†…å®¹æŒ‡çº¹ï¼‰ã€‚å½“ä¸åŒ URL/æ ‡é¢˜çš„å†…å®¹ç›¸åŒï¼Œä»…ä¿ç•™é¦–æ¬¡å‡ºç°
        if self.global_dedup_enabled:
            scope = "global" if self.dedup_scope == "global" else site
            set_key = f"content_seen:{scope}"
            try:
                added = self.redis.sadd(set_key, cfp)  # 1=é¦–æ¬¡ï¼Œ0=å·²å­˜åœ¨
                if self.dedup_ttl_seconds > 0:
                    try:
                        ttl = self.redis.ttl(set_key)
                    except Exception:
                        ttl = -1
                    if ttl == -1:
                        self.redis.expire(set_key, self.dedup_ttl_seconds)
                if added == 0:
                    raise DropItem("è·¨URLå†…å®¹é‡å¤ï¼ŒæŒ‰æŒ‡çº¹å»é‡ä¸¢å¼ƒ")
            except DropItem:
                raise
            except Exception as e:
                logger.warning(f"âš ï¸ å…¨å±€å†…å®¹å»é‡å¤±è´¥ï¼Œé™çº§é€šè¿‡: {e}")
        return item


class MongoPipeline:
    """MongoDBå­˜å‚¨ç®¡é“"""

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
        self.client = None
        self.db = None

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get("MONGODB_URI"),
            mongo_db=crawler.settings.get("MONGODB_DATABASE", "crawler_db"),
        )

    def open_spider(self, spider):
        """çˆ¬è™«å¼€å§‹æ—¶è¿æ¥MongoDB"""
        try:
            import pymongo

            self.client = pymongo.MongoClient(self.mongo_uri)
            self.db = self.client[self.mongo_db]
            logger.info("MongoDBè¿æ¥æˆåŠŸ")
            try:
                from crawler.monitoring.db_instrumentation import (
                    instrument_mongo_client,
                )

                instrument_mongo_client(self.client, db="mongodb")
            except Exception:
                pass
        except ImportError:
            logger.error("pymongoæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨MongoDBç®¡é“")
        except Exception as e:
            logger.error(f"MongoDBè¿æ¥å¤±è´¥: {e}")

    def close_spider(self, spider):
        """çˆ¬è™«ç»“æŸæ—¶å…³é—­MongoDBè¿æ¥"""
        if self.client:
            self.client.close()
            logger.info("MongoDBè¿æ¥å·²å…³é—­")

    def process_item(self, item, spider):
        """å­˜å‚¨æ•°æ®åˆ°MongoDB"""
        if self.db is None:
            logger.warning("âŒ MongoDBæ•°æ®åº“è¿æ¥ä¸ºç©º")
            return item

        logger.info("ğŸ’¾ MongoPipeline å¤„ç†æ•°æ®é¡¹")

        try:
            adapter = ItemAdapter(item)
            site = adapter.get("site_name") or adapter.get("site")
            collection_name = f"{site or spider.name}_data"
            # collection_name = f"{spider.name}_data"
            collection = self.db[collection_name]

            logger.info("ğŸ“Š å‡†å¤‡å­˜å‚¨åˆ°é›†åˆ: %s", collection_name)
            logger.info("ğŸ“„ æ•°æ®é¡¹å­—æ®µæ•°: %s", len(adapter.asdict()))

            # å­˜å‰æ ¡éªŒ
            title = str(adapter.get("title", ""))[:30]
            clen = len(adapter.get("content", "") or "")
            logger.info("ğŸ§¾ å­˜å‰æ ¡éªŒ: title='%s' content_len=%s", title, clen)

            # æ’å…¥æ•°æ®
            result = collection.insert_one(adapter.asdict())
            logger.info("âœ… æ•°æ®å·²å­˜å‚¨åˆ°MongoDB: %s", result.inserted_id)
            try:
                from crawler.monitoring.metrics import ITEM_STORED, labels_site

                ITEM_STORED.labels(**labels_site(spider.name, site)).inc()
            except Exception:
                pass

        except Exception as e:
            logger.error("âŒ MongoDBå­˜å‚¨å¤±è´¥: %s", e)
            import traceback

            logger.error("âŒ é”™è¯¯è¯¦æƒ…: %s", traceback.format_exc())
            raise DropItem(f"MongoDBå­˜å‚¨å¤±è´¥: {e}")

        return item


"""
å·²ç§»é™¤ PostgresPipeline ç±»ï¼ˆç³»ç»Ÿä¸å†æ”¯æŒ PostgreSQL å­˜å‚¨ï¼‰ã€‚
"""


class JsonWriterPipeline:
    """JSONæ–‡ä»¶å†™å…¥ç®¡é“"""

    def open_spider(self, spider):
        """çˆ¬è™«å¼€å§‹æ—¶æ‰“å¼€æ–‡ä»¶"""
        self.file = open(f"data/{spider.name}_items.jl", "w", encoding="utf-8")

    def close_spider(self, spider):
        """çˆ¬è™«ç»“æŸæ—¶å…³é—­æ–‡ä»¶"""
        self.file.close()

    def process_item(self, item, spider):
        """å†™å…¥æ•°æ®åˆ°JSONæ–‡ä»¶"""
        line = json.dumps(ItemAdapter(item).asdict(), ensure_ascii=False) + "\n"
        self.file.write(line)
        return item
