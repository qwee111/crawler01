version: '3.8'

services:
  # Redis服务 - 用于任务队列和缓存
  redis:
    image: redis:7-alpine
    container_name: crawler_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    restart: unless-stopped
    networks:
      - crawler_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # MongoDB服务 - 用于原始数据存储
  mongodb:
    image: mongo:6
    container_name: crawler_mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
      - mongodb_config:/data/configdb
      - ./mongodb/init-mongo.js:/docker-entrypoint-initdb.d/init-mongo.js:ro
    restart: unless-stopped
    networks:
      - crawler_network
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGODB_ROOT_USERNAME:-admin}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGODB_ROOT_PASSWORD:-password}
      - MONGO_INITDB_DATABASE=${MONGODB_DATABASE:-crawler_db}
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # PostgreSQL服务 - 用于结构化数据存储
  postgresql:
    image: postgres:15
    container_name: crawler_postgresql
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgresql/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
      - ./postgresql/postgresql.conf:/etc/postgresql/postgresql.conf:ro
    restart: unless-stopped
    networks:
      - crawler_network
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-crawler_db}
      - POSTGRES_USER=${POSTGRES_USER:-crawler_user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-crawler_pass}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-crawler_user} -d ${POSTGRES_DB:-crawler_db}"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # MinIO服务 - 用于文件存储
  minio:
    image: minio/minio:latest
    container_name: crawler_minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    restart: unless-stopped
    networks:
      - crawler_network
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER:-minioadmin}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-minioadmin123}
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Redis Commander - Redis管理界面
  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: crawler_redis_commander
    ports:
      - "8081:8081"
    environment:
      - REDIS_HOSTS=local:redis:6379
    networks:
      - crawler_network
    depends_on:
      - redis
    restart: unless-stopped
    profiles:
      - tools

  # MongoDB Express - MongoDB管理界面
  mongo-express:
    image: mongo-express:latest
    container_name: crawler_mongo_express
    ports:
      - "8082:8081"
    environment:
      - ME_CONFIG_MONGODB_ADMINUSERNAME=${MONGODB_ROOT_USERNAME:-admin}
      - ME_CONFIG_MONGODB_ADMINPASSWORD=${MONGODB_ROOT_PASSWORD:-password}
      - ME_CONFIG_MONGODB_URL=mongodb://${MONGODB_ROOT_USERNAME:-admin}:${MONGODB_ROOT_PASSWORD:-password}@mongodb:27017/
      - ME_CONFIG_MONGODB_DATABASE=crawler_db
      - ME_CONFIG_BASICAUTH_USERNAME=${MONGO_EXPRESS_USER:-admin}
      - ME_CONFIG_BASICAUTH_PASSWORD=${MONGO_EXPRESS_PASSWORD:-pass}
    networks:
      - crawler_network
    depends_on:
      - mongodb
    restart: unless-stopped
    profiles:
      - tools

  # pgAdmin - PostgreSQL管理界面
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: crawler_pgadmin
    ports:
      - "8083:80"
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    environment:
      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_EMAIL:-admin@crawler.com}
      - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_PASSWORD:-admin123}
    networks:
      - crawler_network
    depends_on:
      - postgresql
    restart: unless-stopped
    profiles:
      - tools

  # Selenium Grid Hub
  selenium-hub:
    image: selenium/hub:4.15.0
    container_name: crawler_selenium_hub
    ports:
      - "4444:4444"
    environment:
      GRID_MAX_SESSION: 16
      GRID_BROWSER_TIMEOUT: 300
      GRID_TIMEOUT: 300
      GRID_NEW_SESSION_WAIT_TIMEOUT: 60
    volumes:
      - selenium_data:/opt/selenium
    networks:
      - crawler_network
    restart: unless-stopped
    profiles:
      - selenium
      - tools

  # Chrome节点
  chrome-node:
    image: selenium/node-chrome:4.15.0
    container_name: crawler_chrome_node
    depends_on:
      - selenium-hub
    environment:
      HUB_HOST: selenium-hub
      HUB_PORT: 4444
      NODE_MAX_INSTANCES: 2
      NODE_MAX_SESSION: 2
      SE_EVENT_BUS_HOST: selenium-hub
      SE_EVENT_BUS_PUBLISH_PORT: 4442
      SE_EVENT_BUS_SUBSCRIBE_PORT: 4443
    volumes:
      - /dev/shm:/dev/shm
    networks:
      - crawler_network
    restart: unless-stopped
    profiles:
      - selenium
      - tools

  # Firefox节点
  firefox-node:
    image: selenium/node-firefox:4.15.0
    container_name: crawler_firefox_node
    depends_on:
      - selenium-hub
    environment:
      HUB_HOST: selenium-hub
      HUB_PORT: 4444
      NODE_MAX_INSTANCES: 2
      NODE_MAX_SESSION: 2
      SE_EVENT_BUS_HOST: selenium-hub
      SE_EVENT_BUS_PUBLISH_PORT: 4442
      SE_EVENT_BUS_SUBSCRIBE_PORT: 4443
    volumes:
      - /dev/shm:/dev/shm
    networks:
      - crawler_network
    restart: unless-stopped
    profiles:
      - selenium
      - tools

  # 爬虫服务
  crawler:
    build:
      context: ../.. # 构建上下文为项目根目录
      dockerfile: ./deployment/docker/Dockerfile
    container_name: crawler_app
    volumes:
      - ../..:/app # 将项目根目录挂载到容器的/app
      - ./logs:/app/logs # 挂载日志目录
    networks:
      - crawler_network
    depends_on:
      - redis
      - mongodb
      - postgresql
      - selenium-hub # 如果爬虫需要Selenium
    environment:
      # 传递环境变量，例如数据库连接信息
      REDIS_HOST: redis
      REDIS_PORT: 6379
      MONGODB_HOST: mongodb
      MONGODB_PORT: 27017
      POSTGRES_HOST: postgresql
      POSTGRES_PORT: 5432
      SELENIUM_HUB_URL: http://selenium-hub:4444/wd/hub
      # 从.env文件加载其他环境变量
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      MONGODB_ROOT_USERNAME: ${MONGODB_ROOT_USERNAME:-admin}
      MONGODB_ROOT_PASSWORD: ${MONGODB_ROOT_PASSWORD:-password}
      MONGODB_DATABASE: ${MONGODB_DATABASE:-crawler_db}
      POSTGRES_DB: ${POSTGRES_DB:-crawler_db}
      POSTGRES_USER: ${POSTGRES_USER:-crawler_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-crawler_pass}
    command: python start_scheduler.py # 启动调度器
    restart: unless-stopped

# Nginx - 反向代理 (可选，主要用于生产环境)
# nginx:
#   image: nginx:alpine
#   container_name: crawler_nginx
#   ports:
#     - "80:80"
#     - "443:443"
#   volumes:
#     - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
#     - ./nginx/conf.d:/etc/nginx/conf.d:ro
#     - nginx_logs:/var/log/nginx
#   networks:
#     - crawler_network
#   depends_on:
#     - redis-commander
#     - mongo-express
#     - pgadmin
#   restart: unless-stopped
#   profiles:
#     - tools

volumes:
  redis_data:
    driver: local
  mongodb_data:
    driver: local
  mongodb_config:
    driver: local
  postgres_data:
    driver: local
  minio_data:
    driver: local
  pgadmin_data:
    driver: local
  selenium_data:
    driver: local
# nginx_logs:
#   driver: local

networks:
  crawler_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
