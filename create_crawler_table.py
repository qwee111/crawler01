#!/usr/bin/env python3
"""
åˆ›å»ºcrawler_dataè¡¨çš„è„šæœ¬
"""

import os

import psycopg2
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# PostgreSQLè¿æ¥é…ç½®
postgres_host = os.getenv("POSTGRES_HOST", "localhost")
# å¦‚æœæ˜¯Dockerå®¹å™¨åï¼Œè½¬æ¢ä¸ºlocalhost
if postgres_host in ["postgresql", "postgres"]:
    postgres_host = "localhost"

POSTGRES_CONFIG = {
    "host": postgres_host,
    "port": int(os.getenv("POSTGRES_PORT", "5432")),
    "database": os.getenv("POSTGRES_DB", "crawler_db"),
    "user": os.getenv("POSTGRES_USER", "crawler_user"),
    "password": os.getenv("POSTGRES_PASSWORD", "crawler_pass"),
    "connect_timeout": 10,
}

# åˆ›å»ºè¡¨çš„SQL
CREATE_TABLE_SQL = """
-- åˆ é™¤ç°æœ‰è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
DROP TABLE IF EXISTS crawler_data;

-- åˆ›å»ºæ–°è¡¨
CREATE TABLE crawler_data (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    url VARCHAR(1000) NOT NULL,
    title TEXT,
    content TEXT,

    -- åŸºæœ¬ä¿¡æ¯
    spider_name VARCHAR(100),
    spider_version VARCHAR(20),
    site_name VARCHAR(200),
    page_type VARCHAR(100),
    content_type VARCHAR(200),
    status_code INTEGER,

    -- åˆ—è¡¨é¡µç‰¹æœ‰å­—æ®µ
    items TEXT,  -- JSONæ ¼å¼å­˜å‚¨åˆ—è¡¨é¡¹

    -- æ—¶é—´ä¿¡æ¯
    extraction_time TIMESTAMP WITH TIME ZONE,
    extraction_timestamp TIMESTAMP WITH TIME ZONE,
    crawl_timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,

    -- å…ƒæ•°æ®ï¼ˆä½¿ç”¨TEXTå­˜å‚¨JSONå­—ç¬¦ä¸²ï¼‰
    page_analysis TEXT,
    response_meta TEXT,
    extraction_metadata TEXT,
    processing_metadata TEXT,
    validation_metadata TEXT,
    quality_metadata TEXT,

    -- å…¶ä»–å­—æ®µ
    content_length INTEGER,
    content_fingerprint VARCHAR(64),
    chinese_char_count INTEGER,
    extraction_config VARCHAR(200),

    -- å®¡è®¡å­—æ®µ
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_crawler_data_url ON crawler_data(url);
CREATE INDEX IF NOT EXISTS idx_crawler_data_spider ON crawler_data(spider_name);
CREATE INDEX IF NOT EXISTS idx_crawler_data_site ON crawler_data(site);
CREATE INDEX IF NOT EXISTS idx_crawler_data_crawl_time ON crawler_data(crawl_timestamp);
"""


def create_table():
    """åˆ›å»ºcrawler_dataè¡¨"""
    try:
        print(
            f"è¿æ¥PostgreSQLæ•°æ®åº“: {POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}/{POSTGRES_CONFIG['database']}"
        )
        print(f"ç”¨æˆ·: {POSTGRES_CONFIG['user']}")

        # è¿æ¥æ•°æ®åº“
        conn = psycopg2.connect(**POSTGRES_CONFIG)
        cursor = conn.cursor()

        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")

        # æ‰§è¡Œåˆ›å»ºè¡¨çš„SQL
        print("åˆ›å»ºcrawler_dataè¡¨...")
        cursor.execute(CREATE_TABLE_SQL)

        # æäº¤æ›´æ”¹
        conn.commit()
        print("âœ… crawler_dataè¡¨åˆ›å»ºæˆåŠŸ")

        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute(
            """
            SELECT column_name, data_type
            FROM information_schema.columns
            WHERE table_name = 'crawler_data'
            ORDER BY ordinal_position;
        """
        )

        columns = cursor.fetchall()
        print(f"âœ… è¡¨ç»“æ„éªŒè¯æˆåŠŸï¼Œå…±æœ‰ {len(columns)} ä¸ªå­—æ®µ:")
        for col_name, col_type in columns:
            print(f"  - {col_name}: {col_type}")

        # å…³é—­è¿æ¥
        cursor.close()
        conn.close()
        print("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")

        return True

    except Exception as e:
        print(f"âŒ åˆ›å»ºè¡¨å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹åˆ›å»ºcrawler_dataè¡¨...")
    success = create_table()

    if success:
        print("ğŸ‰ è¡¨åˆ›å»ºå®Œæˆï¼")
    else:
        print("ğŸ’¥ è¡¨åˆ›å»ºå¤±è´¥ï¼")
